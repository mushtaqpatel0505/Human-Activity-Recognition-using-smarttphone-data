{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "    \n",
    "\n",
    "    return X_train,  X_test, y_train,  y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "epochs = 25\n",
    "batch_size = 16\n",
    "n_hidden = 32\n",
    "timesteps=128\n",
    "input_dim = 9\n",
    "n_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mushtaq\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "#to save it\n",
    "with open(\"har_data.pkl\", \"wb\") as f:\n",
    "    pkl.dump([X_train, X_test, Y_train, Y_test], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                5376      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 5,574\n",
      "Trainable params: 5,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/25\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 1.3139 - acc: 0.4358 - val_loss: 1.1352 - val_acc: 0.4700\n",
      "Epoch 2/25\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.9788 - acc: 0.5773 - val_loss: 0.9513 - val_acc: 0.5884\n",
      "Epoch 3/25\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.7977 - acc: 0.6457 - val_loss: 0.8343 - val_acc: 0.6013\n",
      "Epoch 4/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.6989 - acc: 0.6582 - val_loss: 0.7532 - val_acc: 0.6098\n",
      "Epoch 5/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.6359 - acc: 0.6797 - val_loss: 0.7335 - val_acc: 0.6183\n",
      "Epoch 6/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.5819 - acc: 0.6865 - val_loss: 0.8786 - val_acc: 0.6098\n",
      "Epoch 7/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.5676 - acc: 0.7058 - val_loss: 0.8191 - val_acc: 0.6132\n",
      "Epoch 8/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.5583 - acc: 0.7217 - val_loss: 0.6639 - val_acc: 0.7190\n",
      "Epoch 9/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.5386 - acc: 0.7557 - val_loss: 0.6388 - val_acc: 0.7167\n",
      "Epoch 10/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.4804 - acc: 0.7911 - val_loss: 0.5077 - val_acc: 0.7509\n",
      "Epoch 11/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.4320 - acc: 0.8052 - val_loss: 0.5143 - val_acc: 0.7418\n",
      "Epoch 12/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.4279 - acc: 0.8062 - val_loss: 0.4951 - val_acc: 0.7472\n",
      "Epoch 13/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.3911 - acc: 0.8130 - val_loss: 0.5606 - val_acc: 0.7516\n",
      "Epoch 14/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.3898 - acc: 0.8313 - val_loss: 0.4518 - val_acc: 0.8137\n",
      "Epoch 15/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.3308 - acc: 0.8942 - val_loss: 0.4732 - val_acc: 0.8633\n",
      "Epoch 16/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.2891 - acc: 0.9176 - val_loss: 0.3794 - val_acc: 0.8765\n",
      "Epoch 17/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.2660 - acc: 0.9246 - val_loss: 0.5082 - val_acc: 0.8660\n",
      "Epoch 18/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.2538 - acc: 0.9251 - val_loss: 0.4772 - val_acc: 0.8806\n",
      "Epoch 19/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.2502 - acc: 0.9312 - val_loss: 0.7013 - val_acc: 0.8307\n",
      "Epoch 20/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.1980 - acc: 0.9382 - val_loss: 0.3988 - val_acc: 0.8890\n",
      "Epoch 21/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.2018 - acc: 0.9372 - val_loss: 1.7682 - val_acc: 0.7075\n",
      "Epoch 22/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.2455 - acc: 0.9310 - val_loss: 0.5812 - val_acc: 0.8687\n",
      "Epoch 23/25\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.2194 - acc: 0.9329 - val_loss: 0.6468 - val_acc: 0.8744\n",
      "Epoch 24/25\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.2282 - acc: 0.9304 - val_loss: 0.4721 - val_acc: 0.8741\n",
      "Epoch 25/25\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.2166 - acc: 0.9359 - val_loss: 0.4131 - val_acc: 0.8938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xfd26dcdc50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the confusion matrices\n",
    "def plot_confusion_matrix(y_test, y_predict):\n",
    "    C = confusion_matrix(y_test, y_predict)\n",
    "    # C = 6,6 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    #divid each element of the confusion matrix with the sum of elements in that column\n",
    "    B =(C/C.sum(axis=0))\n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    labels = ['LAYING' , 'SITTING',  'STANDING', 'WALKING',  'WALKING_DOWNSTAIRS','WALKING_UPSTAIRS']         \n",
    "    # representing A in heatmap format\n",
    "    cmap=sns.light_palette(\"green\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # representing B in heatmap format\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABOcAAAGECAYAAAB9OkTiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xd8FVX6x/HPkwYEEkLvTUFUVJrSFGkLBFERGyrWnyurYlsLKCooVtRdwVURVhRUUGwoSlWwLAiCglSRJkgndAg1yfn9MZN40xOScHPl+3698uLOmTNnnhvmzpM598wZc84hIiIiIiIiIiIiJ15YsAMQERERERERERE5WalzTkREREREREREJEjUOSciIiIiIiIiIhIk6pwTEREREREREREJEnXOiYiIiIiIiIiIBIk650RERERERERERIJEnXMiJxEzK2VmX5jZXjP7qADt9Daz6YUZW7CYWVsz+y3YcYiIFGdmtszM2udSp7aZHTCz8BMUVpHy38spwY5DROSvxszam9nGgOV1Zva3YMYUyMzeMLPHgx2HnFzUOSdSDJnZdWb2k39hsMXMppjZBYXQ9JVAFaCCc+6q423EOTfWOdelEOIpUmbmzKx+TnWcc/9zzjU8UTGJiBQm/4LmkJ8vtpnZ22ZWprD345xr5Jz7Npc6fzjnyjjnkgt7/4XJzL41s7/nVs9/L2tPREwiIsGSIY9sNbPRRZFHigMzu9nMZuVWzzl3u3PuqRMRk0gqdc6JFDNmdj8wFHgWryOtNvA60KMQmq8DrHTOJRVCWyHPzCKCHYOISCG4xDlXBmgGnAc8lrGCefR3Xx4oN4jISSg1jzQBmgKPBDmeoPmrjP6W0KM/0kSKETMrCwwG+jrnPnXOJTrnjjnnvnDOPeTXKWFmQ81ss/8z1MxK+Ovam9lGM3vAzLb7o+5u8dc9CQwEevnfjN1qZk+Y2XsB+6/rjzaL8JdvNrO1ZrbfzH43s94B5bMCtmtjZvP922Xnm1mbgHXfmtlTZjbbb2e6mVXM5v2nxt8vIP7LzOwiM1tpZrvMbEBA/RZmNsfM9vh1XzWzKH/d9361Rf777RXQfn8z2wq8HTis3sxO9ffRzF+ubmY7cruVS0SkOHDObQKmAGdB2vn3GTObDRwETjGzsmY2yj9nbjKzpwMvRMzsNjP71T9fLw84H6bdcuSfe38ys33+aL1/++UZc0h1M5von1dXm9ltAft5wsw+NLN3/H0tM7Nzs3tvfrt3mtkqv/5T/jl7jh/HhwHn/3Jm9qWZJZjZbv91TX/dM0Bb4FU/N7wa0H5fM1sFrAooq29mUWb2i5nd7ZeH+zltYKH8x4mIFBPOua3ANLxOOiDt2uMlM/vDP+e/YWalAtb38M+R+8xsjZnF++W3BOSTtWb2j+OJybyRfK+bdyfRAf/8W9W8a6DdZrbCzJoG1H/YjyM1j/X0y88A3gBa++3sCWh/uJlNNrNEoINf9rS/vr+ZzQ3IbXf4Oavk8bwfkeyoc06keGkNlAQm5FDnUaAVXtJsDLQg/SiJqkBZoAZwK/CamZVzzg3CG4033r9VZ1ROgZhZaeAVoJtzLgZoA/ySRb3ywCS/bgXg38AkM6sQUO064BagMhAFPJjDrqvi/Q5q4HUm/he4HmiOd0E10P6cAygZ+CdQEe931wm4E8A5d6Ffp7H/fscHtF8ebxRhn8AdO+fWAP2BsWYWDbwNjM7tVi4RkeLAzGoBFwELA4pvwDvXxQDrgTFAElAfb3REF+Dv/vZXAU8ANwKxwKXAzix2NQwY5pyLBU4FPswmpPeBjUB1vGkVnjWzTgHrLwU+AOKAicCrubzFeLxc0AroB4wEegO18Dokr/XrheGdv+vgjT4/lNq2c+5R4H/AXX5uuCug/cuAlsCZgTt1zh3Fy0OD/Yu7h4Fw4Jlc4hURCSn+FxndgNUBxUOA0/CuPerz59/omFkL4B3gIbxz+YXAOn+77cDFePnkFuDl1C98jsPVeNc7FYEjwBxggb/8Md71R6o1eNcMZYEngffMrJpz7lfgdmCOf/6PC9jmOrxzegyQ8bbXF4GjwGNm1gDveup659zh43wvIllS55xI8VIB2JHLbae9gcHOue3OuQS8pHNDwPpj/vpjzrnJwAHgeOdUSwHOMrNSzrktzrllWdTpDqxyzr3rnEtyzr0PrAAuCajztnNupXPuEN5FXJMs2gmM/xnn3DG8i7aKeBeB+/39LwPOAXDO/eycm+vvdx0wAmiXh/c0yDl3xI8nHefcf/FGTfwIVMPrDBURKc4+80cAzAK+w7twSDXaObfMzyvl8S667vNHZm8HXgau8ev+HXjBOTffeVY759Znsb9jQH0zq+icO+Ccm5uxgt9ReAHQ3zl32Dn3C/Am6fPVLOfcZH+OunfxvnDKyRDn3D4/FywFpjvn1jrn9uKNGGwK4Jzb6Zz7xDl30Dm3H++CK7fcAPCcc25XNrlhKfA03pdnDwI3FPe59URE8uEzM9sPbMDrVBsE3pQIwG3AP/3z4368HJOaN24F3nLOfeWcS3HObXLOrQBwzk1yzq3x88l3wHS8TrPjMcH/u/8w3nn4sHPuHf88PB7//O/v9yPn3GY/nvF4f9e3yKX9z51zs/1t0nW6OedS8L60ugfvi6QXnHMLs2pEpCDUOSdSvOwEKlrO891Uxxv9kGq9X5bWRobOvYNAvid1dc4lAr3wvmHaYmaTzOz0PMSTGlONgOWt+YhnZ8AFT+oF0raA9YdStzez0/zblbaa2T68PxayvGU2QEIevun6L94ojP84547kUldEJNguc87FOefqOOfuzNC5tCHgdR0gEu+cvsfv0BuBN6oZvBFoa/Kwv1vxRlGsMG8qg4uzqFMdSL2QS5VbbiiZS/7LmAuyyw3RZjbCzNb7ueF7IM5yn0doQy7rxwB1gcnOuVW51BURCSWX+XfKtAdO58+/pysB0cDPAXljql8OOeQNM+vm3w66y9/uInL/Oz07eTr/+/u90b/NNjXes/Kw3xzP//4ggG/wcsBreQ9bJO/UOSdSvMwBDuPdWpOdzXgXWKlq+2XHIxEv4aaqGrjSOTfNOdcZbwTZCrxOq9ziSY1p03HGlB/D8eJq4N9eNQCwXLZxOa007+lUQ4FRwBP+bbsiIqEq8Jy3Ae92oIp+Z16ccy7WOdcoYP2puTbo3Crn3LV4nXpDgI/9qRACbQbKm1lMQNmJyg0P4I0Yb+nnhtRpDlLzQ3Z5IMf8gPdwpi+BrlY4T1AXESlW/BFuo4GX/KIdeJ1fjQLyRlnnPTwCsskb5s2H/YnfThX/FtLJ5P53eoGYWR2865W7gAr+fpdSwPO/mV2EN4XODLzbXEUKnTrnRIoR/9acgXjzxF3mf/sf6X/z9IJf7X28OQ8qmfdghYHAe9m1mYtfgAvNrLZ5D6NIezKTmVUxs0v9C64jeLfHZnULz2TgNDO7zswizKwX3nw9Xx5nTPkRA+wDDvij+u7IsH4bcEqmrXI2DPjZOfd3vLn03ihwlCIixYBzbgvebUX/MrNYMwsz76EKqbd8vgk8aGbNzVPfv9BJx8yuN7NK/q0+e/zidPnBObcB+AF4zsxKmtk5eCPuxhbV+wsQg3cxucf/gmVQhvX5zg1mdgPefHc3493aNMb/MkdE5K9mKNDZzJr45/n/4s0XVxnAzGqYWVe/7ijgFjPr5OeUGv7f5FFACSABSDKzbnhznBa10ngdbQl+rLfgPyTJtw2oaf4DhPLCv94ahTf1w03AJX5nnUihUuecSDHjnPs3cD/epKcJeN9I3QV85ld5GvgJWAwswZsM9enj3NdXePM0LAZ+Jn2HWhje6IPNwC68+XruzKKNnXiTvT6Ad1tuP+Bi59yO44kpnx7Em8B1P94fDuMzrH8C7wJqj5ldnVtjZtYDb8Lx2/2i+4Fm5j+lVkTkL+BGvIum5cBuvIm0q4E3Tw/e/Gzj8M6rn+HNU5dRPLDMzA7gfaFxTTbTBVyLdwvQZrw5ggb5eaeoDQVK4Y34mIt3C1agYcCV5j3l75XcGjOz2n6bN/pz7I3Dy8MvF27YIiLB589p/Q7wuF/UH+8BEXP9qQK+xp/P2jk3D/9hD8BevHlP6/hTGtyDN9f0bry/1yeegNiXA//CuxtpG3A2MDugyky8+au3mller1VG4s1JN9m/7rkVeDPDw+9ECsycy20Ev4iIiIiIiIiIiBQFjZwTEREREREREREJEnXOiYiIiIiI+MzsLTPbbmZLs1lvZvaKma02s8Vm1uxExygiIsFTFHlCnXMiIiIiIiJ/Go03t2J2ugEN/J8+eE+PFxGRk8doCjlPqHNORERERETE55z7Hu9hWNnpAbzjPHOBODOrdmKiExGRYCuKPBFRmAGK2JMWMk8Y2TRoU7BDEDlpVKe6FWT7/Jxb3CBXoH1J0VKeEJGsnMg8wRP8A28kQ6qRzrmR+dhdDWBDwPJGv2xLPtqQbChPiEhGBc0RUPzzhDrnRERERETkpOFfYOXnIiujrC4SQ6ZDSUREchaMPKHbWkVERERERPJuI1ArYLkmsDlIsYiISPGT7zyhzjkREREREZG8mwjc6D+NrxWw1zmnW1pFRCRVvvOEbmsVERERERHxmdn7QHugopltBAYBkQDOuTeAycBFwGrgIHBLcCIVEZFgKIo8oc45ERERERERn3Pu2lzWO6DvCQpHRESKmaLIE7qtVUREREREREREJEjUOSciIiIiIiIiIhIk6pwTEREREREREREJEnXOiYiIiIiIiIiIBIk650RERERERERERIJEnXMiIiIiIiIiIiJBos45ERE56ZjZOjNbYma/mNlPfll5M/vKzFb5/5bzy83MXjGz1Wa22MyaBTd6ERERERH5K1HnnIiInKw6OOeaOOfO9ZcfBmY45xoAM/xlgG5AA/+nDzD8hEcqIiIiIiJ/WeqckyLz+72/s/j2xSz8x0Lm3zYfgMEdBrPo9kUs/MdCpl0/jWplqgHwYJsHWfiPhSz8x0KW3LGEpMeTKFeyXKY268bVZe6tc1l510o+uOIDIsMiAYgKj+KDKz5g1d2rmHvrXOqUrZO2zcMXPMyqu1exou8KupzaJd/vY97387ix64307tybcSPHZVp/9OhRnrzvSXp37s0dV93B1o1b09aNHTGW3p17c2PXG5n3v3l5brMgFK/iDcVYi4kewBj/9RjgsoDyd5xnLhBnZtWCEeDJYtSlo9j24DaW3LEk2zrD4oex6u5VLLp9EU2rNk0rv7Hxjay8ayUr71rJjY1vTCtvVq0Zi29fzKq7VzEsflihxhtqnzXFq/NuqMYrkkp5QueGUI03lGINxXhDmTrnQoSZHchh3SIzez9guY+ZjQ9YjjWzNWZWz8xGm9mVfvm3qbdz+cvnmtm3Acst/DqrzGyBmU0ys7PzE3eHMR1oOqIp5/33PABenP0ijd9oTNMRTfly5ZcMbDcQgJd+eImmI5rSdERTHpnxCN+t/47dh3dnam/I34bw8tyXOe3V09h9eDe3NrsVgFub3sruw7tp8J8GvDz3ZYb8bQgAZ1Q8g2saXUOj1xsRPzae1y96nTDL+2GfnJzMsMHDeP7N5xk9aTQzvpzButXr0tWZ/NFkYmJjGPvVWK66+SpGvDQCgHWr1zFz0kzenvQ2Q94cwrAnh5GcnJynNo+X4lW8oRhrYfPPgT8F/PTJopoDppvZzwHrqzjntgD4/1b2y2sAGwK23eiXFRuhmiOyM/qX0cS/F5/t+m71u9GgfAMa/KcBfb7ow/Du3mDGciXLMajdIFq+2ZIWb7ZgULtBxJWMA2B49+H0+bIPDf7TgAblGxBfP/v28yPUPmuKV+fdUI1XCkZ5QnlC8QY/3lCKNRTjDXXqnAtxZnYG3v/jhWZW2i/+L1DTzP7mLw8G3nLO/Z5FE5XNrFsW7VYBPgQGOOcaOOeaAc8BpxYk3v1H96e9Lh1VGofLVOfas67l/aXvZyoH6FivIx8v/xiAMYvGcFlDb2BLj4Y9GLPIG/Dy8fKP6XRKJ6/89B58sOwDjiYfZd2edazetZoWNVrkOd4Vi1dQvU51qteqTmRUJB27d2T2jNnp6syeOZuuPbsC0K5rOxbMWYBzjtkzZtOxe0eioqKoVqsa1etUZ8XiFXlq83gpXsUbirEWNufcSOfcuQE/I7Oodr5/XusG9DWzC3No0rLaTaEEW8RCLUek+t8f/2PXoV3Zru9xeg/eWfwOAD9u+pG4knFULVOVrvW78tXar9h9eDd7Du/hq7VfEV8/nqplqhJbIpa5G+cC8M7id7js9MuybT8/Qu2zpnh13g3VeKVoKE8oTyhe5Ym/SryhTp1zoe864F1gOnApgHPOAXcAQ83sXKAT8GI2278IPJZF+V3AGOfcD6kFzrlZzrnP8hqYc47pN0znp9t+4rZmt6WVP93xaf647w96n92bgd8MTLdNqYhSxNeP55Pln2Rqr0KpCuw5vIdklwzAxn0bqRHrDV6pEVuDDXu9gS3JLpm9h/dSoVQFasT8WQ6wcf9GasTkfcDLjm07qFy1ctpypSqV2LFtR+Y61bw64RHhlIkpw77d+7LdNi9tHi/Fq3hDMdZgcM5t9v/dDkwAWgDbUm9X9f/d7lffCNQK2LwmsPnERVsgxTZHFESmc/s+79yeU/nGfRszlReGUPusKV6dd0M1XikyyhPKE4r3BMUbSrGGYryhTp1zoa8XMB54H7g2tdA5txiYhjep+T3OuaPZbD8HOGJmHTKUNwIWFCSw8986n+Yjm9NtbDf6nteXtrXbAvDYzMeoPbQ2Y5eM5a4Wd6Xb5pKGlzD7j9lZ3tJqlnnwive3A1gWA1scLutt8jHgJbX9nOLIqg6Wv/Ks4jweilfx5hRHcY31RDOz0mYWk/oa6AIsBSYCN/nVbgI+919PBG70n9raCtibevtrCCi2OaIg8nvOzyl/FFSofdYUr867OcVSnOOVIqM8oTyRayyKV3kiu7aLU7yhTp1zIczMzgMSnHPr8RJnMzMLfIrCa8Am59w3uTT1NFl/4xW4rx/N7FczyzQ7auD8Tvz0Z/mWA961a8LBBCasmJDpdtJxS8ZxxRlXpCu7ptE12d7SuuPgDuJKxhFu4QDUjK3J5v3e4JWN+zZSq6w3sCXcwilbsiy7Du1KVw5QM+bPbfKiUtVKbN+6PW05YVsCFSpXyFxni1cnOSmZA/sPEBsXm+W2FStXzFObx0vxKt5QjDUIqgCzzGwRMA+Y5JybCjwPdDazVUBnfxlgMrAWWI13q8+dJz7k/CsuOcJfn2WeOF4b92c4t/v5INM5P6C8ZmzN9OUHCmfwY6h91hSvzruhGq8UPuUJ5QnFqzzxV4o31KlzLrRdC5xuZuuANUAsENjbleL/5Mg5NxMoCbQKKF4GNAuo0xJ4HCibxfZp8ztxrlcWHRlNmagyaa+7nNqFpduXUr98/bTtLm14KSt2rEhbji0RS7u67fj8t8/Jzje/f8OVZ14JwE2Nb0qrO3HlRG5q7A14ufLMK5n5+0yv/LeJXNPoGqLCo6gbV5cGFRowb9O8rBvPwulnn86mdZvYsmELx44eY+akmbTp2CZdnTYd2zBtwjQAvpv2HU1bNcXMaNOxDTMnzeTo0aNs2bCFTes2cfo5p+epzeOleBVvKMZ6ojnn1jrnGvs/jZxzz/jlO51znfy5cTo553b55c4519c5d6pz7mznXCFcNpwQxSJH+Osz5YmCmPjbRG48x3vCXssaLdl7ZC9bD2xl2uppdDmlC3El44grGUeXU7owbfU0th7Yyv4j+2lZoyUAN55zI5+vyD7X5EeofdYUr867oRqvFAnlCeUJxas88ZeJN9RFBDsAOT5mFgZcBZzjnNvkl3XA+9bqzeNo8hngDbzRIeB9U/ajmU0LmCsiOq+NVSldhQm9JgAQERbBuKXjmLZmGh9f9TENKzYkxaWwfs96bp90e9o2PU/vyfQ10zl47GC6tiZdN4m/T/w7Ww5sof/X/fngyg94uuPTLNyykFELRwEwasEo3u35LqvuXsWuQ7u45uNrAFiesJwPl3/I8juXk5SSRN/JfUlxuf6NkSY8Ipx7Bt5Dv7/3IyU5hW5XdKNeg3q8NewtGp7VkPM7nU/3K7vz7EPP0rtzb2LLxvL4y48DUK9BPTp068AtF91CeHg49w68l/Bwb9RfVm0WBsWreEMxVil8xT1H5Gbc5eNoX7c9FaMrsuGfGxj07SAiwyIBGPHzCCavmsxFDS5i9d2rOXjsILd8fgsAuw/v5qnvn2L+bfMBGPz94LRpEu6YdAejLxtNqYhSTFk9hSmrpxRKrKH2WVO8Ou+GarxSuJQnlCcUr/LEXy3eUGeFdS+9FC0zSyH9BOT/Bq52zrUKqBOON3F5M+fcFjOrC3zpnDsroM5ov+xj8x51/mDqKBAz+xnY75xr7y+3AoYANfAmRt8BDM5p1Ig9aSFzQG0atCnYIYicNKpTvUCTSeTn3OIGuZNu4opQyRGgPCEiWVOeKFrKE0VDeULkxChojoDinyc0ci5EOOeyugX53xnqJAPVApbXAWdlqHNzwOv2GdY1z7A8F2h3nCGLiMgJohwhIiI5UZ4QESneNOeciIiIiIiIiIhIkKhzTkREREREREREJEjUOSciIiIiIiIiIhIk6pwTEREREREREREJEnXOiYiIiIiIiIiIBIk650RERERERERERIJEnXMiIiIiIiIiIiJBos45ERERERERERGRIFHnnIiIiIiIiIiISJCoc05ERERERERERCRI1DknIiIiIiIiIiISJOqcExERERERERERCZKIYAcgfy2bBm0Kdgh51m1kt2CHkC8vXPJCsEPIl7OrnR3sEESkGFKeKDrKEyLyV6A8UXSUJ0SKL3XOiYhIsXdOtXOCHYKIiBRjyhMiIpKT4p4ndFuriIiIiIiIiIhIkKhzTkREREREREREJEjUOSciIiIiIiIiIhIk6pwTERERERHxmVm8mf1mZqvN7OEs1tc2s2/MbKGZLTazi4IRp4iIBEdR5Al1zomIiIiIiABmFg68BnQDzgSuNbMzM1R7DPjQOdcUuAZ4/cRGKSIiwVJUeUKdcyIiIiIiIp4WwGrn3Frn3FHgA6BHhjoOiPVflwU2n8D4REQkuIokT6hzTkREREREThpm1sfMfgr46ROwugawIWB5o18W6AngejPbCEwG7i7SgEVE5IQKRp6IKGDMIiIiIiIiIcM5NxIYmc1qy2qTDMvXAqOdc/8ys9bAu2Z2lnMupTDjFBGR4AhGntDIOREREREREc9GoFbAck0y3450K/AhgHNuDlASqHhCohMRkWArkjyhzjkRERERERHPfKCBmdUzsyi8ibwnZqjzB9AJwMzOwLvoSjihUYqISLAUSZ5Q55yIiIiIiAjgnEsC7gKmAb/iPW1vmZkNNrNL/WoPALeZ2SLgfeBm51zGW5pEROQvqKjyhOacExERERER8TnnJuNN4B1YNjDg9XLg/BMdl4iIFA9FkSc0ck5ERERERERERCRINHJOTrh538/j1WdeJTklme5Xdee6PtelW3/06FGe6/ccK5etJDYulkEvD6JqzaoAjB0xlskfTyY8LJy7HruLFm1b5KnN3ESFR/H2JW8TGR5JhEXw1e9fMfzn4bSo3oL7W91PZFgky3cs54nvniDZJXNutXMZ2nUom/ZtAmDmupmMWDAiU7s1YmowpNMQYkvEsmLHCgZ8M4CklCQiwyJ5psMznFHxDPYe2Uu/r/ux+YA3h+T/Nfk/ejbsSYpLYcgPQ/hh4w+5xv/tx98y58s5OBytu7emw1UdAPju0+/434T/ERYeRqNWjehxe49M2y7/cTmfvvopKckptO7ems69OwOwc8tORg8ezcF9B6l5Wk1uGHADEZERHDt6jPeee48Nv22gdNnS3DzwZipUq5Cv33eg4ng8/FXiDaVYRQKF2rHbpmYb+rfpT5iFMWHFBN5a9Fa69Q+2fpDzqp0HQKmIUpQrVY62Y9oCcF/L+7iw1oWYGXM3zWXID0OIjozm7UveTtu+SpkqTFo1iRfnvFgo8WZ33k+1etFqPn31Uzav2cxNA2+iafumaes+f+Nzls1dhktxNDy3IVfcfQVmfz60bOSAkezcvJNHRj9SKLFCaB0PoRRrKMYrkirUjt3c8kTV0lV5usPTxETFEGZhDJs3jFkbZgE5XxuEWRjv93yf7YnbuXva3YUWb4HyxIjPWT5nOQBdb+xKs47NAHDOMWnUJBZ+u5CwsDAu6HEB7a5oVyjxhtLxEEqxhmK8oUwj54oxM3vUzJaZ2WIz+8XMWprZt2Z2rpn96Jf9YWYJ/utfzGxbNuV1zWydmVX023Zm9q+AfT1oZk8ELF/v73eZmS0yszfNLK6g7yk5OZlhg4fx/JvPM3rSaGZ8OYN1q9elqzP5o8nExMYw9quxXHXzVYx4yev0Wrd6HTMnzeTtSW8z5M0hDHtyGMnJyXlqMzdHk4/y9y//ztWfXM3Vn1zN+bXOp3GVxjzV/in6z+jPFR9fwZb9W7j0tEvTtlm4ZSG9Pu1Fr097ZdkxB3Bvi3t5b8l7XDr+UvYd2UfPhj0B6Hl6T/Yd2ccl4y/hvSXvcV/L+wA4Je4U4k+N5/KPLufOKXcy4IIBhFnOH9PNazcz58s5PPDGA/R/sz/L5ixj+8btrFy4kiWzltB/VH8GjB5Ax14dM22bkpzCR8M+4vYhtzNgzAB+nvkzW9ZtAbzE2v7K9jw+9nGiy0QzZ/IcAOZOnkt0mWgGjhtI+yvbM3Fkxrkv8664Hg9/hXhDKVY5fsoTwT92wyyMARcM4M4pd9Lzo57E14/nlLhT0tV5ac5Lafni/WXvM3PdTAAaV2lMkypNuPKTK7ni4ytoVKkR51Y7l4PHDqbV7/VpL7bs38KM32cUSrw5nfdTlatcjt4P96b535qnK1+7dC1rl67l4VEP88jbj/DHij9Y/cvqtPWLvl9EiVIlCiXOVKF0PIRSrKEYrxwf5YngH7t5yRO3NbuNaWum0evTXvSf0Z8BFwwAcr826H1Wb9buWVsocaYqSJ5YNmcZG1dupN+b/bh/+P3M+GAGhxIPAfDj1B/ZvX03j77zKI++82hap11BhdLxEErf7b7bAAAgAElEQVSxhmK8oU6dc8WUmbUGLgaaOefOAf4GbEhd75xr6ZxrAgwExjvnmvg/VbIpX5dhF0eAy1OTa4Z9xwP/BLo55xoBzYAfgCoFfV8rFq+gep3qVK9VncioSDp278jsGbPT1Zk9czZde3YFoF3XdiyYswDnHLNnzKZj945ERUVRrVY1qtepzorFK/LUZl4cSvISR0RYBBFhEaSkpHA0+Sjr964HYM6mOXSq1ylfbbao0YKv1n4FwMSVE+lY1+sg61CnAxNXep1aX639ihY1vG8R2tdtz9Q1UzmWcoxN+zexYe8Gzqp0Vo772PbHNuqcWYeoklGER4RTv0l9Fv9vMbM+n0Xn6zoTGRUJQEy5mEzbrl+xnko1KlGxekUiIiNo1rEZS2YvwTnHqgWraNKuifc+4luwZNYSAJbMXkKLeC/eJu2asPLnlRzvHMjF+XgI9XhDKdZgMLNwM1toZl/6y/X8i5RVZjbef/ISZlbCX17tr68bzLgDKU8Uj2P3rEpnsWHvBjbt30RSShJT10ylfd322daPPzWeKaunAN4oghLhJYgMiyQqLIqIsAh2HtqZrn7t2NqUL1WeBVsXFEq82Z33A1WoVoEap9ZINyIOwMw4dvQYSUlJJB1LIjkpmZjyXm45cvAI33z4DV1u6FIocaYKpeMhlGINxXgl/5Qnisexm9c8USaqTNq/CYneQx1zujaoXLoybWu3ZcKKCYUSZ6qC5Imt67dSv3F9wiPCKVGqBDXq1+DXeb8CMOvzWcTfGE9YmNcFkdW1yfEIpeMhlGINxXhDnTrniq9qwA7n3BEA59wO59zmQmw/CRiJlzQzehR40Dm3yd93snPuLefcbwXd6Y5tO6hctXLacqUqldixbUfmOtW8OuER4ZSJKcO+3fuy3TYvbeZFmIUx/vLxfHPjN8zdOJclCUuICIvgzIpnAtC5XmeqlqmaVv+cKufw4RUf8lr8a5xa7tRM7cWViGP/kf0ku2QAtiVuo3JpL87KpSuzNXErAMkumQNHDxBXIo4qpauw7cC2tDYCt8lOtXrVWLN4DYl7Ezl6+CjL5y5nz/Y9JGxIYM2SNfzrjn8x7N5hrF+xPtO2exL2EFfpzy8w4yrFsTdhL4l7EylVphThEeHpygH2JuxN2yY8IpySZUqSuDcxl99u1orz8RDq8YZSrEFyL97TlVINAV52zjUAdgO3+uW3Arudc/WBl/16xYXyBME/dgPP5wDbE7dTpXTW157VylSjRmwN5m2eB8Di7YuZv3k+X1//NV/f8DU/bPyB3/f8nm6bbvW7MW3NtEKJFbI/7+dFvUb1OK3JaTx++eM8dsVjnNHiDKrW8fLipLcm0aFXB6JKRBVarBBax0MoxRqK8cpxUZ4g+MduXvLE8J+G071Bd6ZfN53Xur3G8z88D5DjtUG/1v14+ceXSXEphRJnqoLkieqnVmf5vOUcPXyUA3sOsGrhKvZs3wPAjs07WPDNAl7s8yLD+w1n+8bthRJvKB0PoRRrKMYb6tQ5V3xNB2qZ2Uoze93MCueG/PReA3qbWdkM5Y2APH9Fb2Z9zOwnM/vpvZHv5Vg3qxFWGb9xyXIUluWvPGObeZHiUuj1aS+6jO3CWZXPon65+vSf0Z+HWj/E2MvGkngskeQUr6Pt1x2/Ej8unqs/uZr3l73Py11ezlMMDueHnf26vJanqlqnKn+79m+89uBrDO83nBqn1iAsPIyU5BQO7j/I/a/fz2W3X8bbT7ydpxFuZpb1Pi37eI7n9w3F+3jISijFG0qxnmhmVhPoDrzpLxvQEfjYrzIGuMx/3cNfxl/fyYrPm1aeSFcpOMdulufzbM618afG8/Xar9MupGrF1qJeuXp0GduFzu91pkX1FjSrmv42n66ndmXKmimFEmt28vq7SNiYwNY/tjL4o8E89dFTrFywktWLVrNx1UYSNiXQuG3jQo8tlI6HUIo1u1iKc7xyXJQn0lUqvnmiW/1uTPxtIl3GdaHvlL480+GZLLcD72/xC2tfyK5Du/h1x69Z1ilsef1dnHHeGZzZ8kxe7vsyY54aQ91GdQkL97ocko4mERkVyUMjH6LNxW0YN2RcocQWSsdDKMWaXSzFOd5Qp865Yso5dwBoDvQBEoDxZnZzIe9jH/AOcE92dczsbH+OiTVm1iubdkY65851zp17fZ/rc9xnpaqV2L71z29JErYlUKFyhcx1tnh1kpOSObD/ALFxsVluW7FyxTy1mR/7j+5n/ub5tKnVhsXbF3PLF7fQ+7PeLNiyIO0W18RjiWm3wc7aMIuIsAjiSqSfQmP34d3ElIgh3LzRZ1VKV0kbor4tcRtVS3ujDcItnDJRZdh7ZC/bErdRpcyf36QFbpOT1t1b0++//bj3lXuJjo2mUs1KlK1UlsZtG2Nm1DmjDhZmHNh7IN12cZXi2JOwJ215T8IeYivGUqZsGQ4dOERyUnJaedmKZTNtk5yUzOEDh4mOjc7jbze9UDgeQjXeUIq1sAX+ge//9MlQZSjQD0j9qrkCsMc5l+QvbwRq+K9r4N8C5K/f69cPOuWJ4nHsBp7PwRshsf1g1qMB4k+NT9fR1rFuR5ZsW8KhpEMcSjrE7A2zOafKOWnrTyt/GhEWUagXX9md9/Ni8azF1D2zLiWiS1AiugRntDyDdcvX8fvy39mwcgNP9HqCoXcPZfvG7bxy7yuFEm8oHQ+hFGsoxiv5pzxRPI7dvOSJng17Mm2tN0p68fbFlAgvQbmS5bK9NmhSpQnt67Rn8rWTGdJpCOfVOI9nOzxbKPEWJE8AdL2hK/1H9afvv/qCg0o1K6W12/hC70ucc9qew+a1hTOIM5SOh1CKNRTjDXXqnCvG/OHf3zrnBgF3AVcUwW6G4t22VTqgbBnevBA455b4c05MAUoVdGenn306m9ZtYsuGLRw7eoyZk2bSpmObdHXadGzDtAlecvpu2nc0bdUUM6NNxzbMnDSTo0ePsmXDFjat28Tp55yepzZzU65kOWKivHkPSoSXoFWNVqzbs47yJcsDEBkWyS1NbuHjX72BNRVK/XkCOavSWYRZGHuO7MnU7vzN8+l8ivd0o0tPu5Rv1n8DwLfrv017uETnUzozb5N3i9N3678j/tR4IsMiqRFTg9pla7M0YWmu8e/fvR+AXdt2sej7RTTv1JxzLjiHlQtXArB9w3aSjyVTpmyZdNvVblibhI0J7Nyyk6RjSSyYuYCz25yNmdGgaQN++e4XAOZNncfZ55/tvd82ZzFvqhfvL9/9QoNmDY77247iejz8FeINpVgLW+Af+P7PyNR1ZnYxsN0593PAJlkdwC4P64JOeSL4x+6yhGXULlubGjE1iAiLIP7UeL5b/12menXK1iGmRAyLti1KK9t6YCvNqzUn3MKJsAiaV2vO77v/vK21W/1uhT5qLrvzfl6Uq1yO1b+sJjkpmeSkZNYsWkOVOlVo26MtT3/yNE+Mf4L7/nMflWtW5p5h2V6n50soHQ+hFGsoxivHR3ki+MduXvLElgNbaFmjJQD14uoRFR7FrsO7sr02eGX+K3QZ14WL3r+I/jP6M3/TfAZ8M6BQ4i1InkhJTkmb7mbTmk1sXrOZ0889HSDdtcnqX1ZTuWbOU/fkVSgdD6EUayjGG+oigh2AZM3MGgIpzrlVflETYD2Q89MB8sk5t8vMPsRLqKnP9H4OeMnMejjnNvplBU6k4N2Hfs/Ae+j3936kJKfQ7Ypu1GtQj7eGvUXDsxpyfqfz6X5ld5596Fl6d+5NbNlYHn/5cQDqNahHh24duOWiWwgPD+fegfcSHu6NSsuqzfyoGF2Rp9s/TZiFEWZhTF87ne//+J5/tvwnF9a+kDAL48PlH6bNE9T5lM5cfcbVJLkkjiQdof+M/mltvRr/Kk9+/yQJBxMY+uNQXuj0An3P7cuKnSvSJmyd8NsEnunwDF/0+oJ9R/bRb0Y/ANbsXsP0tdOZcPUEklOSeXb2s3maR2LUwFEk7kskPCKcq+67iuiYaFpd1IpxQ8bx3M3PER4ZzvWPXI+ZsXfHXt5/8X1uH3I74RHhXHnvlbz+0OukpKTQqlsrqtWrBsCl/7iU0YNHM2nUJGo2qEmri1oB0Pqi1rz77LsMvm4w0bHR3Dzw5nz9rgMV1+PhrxBvKMV6gp0PXGpmFwElgVi8i4o4M4vwR8fVBFK/zt0I1AI2mlkEUBbYdeLDzkx5ongcu8kumedmP8fwbsMJCwvjs98+Y83uNdzZ/E6W7ViWdgGW1dxxX/3uPRDo4ys/xuH4YcMPfPfHnxdsXU7xbm8qTNmd9ye9NYnaDWtz9vlns37Fet587E0OHTjE0jlLmTJ6CgNGD/AeArRwJc//3/NgcEaLM/J8wVaQeEPleAilWEMxXsk/5YnicezmJU/8a+6/GHjhQK4/+3qccwz8diBw/NcGBVGQPJGclMzQe4YCUDK6JDc8ekPaHNZ/u+5vvPPMO3z70beUKFWCax+6ttDiDZXjIZRiDcV4Q53lZQ4qOfHMrDnwHyAOb7LV1XhD0j/Gm1z1J7/ezcC5zrm7MmyfqdzM1vllO8zsgHOujF9eBfgdeME594RfdhPwIBAO7AGWAoOcc+mfo53BZjaHzAHVbWS3YIeQLy9c8kKwQ8iXs6sV7QWbhJbqVC/QZBKNRzbO87llUZ9FedqXmbXHO59ebGYfAZ845z4wszeAxc65182sL3C2c+52M7sGuNw5d/XxvIfCpjxR9JQnipbyhAQqjnki1ClPFD3liaKlPCGpCpojoPjnCY2cK6b8W66yGt/ZPkO90cDoLLbPVO6cqxvwukzA621AdIa6Y/hzEnQRkb+6/sAHZvY0sBAY5ZePAt41s9V4I+auCVJ8mShPiIhITpQnRERChzrnRETkpOSc+xb41n+9FmiRRZ3DwFUnNDARERERETmp6IEQIiIiIiIiIiIiQaLOORERERERERERkSBR55yIiIiIiIiIiEiQqHNOREREREREREQkSNQ5JyIiIiIiIiIiEiTqnBMREREREREREQkSdc6JiIiIiIiIiIgEiTrnREREREREREREgkSdcyIiIiIiIiIiIkGizjkREREREREREZEgiQh2ACLB8sIlLwQ7hHzp90W/YIeQL1P6TAl2CCIiBaI8UbSUJ0Qk1ClPFC3lCTmZqHNORESKvXOqnxPsEEREpBhTnhARkZwU9zyh21pFRERERERERESCRJ1zIiIiIiIiIiIiQZJr55yZlTazMP/1aWZ2qZlFFn1oIiISCg4mHiQlJQWANSvXMH3idI4dOxbkqEREpLhQnhAREclZXkbOfQ+UNLMawAzgFmB0UQYlIiKh4/ILL+fI4SNs2bSFXp16Mf7t8fzz5n8GOywRESkmlCdERERylpfOOXPOHQQuB/7jnOsJnFm0YYmISKhwzlEquhRTPp3C/939f4yaMIqVy1cGOywRESkmlCdERERylqfOOTNrDfQGJvllesqriIgA3kXXT3N+4tOxn9KpeycAkpOSgxyViIgUF8oTIiIiOctL59x9wCPABOfcMjM7BfimaMMSEZFQ8eTQJ3n1uVfp1rMbDRs1ZP3a9bTp0CbYYYmISDGhPCEiIpKzXEfAOee+A74D8B8MscM5d09RByYiIqGhdbvWtG7XGoCUlBTKVyzPU688FeSoRESkuFCeEBERyVlentY6zsxizaw0sBz4zcweKvrQREQkFPS9ri/79+3nYOJB2p/ZngsbXsjwF4cHOywRESkmlCdERERylpfbWs90zu0DLgMmA7WBG4o0KhERCRkrl68kJjaGqZ9NpeNFHZn3xzw+efeTYIclIiLFhPKEiIhIzvLSORdpZpF4nXOfO+eOAa5owxIRkVCRdCyJY8eOMfWzqXTt0ZXIyEiwYEclIiLFRajlCTOLN7PfzGy1mT2cTZ2rzWy5mS0zs3EnOkYREQmeosgTeemcGwGsA0oD35tZHWBffgIXEZG/ruv/cT2t6rbiUOIhWl3Yio3rNxITGxPssEREpJgIpTxhZuHAa0A34EzgWjM7M0OdBngPzDvfOdcI7wF6IiJyEiiqPJGXB0K8ArwSULTezDrkI3YREfkLu/WeW7n1nlvTlmvWqclH33wUxIhERKQ4CbE80QJY7ZxbC2BmHwA98ObeTnUb8JpzbjeAc277CY9SRESCpUjyRK6dc/7OugONgJIBxYPzFrdIevO+n8erz7xKckoy3a/qznV9rku3/ujRozzX7zlWLltJbFwsg14eRNWaVQEYO2Iskz+eTHhYOHc9dhct2rbIU5u5GTtkLMvmLCMmLoZHRj8CQOK+REY/OZpdW3dRvmp5bnniFqJjopnxwQx++uonAFKSU9j6x1ae/exZSseWTtfmzi07GT14NAf3HaTmaTW5YcANRERGcOzoMd577j02/LaB0mVLc/PAm6lQrQIA08dOZ+6kuYSFh3HF3VdwRoszso05zMJ4v+f7bE/czt3T7ubZDs/SqFIjklKSWJqwlKe+f4okl5RWv1GlRrzb4136zejH179/nam9MyqewVPtn6JEeAlmbZjFkB+GABBbIpYXOr1A9ZjqbN6/mYe+foj9R/cD0L9Nfy6odQGHkw7z+LePs2Lninz93oc8MoS5384lrkIcb3/5dqb1zjn+88x/+PG7HylZsiT9n+/PaY1OA2DqhKm8N/w9AK6/43rie8YD8NvS3xjyyBCOHD5Cy3YtufvRuzErvHtniuPxWxxi/XrS16xctpIjh4+ktf/Pgf8slPchJ59Q+pwBLP9xOZ+++ikpySm07t6azr07p1s/88OZzJk0h/DwcMrEleG6ftdRvmp5AF5/6HXWL1/PKWefwj+e/0faNuNeGMcfv/0BDirVrMT1D19PiegShRJvm5pt6N+mP2EWxoQVE3hr0Vvp1lctXZWnOzxNTFQMYRbGsHnDmLVhFmVLlOVfnf9Fo0qNmLhyIs/Nfi5T28O6DqNmTE2u+PiKQokVCv942L5lO8/1e45dO3ZhYcbFV1/MlTddWSxjzUuboRJvccoTZtYH6BNQNNI5N9J/XQPYELBuI9AyQxOn+e3MBsKBJ5xzU4soXMlCqH3WCpInfpz6I9PfnQ5Alxu60DLeOxy/fPNL5k2bx8H9B3lp6kuFFivknicebP0g51U7D4BSEaUoV6ocbce0BeC+FvfRtrb3euSCkUxbOw2Aty95m+jIaADKlyrP0oSl/HN6wc8BoZQjiiLevLR5MsVbWIKRJ/LytNY3gF7A3XizQ1wF1MltO0nPzB717zVebGa/mNk3/r+rzWyv//oXM2vj169kZsfM7B8Z2llnZp8ELF9pZqP91zebWYKZLTSzVWY2LbU9f/1oM7vSf/2tmf0UsO5cM/s2YLmFX2eVmS0ws0lmdnZBfw/JyckMGzyM5998ntGTRjPjyxmsW70uXZ3JH00mJjaGsV+N5aqbr2LESyMAWLd6HTMnzeTtSW8z5M0hDHtyGMnJyXlqMzct41tyxwt3pCv7etzXnNbsNB4f+zinNTuNr8Z9BUCnazrRf1R/+o/qz8V9LqZ+4/qZOuYAPh/xOe2vbM/jYx8nukw0cybPAWDu5LlEl4lm4LiBtL+yPRNHTgRgy7otLJi5gEdGP8IdL9zBh0M/JCU5JduYe5/Vm7V71v75e1s9mR4f9uCKj6+gRHgJep7eM21dmIVxX4v7+GHjD9m299gFjzH4+8FcMv4SasfW5vxa5wPwf03+j3mb5nHp+EuZt2ketzbxvvm+oNYF1I6tzSXjL2Hw/wbzWNvHcvwdZyX+8niGvDkk2/U/fv8jm9Zt4r3p7/HAUw/w8hMvA7Bvzz7eefUdXv/wdYZ/NJx3Xn2H/Xu9DsOhTwzlgcEP8N7099i0bhPzvp+X77iyU1yP32DH2v/2/kwcP5G3/vMWzjm+/OhLNq7fWOD3cLJRnvCE0ucMvC9pPhr2EbcPuZ0BYwbw88yf2bJuS7o6NRvU5KERD/HwWw/TuF1jPh/xedq6Ttd04vpHr8/Ubs++PXl41MM8/NbDlKtSju8nfF8o8YZZGAMuGMCdU+6k50c9ia8fzylxp6Src1uz25i2Zhq9Pu1F/xn9GXDBAACOJh/ltfmv8e+5/86y7U51O3Hw2MFCiTNVURwP4eHh3PHwHYyZMobXx7/O5+M+D7nzbmE4mfOEc26kc+7cgJ+RAauz+kYv43zbEUADoD1wLfCmmcUVTbR/Up7whNpnrSB5InFfIlPHTOX+4ffzwBsPMHXMVA7u986zjVo34oE3HiiUGAPlJU+8NOclen3ai16f9uL9Ze8zc91MANrWasvpFU/n6k+u5vrPruemxjdROtK7Trrli1vStlm8fTEzfp9R4FhDKUcUVbx/lTxR3AQjT+Rlzrk2zrkbgd3OuSeB1kCtPGwnPjNrDVwMNHPOnQP8DejtnGsC/B34n3Ouif+T2ntyFTAX7z8yo3PNrFE2uxvvnGvqnGsAPA98ambZDb+qbGbdsoi3CvAhMMA518A51wx4Djg1b+84eysWr6B6nepUr1WdyKhIOnbvyOwZs9PVmT1zNl17dgWgXdd2LJizAOccs2fMpmP3jkRFRVGtVjWq16nOisUr8tRmbuo3rk90THS6siWzl9Ai3uvdbxHfgiWzlmTabsGMBTTv1DxTuXOOVQtW0aRdk0zbB7bbpF0TVv68EuccS2YvoVnHZkRGRVKhWgUq1ajE+hXrs4y3cunKtK3dlgkrJqSVzdowK+310oSlVClTJW352kbX8vXvX7Pr0K4s26tYqiKlo0qzePtiAL5Y9QUd63YEoEOdDkxc6XUgTlw5kQ51vbvaO9TtwBervvDe0/YlxETFULFUxSzbz07j8xoTWzY22/WzZ8ymy2VdMDPObHImifsS2bl9J/Nnzaf5+c2JjYslpmwMzc9vzrz/zWPn9p0kHkikUdNGmBldLuvCrBmzsm0/v4rr8RvsWH/+4WdeeecV4srFcf+g+5k4ZyKbN2wu8Hs4mShP/CmUPmcA61esp1KNSlSsXpGIyAiadWzGktnp88VpTU8jqmQUAHXPrMuehD1p6xo2b0jJUiXJqFTpUoCXT44dOVZok+efVeksNuzdwKb9m0hKSWLqmqm0r9s+U70yUWXS/k1ITADgUNIhFm5byJHkI5nql4ooxQ3n3MB/F/y3cAL1FcXxUKFyhbRR2NFloql9Sm12bNtRLGMtymNXeSJbG0l/rVMTyBjsRvwH5Tnnfgd+w7sIKzLKE38Ktc9aQfLEivkraHhuQ0rHliY6JpqG5zbk13m/AlCvUT3KVihbKDEGymueSBV/ajxTVk8B4JRyp/Dzlp9JdskcSjrEyp0r077wTxUdGU2L6i34Zt03BY41lHJEUcX7V8kTIaZI8kReOucO+f8eNLPqwDGgXp5CllTVgB3OuSMAzrkdzrnc/iK5FngAqGlmNTKsewkYkNtOnXPfACNJPxwz0ItAVsOd7gLGBCR2nHOznHOf5bbP3OzYtoPKVSunLVeqUinTyW7Hth1UrubVCY8Ip0xMGfbt3pfttnlp83js37U/LeGVrVCW/bv3p1t/9PBRfp33K40vbJxp28S9iZQqU4rwiHAA4irFsTdhLwB7E/YSVyku7f2VLFOSxL2J7E3YS7lK5dLaiKsUl+4CLlC/1v14+ceXSXGZR9ZFWAQXN7iY2Ru8k1zl6Mp0rNuRj37Nfm6XyqUrs+3AtrTlbYnbqBzt/U7LlyrPjkPe73PHoR2UL1U+rd1M25SuTGHK+H9bsWrFHP/Pd2zbQaWqlf4sr1o4x0J28RTn4/dExprasVAyuiRbN28lIjKCP37/o8Dv4SSjPOELpc8ZwJ6EPWnndEh/vs/K3ElzObPFmdmuDzT2+bE8dvljbPtjG+0ub1fgWME7329N3Jq2vD1xO1VKV0lXZ/hPw+neoDvTr5vOa91e4/kfns+13b7n9eWdxe9wOOlwocSZqiiOh0BbN25l9a+rOaNx9tNIBDPWojx2lSeyNR9oYGb1zCwKuAaYmKHOZ0AHADOriHf70lqKlvKEL9Q+awXJE3sS9uT5+qCw5CVPpKpWpho1Ymswb7N3p0pqZ1zJ8JLElYjjvOrnUbV01XTbdKzbkR83/UjiscQCxxpKOaKo4v2r5IkQUyR5Ii+dc1/6w+9eBBbgPbn1g3yFLtOBWma20sxeN7Mc/8I2s1pAVefcPLxvnHplqPIh0MzM6udh3wuA07NZNwc4Ypkf8NHI3y5PzKyPmf1kZj+9N/K9HOs6l3G0J5nmA8uqDpa/8oxtFoWlPyyl3ln1sryl1WUa1UraqIes1plZtuUZXVj7QnYd2sWvO37NMq4BFwzg5y0/s3DrQgAeavMQQ+cNzbIjL6f9ZPke0m2UuSjXbfIpv//nRX0shNLxeyJj7XRxJ/bu2csdD91BfLN4WtVtRY9rehx37Ccp5QlfKH3OspNd2/Onz+eP3/6g4zUd89RO74d789THT1G1TlUWfJPnX3fOsWVx8s74++lWvxsTf5tIl3Fd6DulL890eCbL7VI1rNCQ2rG1025rKkxFcTykOpR4iIH3DKTvgL6ULpM5l+dXqB27yhNZc84l4XUsTQN+BT50zi0zs8FmdqlfbRqw08yWA98ADznndhZxaMoTvlD7rGWlIHmiqK9z8pInUsWfGs/Xa79Ou86Ys2kOszbMYkyPMTzf6XkWbVuUbg5sgG6ndmPKmimFEmso5YjsYinOx26oxXuiFFWeyMvTWp/yX35iZl8CJZ1z2Xf1SybOuQNm1hxoi9d7Ot7MHnbOjc5mk2vwEiZ4HaGjgMAJXpLxOksfAXI7s/0/e3cep1P5/3H89TEY2wxhZF8qkaVIJBRG9pBfG5FIKe3K0l60L9+kbxtfqbQRbbKXQREhWSKVUIyxZ19mcf3+OPeMe8x2MzPuuXk/H495zH2uc51zf+4zZ85n5jrXua6szvRn8O52DclwB2Y/AZHATNsSh8AAACAASURBVOfcfcevd97z16MANrM509aZqLJRbNtybKKS7Vu3U6pMqbR14rYRVTaKpMQk9u/bT2SJyHS3LV3Ge4wyq32ejIiSEezZuYfipYqzZ+ceIs6KSLV+aUz6j7QCFCtejEP7D5GUmERY/jB2b99N8dJeL7zkO15nlTmLpMQkDu8/TJHIIpSIKsG/2/9N2Yf/Nv7qnV2PFlVa0KxyM8LDwilasCjPtXyOR2Y/wu0X385Zhc/i6ZlPp9SvXbo2L7byxnU7q9BZXF75cpKOJjH772Ndybfu35rqMdizi57N9oPeY0y7Du2idOHS7Di0g9KFS6c8GrvtwDZvm61+2/gefcopx//Md2zZQekypYkqG8WyRctSyrdv3U69RvWIKhvF9i3HYti+JWfOhYziycvn76mMtcftPQDoeE1HrrzqSo4cPpLp48qSlvLEMaH0ewZpezHs3r6byNJpz//fl/zOzI9mcu+IeylQsEDA+88Xlo/6LesTMy6Gxu0bZzverQe2purFUKZoGbYdTD2BWNcaXek/zRuHdcW2FYSHhXNWobPYdTj9oREuLHMhF5S+gKndp5Lf8lOycElGXzWaWyffmu14c+t8SExI5Il7n+DKTldyRZsrsh1nbsaaW+eu8kTGnHNTganHlT3h99oBD/i+TlVMyhM+ofa7lp08USKqBH8u+zPVttXr5eoT1AHliWTtzm3Hc/OfS1U2+pfRjP5lNADPRz/PP3uO9ZItHl6cOmXqMODbnJkMJpRyRG7GezrkiVCTG3kiw55zZvZ/x38BHYFWvtdyApxzSc65Oc65J/FaWTObxqw70NvMNuB1j7zIzI6/Cn8IXAFUzuKt6+O15mYUVwzeLLz+f/GvAi72q3Mp8DiQ7UENatatSeyGWOI2xpEQn0DMlBiaRDdJVadJdBNmfOnN6jN3xlzqN66PmdEkugkxU2KIj48nbmMcsRtiqXlhzYD2eTLqNKnDouleF+1F0xdRt+mx8WsP7T/E2uVrU5X5MzOq16/OsrnL0mzvv99lc5dR/eLqmBl1m9RlacxSEuIT2Bm3k+2btlOlZtq5V15f/DptPmlDh087MGTWEBbHLuaR2Y/QtUZXmlRswkOzHkrVg63DuA50+NT7+nbdtzw779lUDXPgPa56IP4Adct4MXaq3illHIg5f8+h8/neDYDO53dO2XbOhjl0qt4JgLpl6rI/fn/K4685pUl0E2Z+NRPnHKuXraZoRFFKlSlFw2YNWTJvCfv27GPfnn0smbeEhs0aUqpMKYoULcLqZatxzjHzq5k0bdU06zcKUCidv6ci1k9Gf0JCQgJTv5ia8jVryizmzZrH1C+mphdWnmBmhcxskZktN29g7aG+8mpm9pNv4Orxvm7qmFm4b3mtb33V3IhLecITSr9nAJVrVGb7pu3sjNtJYkIiS2OWUrdJ6tyw8c+NjHt1HLc9d1uaGz3pcc6xfdP2lNerflzF2ZXTf6ToRK3avorKxStTIaIC+fPlp9257Zj799xUdeL2x3FpBW/SsWolqlEwrGCGDXMAE36bQOuPW9Ph0w70ntSbv/f8nSMNc5A754NzjpcefYkq51Th+j7X50icuRVrbp67yhOhR3nCE2q/a9nJEzUb1mTN4jUc3HeQg/sOsmbxGmo2zKgTY84IJE8AVClehYjwCJZvXZ5Sls/yUTzc+xFXL1md80uez4JNC1LWtzmnDd//8z3xSfE5Emso5YjcijfU80ROxhvqMus51ymTdQ74IodjOW2ZWQ3gqHMu+bZHPSDdkf59dYs65yr4lQ3Fu/uV0h3KOZdgZsOBh4B0nyPxdXfvh+9Z50w8C7zDsWeg3wR+MrMZfuNEFEl3yxMUlj+Me5+4l8G3DuZo0lHaX9OeatWrMWbEGGrUqUHTVk3peG1Hnhv0HD1a9yCyeCSPD38cgGrVq9GyfUv6dOhDWFgY9z1xH2Fh3phu6e3zRLw/7H3WLlvL/j37efzax+nQpwOtb2zNe0PfY+HUhZx19ln0eapPSv0VP6yg5iU1CS8cnmo/7wx5h+6DulO8dHE6396Z94e9z5R3p1CxekUad/D+Xrmsw2V8+NyHDLtxGEUii9D7id4AlKtWjvot6vNc7+cICwvjuvuvI19YIE+eex67/DHi9scxtstYAGI2xDBy6chMtxn/f+O54QvvKYdn5z3L0y2eJjx/OPM3zk+ZYGLMsjG8fOXLXF3zarbs38LA7wYC8MPGH2hWuRmTu03mcOJhnpjzRIbvk5GnH3iaZYuWseffPVx3xXX0vqc3SYlJAHTu3pnGzRvz09yf6Nm6J+GFwxnynHdDNrJEJDfdeRN3XHsHAL3u6kVkCe8O5ICnBvDCwy8QfzieRlc04tIrjp/V+uTl1fM3WLGG5Q9j+c/L4ee0729mdPi/Dtn+HLnkCBDt64VQAJhnZtPw7mwNd86NM2+m8r7A277v/zrnzjOzbsCLpH08KFuUJ44Jpd+z5Hivve9a3hr0FkePHqVx+8aUq1aOKWOmULlGZeo2rcvXb39N/KF43nvyPQDOOvss+j3nDd/02j2vsfWfrcQfiufxax/nxsE3UuOSGnz0wkccPnAYHJQ/rzzXD8iZfxCSXBLPz3+et9u/Tb58+fjq96/469+/uLPBnazasYq5f8/lPwv/wxNXPEHPuj1xzqW6vk/tPpViBYpRIKwALau05I6pd6SaOTyn5cb5sHLJSr79+lvOOf8cbu3iNSLe+sCtNG6evZ6JoXjuKk+EDuWJY0Lxd+1k80TRyKK07dWWV25/BYB2N7dLGVLn63e+Zsl3S0g4ksDj1z7OZR0vo0Of7P9OBZInwBsCYcZfM1Jtmz9fft7r7H2GA/EHeGT2IyS5pJT1bc9ty5hlY7IdY7JQyhG5FS+Edp7IyXhDnWX0/LjkHF8X9P8CJYBEYC3Qzzm3w8xaAAOdc1f56j6F9+jwQ37bXwiMc87V8t39usS3bTiwHq97eG8z643XPT0WL/mtB4Y55+b79vM+MNk5N9G8ac4HOueW+Nb9DOxzzrXwLTfG+we0ArAN2OHbV8p06enJqht6XrIyLu3sq3nZ4G8GBzuEEzKtX86MJSGnh/KUz9ZgEjdNvinga8uHV30Y8HuZWRFgHtAfmII3Pk+iebPiPeWca2tmM3yvF5hZfmALEOVyMIEqT+RNyhO5S3lC/OXVPJFXKE/kTcoTuUt5QpJlN0dA3s8TGfacM7MHgD3OuXePK78HCHPOvZbbwZ0unHM/A+n21XTOzQHm+C0/lU6dFUAt3+uqfuVHgPJ+y+8D72cSR2+/1y2OW9fguOWFQM5MDScip6WRr44ksngk3ft2T1U+5r9jSEpK4rb7bwtKXGbWj9Szyo3yjWXjXycMry/HeXh39/8CdvsGeAVv+vPkHgcVgI3gDQBrZnuAUnj/ZOQI5QkROR3l1TwRipQnREROb5k9M3cL3jgExxvlWyciImew8WPGc81NaYe76dGvB+PHjA9CRB7n3Cjn3CV+X6PSqZPknKsHVAQaARektyvf9/TunIXMXX0RkWDJq3lCREQkr8mscc4559KM1Oi7uxJyXcFFRCRnmRkFCxZMUx4eHp7+9Ol5kHNuN15vg8ZACd9jq+A12m32vd4EVALwrS8OZDw6voiIAKdHnhARETkVMh1t3szSTA2WXpmIiJyZtm/dHlBZXmJmUWZWwve6MHAl3ix0s4FrfdVuBr72vZ7kW8a3PiYnx5sTETmdhWKeEBEROdUya5x7GZhiZs3NLML31QL4BnjllEQnIiJ51h2D7qBXx14smLuA/fv2s3/ffn6c8yO9O/Xm9oG3Bzu8zJQDZpvZCmAx8K1zbjIwBHjAzNbijSmXPObqu0ApX/kDeLPaiYhIFkI4T4iIiJxSGU4I4Zwba2bbgWFAHbzxdVYBTzrnNG2KiMgZ7rpe11EqqhSvPPEKa35dg5lRo3YNHhz6INHto4MdXoZ8g2LXT6d8Hd74c8eXHwauOwWhiYicVkI1T4iIiJxqGTbOAfga4dQQJyIi6YpuH61/sEREJEPKEyIiIlnLdMw5ERERERERERERyT1qnBMREREREREREQkSNc6JiIiIiIiIiIgESYZjzpnZA5lt6Jx7NefDERGRUDHy1ZGZrr/9Ac3EJyJyJlOeEBERCUxmE0JEnLIoREQk5BzYdyDYIYiISB6mPCEiIhKYDBvnnHNDT2UgIiISWh54MtMO1iIicoZTnhAREQlMZj3nADCzQkBfoDZQKLncOXdLLsYlkuvqlqsb7BBOyLR+04IdwgmpMLRCsEM4IbFPxgY7hJB1+PBhxr07jt9X/c6Rw0dSyl8do9EPJLQpT+Qu5Ykzh/KEnK6UJ3JXKOUJ5QjJriwb54APgTVAW2AY0AP4LTeDEhGR0HHvTfdyXs3zmDtjLvc/cT9ffvwl1S+onqPvcWG5C3N0fyIicuooT4iISLDl9TwRyGyt5znnHgcOOOc+ADoCoXWLQEREcs2GtRsY/PRgihQtwvU3X8/YKWP5baXu4YiIiEd5QkREJHOBNM4l+L7vNrM6QHGgaq5FJCIiIaVAgQIARJaIZM2va9i3Zx+bNmwKclQiIpJXKE+IiIhkLpDHWkeZ2VnA48AkoBjwRK5GJSIiIaNHvx7s/nc3g54eRJ/OfTiw/wADhw0MdlgiIpJHKE+IiIhkLsvGOefcaN/LucA5uRuOiIiEmhtvvRGAy5pfxoJ1C4IcjYiI5DXKEyIiIpkLZLbWcOAavEdZU+o754blXlgiIhIqjhw5wtTPp7Jxw0aSEpNSygc8MSCIUYmISF6hPCEiIpK5QB5r/RrYA/wMHMmiroiInGFu6XILEcUjuLDBhRQMLxjscEREJI9RnhAREclcII1zFZ1z7XI9EhERCUlxm+L4ePrHwQ5DRETyKOUJERGRzAUyW+uPZlY31yMREZGQdEmTS/ht5W/BDkNERPIo5QkREZHMBdJzrhnQ28zW4z3WaoBzzl2Yq5GJiEhIWDRvEZ+9/xmVqlUiPDwc5xxmxncrvgt2aCIikgcoT4iIiGQukMa59rkehYiIhKyPpn0U7BBERCQPU54QERHJXIaNc2YW6ZzbC+w7hfGIiEiI2Ld3HxGRERSNKBrsUEREJA9SnhAREQlMZj3nPgGuwpul1eE9zprMAefkYlwiIpLH3XXjXYydPJb2DdpjZjjnUtaZGQvWLQhidCIiEmzKEyIiIoHJsHHOOXeV73u1UxeOnAkWfb+IN559g6SjSXS8riM39rsx1fr4+HieH/w8f6z6g8gSkTw5/EnKViwLwMcjP2bqxKmE5Qvj7sfuptHljQLap+INfrz5LB9LbltC7L5YOn3aife6vEfzKs3Zc2QPAL2/6s3yrcsZ2GQgPer2ACB/vvxcUPoCol6O4t/D/6baX9USVRl3zThKFi7J0ril3PTlTSQcTaBgWEHGXj2WBuUbsPPgTm6YeAN/7/kbgIeaPUTf+n1JOprEvdPvZeZfMzONOf5IPPf1uI/4+HiSkpJo3rY5fe7tg3OOd197l7nT55IvXz46d+/MNb2uSbP99C+n89Hb3qM8Pfv3pF1Xb+Lr33/9nRcffpEjh49wafNLuefRezAz9u7ey7ABw9gSu4WyFcry5GtPElE84oSPdWaxp6qTA+cCwML1C08qRpGM5NXrmOINTrxtz23LiHYjCMsXxuilo3lx/oup1lcuXpkxnccQVTSKXYd20fOLnsTui01ZH1Ewgt/u+o0v13zJPdPuAaBbnW480uwRHI7N+zbT84ue7Dy0M9uxnuyx/XbSt4x/d3xKvXW/r2PUl6M474LzUsoeveNRNm/azHuT38t2nNmNF9I/F/5Z9w/DBgxL2X7n5p1MfH+i8oTkuFC7jine3Iv3ZHPERWdfxNsd3yYyPJIkl8SzPzzLZ6s+A+Cuhndxf+P7Oa/keZR+qXSO5IdkuZEnBvcdzM7tO0lKSuLCBhdy35P3ERYWFtR4IeNzYf/e/bz82Mus/2M9Zsbg5wZTu37tHIk3lGU5W6uZXZzO17lmFsh4dZIJMxtuZvf7Lc8ws9F+y/8xswd8rweY2WEzK+63voWZTU5nv3PM7BLf66pm9qeZtfWvb2a9zeyomV3ot92vZlbV97qYmb1tZn+Z2S9m9rOZ3Zbdz5yUlMSIYSN4YfQLvD/lfWZNnsWGtRtS1Zk6YSoRkRF8/O3HXNf7Oka+MhKADWs3EDMlhvemvMeLo19kxNARJCUlBbRPxRv8eO+79D5+25F6prZB3w6i/sj61B9Zn+VblwPwyo+vpJQ9POth5v49N03DHMCLV77I8IXDOf+N8/n38L/0vbgvAH3r9+Xfw/9S/b/VGb5wOC9e6SXoC0pfQLfa3aj9Vm3afdyOtzq8RT7L/BJYoGABXv3gVd6d9C6jvxrNoh8WsXrZaqZ/MZ1tcdv4YNoHfDDtA6I7RqfZdu/uvYx9YyxvffYWb094m7FvjGXfHm+UgNeeeo0Hhz3IRzM/InZDLIu+XwTAJ6M+4eLLLuajmR9x8WUX88moT07wKGcdu7+cOhdWLl2Z5mvDXxtITEw86fjlzMwRkLevY4r31Mebz/LxZoc3af9xe2q9WYvudbpzQekLUtV5pfUrjF0xloveuYhhc4fxfKvnU61/Ovpp5v49N2U5zMIY0W4ELT9oyUXvXMSKrSu4u9Hd2Y41O8e2defWjP56NKO/Hs0jLz1C2QplUzXMfT/zewoVLZTtGHMq3ozOhcrnVE75HCO/GEl44XCatW6mPJFLlCdC4zqmeHMv3uzkiIMJB+n1VS/qvF2Hdh+147W2r1E83Pv1mL9xPleOvZINu7Mfo7/cyhNPjniSdye9y3uT32P3v7uZO33u8W99yuPN6FwA+O+z/6XR5Y0YO30so78eTZVzq+RIvKEuy8Y54C1gITAK+J/v9TjgDzNrk4uxnQl+BJoAmFk+oDTg32TcBJjve90dWAx0DXTnZlYRmAE86JybkU6VTcCjGWw+GvgXqO6cqw+0A0oG+t4ZWbNiDeWrlKd8pfIUKFiA6I7RzJ81P1Wd+THzadu1LQDN2zZn6YKlOOeYP2s+0R2jKViwIOUqlaN8lfKsWbEmoH0q3uDGWyGiAh2rd2T00tFZV/bTvU53Pv3103TXRVeLZuLqiQB8sPwDrq5xNQBdanThg+UfADBx9URandPKK6/ZhXGrxhGfFM+G3RtYu2stjSo0yvT9zYzCRQsDkJiYSFJiEhhM+nQSN991M/nyeZfQs0qdlWbbxfMW06BpAyJLRBJRPIIGTRuw6IdF7Ny2kwP7D1C7fm3MjDZXt2HerHkA/DjrR9pe7f1s2l7dlvnfnfx5kVHs/nLqXHj4zofp1LgTg/sNZtBtg+jUuBN3druTy8+/nLkzc+aPgzPUGZcjIO9exxRvcOJtVKERa3etZf3u9SQcTWDcqnF0qdklVZ1aUbWYtW4WALM3zE61/uJyF3N20bNT9ZQ2MwyjaEFvHLTI8Eg279uc7Vizc2z9zZoyi+irjt30OXTgEBPem8BN/W/Kdow5FW9G54K/pQuWUr5SecpWKKs8kXuUJ0LgOqZ4cy/e7OSIP3f9ydpdawGI2x/HtgPbiCoaBcCyLctSnrzJSbmVJ4oW8/JZUmISiQmJaf7mD0a8GZ0LB/YfYMXiFXS4tgPgdSgoFlksZwIOcYE0zm0A6jvnLnHONQDqAb8CVwIv5WJsZ4L5+BIqXiL9FdhnZmeZWThwAfCLmZ0LFAMew0usgSgLzAQec85NyqDOZKC2mdXwL/S9XyPftkcBnHPbnXMvprOPE7Jj6w7KlC2Tshx1dhQ7tu5IW6ecVycsfxjFIoqx99+9GW4byD4Vb3Djfa3dawz+bjBHvdMpxbPRz7L8juW82vZVCoYVTLWucP7CtDuvHZ+v/jzN/koVLsXuw7tJct7dl017N1EhsgIAFSIrsHHPRgCSXBJ7Du+hVOFSVIg4Vg6wad8mKkRUyDL2pKQkbu1yK12bdKVBkwbUuqgWmzduZvbU2dz+f7cz5NYhbNqwKc12mR3PqLJRx8rLHjueu3buolSZUt5nLFOKf3el7TF4ItKLPU2MOXAuVKpaiRm/zGDakmlM/3k6M5fNpEadGoz7bhzPDH4mW5/hDHfG5QjIu9cxxRuceCtEVGDjXr9r99601+7lW5dzTS1vaIGuNbsSGR5JycIlMYz/tPkPg74dlKp+4tFE+k/pz8r+K9n8wGZqRdXi3V/ezXas2Tm2/uZMnUOrjq1SlseMGMP1t1xPoUI523MuN84FfzFTYmh1lfc5lCdyjfIEef86pnhzL97s5Ah/Dcs3pGBYQf7a9Ve2Y8pMbuUJgEF9B9G1SVcKFy1M87bNgx5vRtvGbYyjRMkSvPjwi9x29W28/OjLHDp4KEfiDXWBNM7VdM6tSl5wzq3Ga6xbl3thnRmcc5uBRDOrjJdYFwA/AZcBlwArnHPxeEn0U+AHoIaZlclgl/7GAm845yZkUucoXgPrI8eV1waWJyfTrJhZPzNbYmZLPhr1UaZ1j2/1922fZR3sxMqP3+fJUrzZj7dj9Y5sO7CNpXFLU5U/POthar5Zk4b/a0jJQiUZ0nRIqvWdanRi/j/z032kNb33T47T0rlV5HDpb0M6n/k4YWFhjP56NBPmTmDNijWs/2M98fHxFAwvyMgvRtLx+o689Eja+xQZHbfc/PkHEntWMZ7MubB2zVpq1D72d/n5tc7n119+pco56qKeHadLjgDlCcV78gK5dg+cOZDmVZqztN9Smldtzqa9m0g8msidDe9k6p9T2bQ39Q2U/Pny0/+S/tQfWZ/yr5ZnxdYVPNzs4WzHmp1jm2z18tWEFw6n2vnekM9rf1tL7D+xXN768mzHd7zcOBeSJcQn8GPMjzRv5/2DqDyRO5QnUm2fZR1ddwMTSvFmJ0ckK1usLB92/ZA+X/cJ6H+D7MiNPJHs5Xdf5vN5n5MQn8AvC38JerwZlSclJvHH6j/o3L0z//vqfxQqXIhPR6X/pNSZJpDGud994wU09329hfdIaziQkMvxnQmS73glJ9QFfss/+up0A8b5EtwXwHUB7Pc74CYzK5JFvU+AxmaW4cQfZvaomS0zs3Sf+XDOjfL1rLykZ7+emb5ZVNkotm3ZlrK8fev2lJ5CqerEeXWSEpPYv28/kSUi0922dJnSAe3zZCne7MfbtHJTOtfozPr71jPu2nFEV4vmw64fsmX/FgDik+J5b9l7aR4x7Va7W4aPtO44uIMShUoQZt5ApxUjK6Y8krRp7yYqFa8EeOMKFS9UnF2HdqUqB6gYUfGEHmMqFlmMepfWY9EPi4g6O4or2lwBwOWtL2fd72nvVWR03KLKRrF9y/Zj5VuOHc+SpUqyc5s34OzObTs5q2Tax2VPhn/saWLMgXPh3Brn8lD/h1gwdwEL5i7g4Tsf5pzzz+HIkSMUKFAgRz7DGSzkcwQoTyjek7dp7yYqRfpduyPTXrvj9sdxzWfXcPGoi3l0lveE3d4je7ms4mXc3ehu1t+3nlfavEKvi3rxfKvnqVe2HgDr/vWu3Z+t+owmlZqQXdk5tslmT5mdahzTVb+s4o9f/6BbdDfuufEeNm3YxP033U9OyI1zIdlP3//E+bXPp2Rpr3dKqOUJM2tnZr+b2VozeyiTeteamUseny1IlCfy+HVM8eZevNnJEeBNGDTlxik8Nvsxfor9KdvxZCU38oS/guEFaRLdJMcecc6tcyGqbFTKEz3N2zXnj9V/5Ei8p1Ju5IlAGud6A2uB+4EBwDpfWQLQMpDAJVPJY0XUxeuKvhDvblcTYL5vkNXqwLdmtgEvuQbSHf0lvDtnEyyTyTucc4nAfwD/bkurgYt8Y1fgnHvWOVcPiExnFyekZt2axG6IJW5jHAnxCcRMiaFJdOo/iJtEN2HGl96wFnNnzKV+4/qYGU2imxAzJYb4+HjiNsYRuyGWmhfWDGifijd48T4y6xEqDa9EtRHV6DaxGzHrY7jpy5soW6xsSp2ra17Nr9t+TVmODI+kedXmfP371xnud/b62Vxb61oAbr7o5pS6k/6YxM0X3QzAtbWuJWZ9jFf++yS61e5GwbCCVC1RleqlqrModlH6O/fZvWs3+/fuB+DI4SP8/OPPVD6nMs2ubMbShV5PwOWLllOxasU02zZs1pAl85awb88+9u3Zx5J5S2jYrCGlypSiSNEirF62GuccM7+aSdNWTQHfz+Yr72cz46sZNGl18udFRrH7y6lzYfj7w6l6XlVGvzaa/w3/H1XOqcJr779GgQIFmDA7sxvuEoAzKkdA3ryOKd7gxbs4djHVS1WnaomqFMhXgG61uzHp99RP2JUqXCql1/TDlz/MmF/GANDzy55Uea0K1UZUY+DMgYxdPpaHZz1M7N5YakXVonQRrzGp9bmt00xYdDKyc2wBjh49ypzpc1L909Xlxi5MnDeRcTHj+O8n/6Vi1Yq89uFr2Y41u/FmdC4ki5kSk+pzhFKeMLMw4E2gPVAL6G5mtdKpFwHci3ctDSbliTx+HVO8uRdvdnJEgXwF+PKGLxm7fGzKONa5LTfyxKEDh1Ju7iclJvHT3J/S/M0fjHgzOhdKRpWkTNky/LPuH8Abn7TquVVzJN5TJbfyRJYzrjrnDuFdcP+Tzur9gbyJZGo+8CCwzjmXBOwysxJ43cFv8617yjmXMvWYma03s0CeAxiAdzfrXTPrnUm994HBQASAc26tmS0BnjGzx51zSWZWiBwYWjIsfxj3PnEvg28dzNGko7S/pj3VqldjzIgx1KhTg6atmtLx2o48N+g5erTuQWTxSB4f/jgA1apXo2X7lvTp0IewsDDue+LYFNHp7TMnKN7ci/fj//uYqCJRmBnLtizjjsl3pKzrWrMrM/+aycGEg6m2mXLjFG6ddCtx++MY8t0Qxl07jmein+GXuF9Sxgt6d+m7fNj1Q/685092HdpF61XxsgAAIABJREFUt4ndAFi9fTWfrf6M1XeuJvFoIndNvSvNGHjH27ltJy889AJHk45y1B2lRbsWXNbyMuo2qMszA59h4gcTKVykMAOfHQjA7yt/Z9K4SQx6dhCRJSK56c6buONa73P1uqtXyl2vAU8N4IWHXyD+cDyNrmjEpVdcCkD3ft0Zev9Qpk6cSplyZXhqxFMnfXwzij23zoU7HryDOx68I00cyQPUykk7o3IEhNZ1TPHmfrxJLom7p97NjJ4zCLMwxiwbw+rtqxnaYihLNi/hmz++oUXVFjzf6nkcju///p67pt6V6T7j9scxdO5Qvu/9PQlHE/h799/0/rp3tmPNzrEFWLF4BVFloyhfqXy2Y8nteDM7Fw4fOszPP/7MA8MeSHmvwoULh1KeaASsTR7Cx8zGAV3wGpz8PY3XgDXw1IaXhvJEHr+OKd7cizc7OeL62tdzRZUrKFWkFL3r9Qag91e9Wb51Ofc0uofBTQdTtlhZVvRfwdQ/p3LbN9mfbDg38sShQ4d4tP+jJMQnkHQ0iYsbX0znbp2zHWt24830XHj8Xp4d+CyJCYmUq1SOIc8PySyMvChX8oSl+yyw9wafOeeuN7OVkPbha+fchelsJifI1+r6L/C6c+4xX9n7wGXOuRpmth5o75xb47fNq8BWvBbYacBOv11eBzwPDHTOLTGzgniDtS4HpvjKr/Il2Eucc3f79nkvMAKo5pzbYGaRwMtAG2AXcAivO/wbmX2ezWzO3Qf1JWRUGJr1ZAt5SeyTscEOIeTcfv3tjPxsJK3qtkp3zI/vVnyX8ro85bP1B/nLP78c8LVlUINBuTOIXxCcbjkClCfkGOWJ019ezRODLxl8O9DPr2iUc24UeI8gAe2cc7f6lm8CLk2+HvrK6uNNdnCNmc3Bd03NTvwnS3lCTmehlCeUI3JXdnME5P08kVnPuft8368K9APIifPd4Yo8rqy33+s0txSccw/4LRZOZ7ct/OrG4yXFZHN85e/j3eVKrvc68Lrf8l7g9gA+goicoYaNGAbAB5M/CHIkpy/lCBEJZXk1T/j+wRqVwer0/gFM+YfO96jmcLxhfoJOeUJEJOcFI09kNn5AnO9OzLvOuStPZKciInL6O7vc2SQlJfFg3wcZ/934YIcjIiJ5TIjmiU1AJb/lioD/CPMRQB1gjq83YFlgkpl1DlbvOREROaVyJU9kOiGE707MQTMrfrJRi4jI6SssLIzCRQqzd8/eYIcSMDOrZGazzew3M1tlZvf5ykua2bdm9qfv+1m+cjOz132zMa0ws4uD+wlEREJHCOaJxUB1M6vme6SzG5Aywrxzbo9zrrRzrqpzrireBAxqmBMROXPkSp7IckII4DCw0sy+BQ74veG9J/EhRETkNBNeKJxWdVtxResrKFK0SEr5068/HcSoMpUIPOicW+qbRelnX47rDcxyzr1g3pToD+HNPtceb6a76sClwNu+7yIiEoBQyhPOuUQzuxuYAYQBY5xzq8xsGLDEOTcp8z2IiMjpLLfyRCCNc1N8XyIiImm06tiKVh1bBTuMgDnn4oA43+t9ZvYbUAFvlqUWvmof4I2rM8RXPtZ5MygtNLMSZlbOtx8REclCCOaJqcDU48qeyKBui1MRk4iI5B25kScCaZwbD5yHN8DdX865w4HsWEREzgydb+jMhrUbMDOqnFuFQoUKBTUeM+tHBrMrpVO3KlAfb8a6s5Mb3HzjrpbxVasAbPTbbJOvTI1zIiIByGt5QkREJK/JsHHOzPIDzwG3AH/jjU9X0czeAx51ziWcmhBFRCQvSkxM5IVHXmDcmHFUrFKRo0ePErcpjhv63MCQZ4dQoECBoMSVxexKKcysGPA5cL9zbq9vwNZ0q6b3NicfoYjImSGv5gkREZG8JrMJIV4GSgLVnHMNnHP1gXOBEsArpyI4ERHJu54e9DS7d+1m4fqFTP95OjN/mcmPf/3I3t17eXpg3htHyJ+ZFcBrmPvYOfeFr3irmZXzrS8HbPOVZzUjk4iIpCOU84SIiMiplFnj3FXAbc65fckFzrm9QH+gQ24HJiIiedt3k7/j5f+9TLGIYillEZERPP/288yaOiuIkWXOvC5y7wK/Oede9Vs1CbjZ9/pm4Gu/8l6+WVsbA3s03pyISNZCNU+IiIicapmNOed8g18fX5hkZnqcR0TkDGdmpPcoaFhYWLrleUhT4Ca8mciX+coeAV4APjOzvsA/wHW+dVPxbkqtBQ4CfU5tuCIioSmE84SIiMgplVnPudVm1uv4QjPrCazJvZBERCQUnF/rfCaMnZCm/POPPue8mucFIaLAOOfmOefMOXehc66e72uqc26nc66Vc6667/suX33nnLvLOXeuc66uc25JsD+DiEgoCNU8ISIicqpl1nPuLuALM7sF+Blv8OuGQGGg6ymITURE8rBn33yW2/7vNsaPGU/dBnUxM5YvXs7hQ4cZ/eXoYIcnIiJBpjwhIiISmAwb55xzscClZhYN1MabrW6ac04DRIiICOUqlGPyT5OZFzOPP1b9gXOOlu1bcnmry4MdmoiI5AHKEyIiIoHJrOccAM65GCDmFMQiIqeR2Cdjgx3CCWk/qn2wQwjYtH7Tgh1CKs2im9EsulmwwxCREBNqeaLC0ArBDiFgee3YKk+IyMnIa9eyzIRSjoDQOrZniiwb50RERILtwvIXBjsEERHJw5QnREQkM3k9T2Q2IYSIiIiIiIiIiIjkIjXOiYiIiIiIiIiIBIka50RERERERERERIJEjXMiIiIiIiIiIiJBosY5ERERERERERGRIFHjnIiIiIiIiIiISJCocU5ERERERERERCRI1DgnIiIiIiIiIiISJGqcExERERERERERCRI1zomIiIiIiIiIiASJGudERERERERERESCRI1zIiIiIiIiIiIiQaLGOTnlFn2/iF5te9GjdQ8+GfVJmvXx8fEMvX8oPVr3oP91/dmyaUvKuo9HfkyP1j3o1bYXi35YFPA+Fa/iPVH5LB/j/288/237XwC61e7GNzd8w/J+yykRXiKlXosqLZhwzQTG/994Pun6CfXPrp/u/i4ofQETr53INzd8w5AmQ1LKI8MjeafDO0y6YRLvdHiHiIIRKeuGNBnCNzd8w4RrJlCzVM0Tiv+fdf9wa5dbU746XtyRie9PTFXHOcfrz7xOj9Y96NupL3+s+iNl3fQvp9OzTU96tunJ9C+np5T//uvv3NLpFnq07sHrz7yOc+6E4hIJRF69LihexZuVtue2Zc1da/jznj8Z0nRImvWVIisR0yuGpf2WsvyO5bQ/r32a9fse3seDlz2YUnZ/4/v5tf+vrOy/kk/+7xPCw8JzLN6TPbZ7/t3DgJsG0L5+e0YMG5FS/+D+g6lyT5dLu/DGs2/kWLwiyULpuqB4Fa+/rPJE5eKV+e6m71h+x3Jm3zybChEVUta9eOWL/Nr/V1bfuZoR7Y5de6f1mMay25fxa/9febvj2+SznGvmUZ44ddQ4J6dUUlISI4aN4IXRL/D+lPeZNXkWG9ZuSFVn6oSpRERG8PG3H3Nd7+sY+cpIADas3UDMlBjem/IeL45+kRFDR5CUlBTQPhWv4j1RPer0YN3udSnLy7Ys4/YptxO7LzZVvZ9if+K6z6/jhi9u4Mm5T/Jk8yfT3d9jzR5j2PfD6DS+E5UjK9O0UlMAbql3C4tiF9F5fGcWxS6ib72+ADSr1IzKkZXpNL4Tw34YxmOXP3ZC8Vc+pzKjvx7N6K9HM/KLkYQXDqdZ62apY//+J2I3xPLRzI948OkHGf7UcAD27t7L2DfG8tZnb/H2hLcZ+8ZY9u3ZB8BrT73Gg8Me5KOZHxG7IZZF3y9K894i2ZGXrwuKV/FmJp/l480Ob9L+4/bUerMW3et054LSF6Sq89gVj/HZ6s+4eNTFdJvYjbc6vpVq/fC2w5n257SU5fIR5bm30b1c8r9LqPt2XcLyhdGtTrdsxwrZO7YFwwtyy3230H9w/1T1ixQrkpJ7Rn89mrMrnM3lbS7PkXhFkoXSdUHxKl5/geSJV1q/wtgVY7nonYsYNncYz7d6HoDLKl5G00pNufCdC6nzdh0alm9I8yrNAbh+wvXUG1mPOm/XIapIFNfVui7bsYLyxKmWa41zZjbczO73W55hZqP9lv9jZg/4Xg8ws8NmVtxvfQszm5zOfueY2SW+11XN7E8za+tf38x6m9lRM7vQb7tfzayq73UxM3vbzP4ys1/M7Gczuy2Tz1LVzA756v5mZovM7Obj6lxtZivMbI2ZrTSzq33lF5nZMr963c3soJkV8C3XNbMVfp9tiV/dS8xsju91ETP72LfvX81snplVMbNlvq8tZhbrt1zQt11XM3NmVtNvv1XN7Fe/47zH99nWmNkrfvXONrPJZrbczFab2dSMjlGg1qxYQ/kq5SlfqTwFChYgumM082fNT1Vnfsx82nZtC0Dzts1ZumApzjnmz5pPdMdoChYsSLlK5ShfpTxrVqwJaJ+KV/GeiDJFy3B55cv5cs2Xx2LduYbN+zenqXso8VDK68L5C6fbk6x04dIULViUFdtWAPDNn98QXTUagJZVWjLpj0kATPpjEi2rtvTKq7bkmz+/AWDltpVEFIygdOHSJ/Q5ki1dsJTylcpTtkLZVOXzZ82nzdVtMDNq1avFgb0H2LltJ4vnLaZB0wZElogkongEDZo2YNEPi9i5bScH9h+gdv3amBltrm7DvFnzTiqmE80Rnap14sDeAynbL/9xOU/0eiLNfgddMwjliNDNEZB3rwuKV/FmpVGFRqzdtZb1u9eTcDSBcavG0aVml1R1HI7I8EgAihcqzuZ9x/JKlxpdWLd7Hau2r0q1Tf58+SmcvzBhFkaRAkVSbZMd2Tm2hYsUpu4ldSkYXjDD/W/asIndO3dz4SUXZlgnM8oTyhMZCaXrguJVvP4CyRO1omoxa90sAGZvmJ2y3uEolL8QBcMKEh4WToGwAmw9sBWAffHeTfT8+fJTMKwgjpx5siWv54nTTW72nPsRaAJgZvmA0kBtv/VNgOSfbHdgMdA10J2bWUVgBvCgc25GOlU2AY9msPlo4F+gunOuPtAOKJnFW/7lnKvvnLsA6AYMMLM+vlguAl4BujjnagKdgVd8CX0lUMXMkp9VawKsAer7Lfuf4WXMLPUzDp77gK3OubrOuTpAX2CLc66ec64e8A4wPHnZORfv2647MM8Xc0Z+8B2H+sBVZtbUVz4M+NY5d5FzrhbwUBbHKEs7tu6gTNkyKctRZ0exY+uOtHXKeXXC8odRLKIYe//dm+G2gexT8SreEzH4ssEM/2k4R93RgOpHV43mq+u/4o12b/Dk3LQ958oULcPW/VtTlrce2EqZIl6MJQuXZMchL74dh3ZQsrB3KSpTJJ1tipbhZMRMiaHVVa3SlB9/rEqXLZ3pMdyxdQdRZaOOlZfN1rlwQjni/IvOZ/60wP8oUo4IzRwBefe6oHgVb1YqRFRg496NKcub9m5K9TgSwFNznqJn3Z5sHLCRqTdO5Z5p9wBQpEARhjQdwtA5Q1PV37xvM68seIV/BvxD3INx7Dm8h2/XfZvtWCF7xzYQsybPomWHlpjZyYaoPKE8ka5Qui4oXsXrL5A8sXzrcq6pdQ0AXWt2JTI8kpKFS7Jw00Jmb5hN3INxxD0Yx4y/ZrBmx5qU7ab3mM62gdvYF7+PiatTD2VzskIgT5xWcrNxbj6+hIqXSH8F9pnZWWYWDlwA/GJm5wLFgMfwLv6BKAvMBB5zzk3KoM5koLaZ1fAv9L1fI9+2RwGcc9udcy8G+sGcc+uAB4B7fUUDgeecc+t969cDzwODfO+xGLjUV7cB8CbHjk0TvD8+kr2MdyyOVw5IeZ7OOfe7c+5IZnGaWTGgKV7yzfIZCOfcIWAZkHyFKIf3h0ny+hVZ7SOA90gvzizrYCdWnlO/4Ir3zIv3ispXsOvQLn7b8VvA28RsiOHqz67m/pn3c9cldwX0/lne0Uon5JO5C5YQn8CPMT/SvF3ztPs7wWOYw+fCCeWIm4fczJyv5gS6b+WIEM0Rvv2kF2uWdXQdC4zizb14A7nWd6/TnfeXv0+l4ZXo8EkHPuz6IYYxtMVQhi8czoGEA6nqlyhUgi41ulBtRDXKv1qeogWL0qNuj2zHCtk7toGYPXU20R2jTya0ZMoTyhMZvU96sWZZR9exwCje4OaJgTMH0rxKc5b2W0rzqs3ZtHcTiUcTOfesc7mg9AVUfLUiFV6tQHTVaC6vfOxx0HYft6Pcf8oRHhZOdLVsXXuPxZb388RpJdca55xzm4FEM6uMlzQWAD8BlwGXACt8d2S6A58CPwA1zCyQriFjgTeccxMyqXMUeAl45Ljy2sDy5GSaDUuB5O7dtYGfj1u/hGN3934EmphZUV9cc0idUP3vdi0AjphZy+P2NwYYYmYLzOwZM6seQIxXA9Odc38Au8zs4swqm9lZQHXge1/Rm8C7ZjbbzB41s/IZbNfPzJaY2ZKPRn2UaUBRZaPYtmVbyvL2rdspVaZU2jpxXp2kxCT279tPZInIdLctXaZ0QPs8WYr3zIu33tn1aFGlBVO7T+XFVi/SsEJDnmv5XEDbLt2ylEqRlVJNGAGwdf9Wzi52dsry2UXPZvvB7QDsOrQr5XHV0oVLs+vQLgC2HdiWdpsD2wP+HMl++v4nzq99PiVLp72hf/yx2rFlR6bHMKpsFNu3HIth+5aTPxdONEfUubQOm/7axO4duwPZvXJEHsoRvm2VJxTvaR/vpr2bqBRZKWW5YmTFNI+g9q3fl89WfQbAwk0LKZS/EKWLlObSCpfyUuuXWH/feu5vfD+PXP4IdzW8iyvPuZL1u9ez4+AOEo8m8sVvX9CkUhNyQnaObVbWrllLUlISNerUyLJuRpQnlCcyEkrXBcWreP0Fkifi9sdxzWfXcPGoi3l0ltd5d++RvXS9oCsLYxdyIOEABxIOMG3tNBpXbJxq2yNJR5j0xyS61Ej9qOzJyut54nST2xNCJN/xSk6oC/yWk+/wdAPG+RLcF0Agoxd+B9xkZkWyqPcJ0NjMqmVUwZcolpnZiQ7gYce9Pr7J2L8s+Tg0AhY75/4CzjOzKKCY7+6Zv2c47o6Xc24ZcA7e3bCSwGIzu4DMdQfG+V6PI+OeiZebN1bFFmCyc26L7z1n+N7zf3h/PPziizkV59wo59wlzrlLevbrmWlANevWJHZDLHEb40iITyBmSgxNolP/kdkkugkzvvSeLpg7Yy71G9fHzGgS3YSYKTHEx8cTtzGO2A2x1LywZkD7PFmK98yL9/XFr9PmkzZ0+LQDQ2YNYXHsYh6Zffzf5cf4J9iapWpSIKwAu4+k/sdgx6EdHIg/QN0ydQHoVL0TszfMBmDO33PofH5nADqf35nZf/vKN8yhU/VOANQtU5f98ftTHn89ETFTYjK8I9Ukugkzv5qJc47Vy1ZTNKIopcqUomGzhiyZt4R9e/axb88+lsxbQsNmDSlVphRFihZh9bLVOOeY+dVMmrZqmu6+AxRwjsiXLx9NOzTl+2++T3dHx1GOyEM5wldXeULxnvbxLo5dTPVS1alaoioF8hWgW+1uTPo9daesf/b8Q6tq3jADNUvXpFD+Qmw/uJ0r3r+CaiOqUW1ENV5b+BrP/fAcby5+k3/2/EPjCo0pnL8wAK2qtTqhnt2Zyc6xzUrM5IxzzwlSnvAoT/gJpeuC4lW8/gLJE6UKl8J8l4eHL3+YMb+MAbz80bxKc8IsjPz58tO8SnN+2/EbRQsUpWwxb1zpMAujw3kdUj3umh0hkidOG/lzef/JY0XUxeuKvhF4ENgLjDFvHIXqwLe+H2BBYB3eXZbMvAT0BCaYWRfnXGJ6lZxziWb2H8B/juLVwEVmls85d9Q59yzwrJntP8HPVh9I/utoFb47eH7rL/a9F8BCoCHQDO+PCvC6eHcjdTf05LhjzOxpoPFx5fvxGjC/MLOjQAe/GFIxs1JANFDHzBwQBjgzG5xO9R+cc1eZ2fnAPDP70pfAcc7twvvD5BPzBsm9Avg8/UOStbD8Ydz7xL0MvnUwR5OO0v6a9lSrXo0xI8ZQo04NmrZqSsdrO/LcoOfo0boHkcUjeXz44wBUq16Nlu1b0qdDH8LCwrjvifsICwsDSHefOUHxKt5kN9a+kd4X9aZUkVJMuHYC8zbOY+j3Q7my2pV0qt6JhKMJHEk6wuDvjv2Kjf+/8dzwxQ0APDvvWZ5u8TTh+cOZv3E+8zZ6EymMWTaGl698matrXs2W/VsY+N1AAH7Y+APNKjdjcrfJHE48zBNz0g5qnZXDhw7z848/88CwB1LKJn3q/QHQuXtnGjdvzE9zf6Jn656EFw5nyHPepTKyRCQ33XkTd1x7BwC97uqVcgdswFMDeOHhF4g/HE+jKxpx6RWXkg0B54hejXqRmJBI2cpl6dync1b7VY4I0RwBoXVdULyK11+SS+LuqXczo+cMwiyMMcvGsHr7aoa2GMqSzUv45o9veHDmg/yv0/8Y0HgADkfvr3pnus9FsYuY+NtElt6+lMSjifwS9wujfh6V7Vghe8cWoFt0Nw7uP0hCQgLzvpvHy2Nepup5VQGYM20OL4x6ISfCVJ7wKE/4CaXrguJVvP4CyRMtqrbg+VbP43B8//f33DXVGzJn4uqJRFeLZmX/lTgc09dOZ/IfkylTtAyTuk0iPH84YRZGzIYY3lnyTrZjhZDJE6cNS/cZ4ZzauVk9vASwzjl3pa/sZ7xxCOrgS67Ouef9tlkPtACqAQOdc1cdt885eOMy/Ix3oY8HegPNk+ubWW/gEufc3ebNNLQaiAAudc5tMLPPgLXA4865JDMrBOx0zhXN4HNUxbsLVMdv+Qvgv86593yfcwLQ2rf/qnh35K5NTkzmzbIUAbRwzm00s4eBW4G3nHP/8f9szrklZtYBb2DWdc65FuYNrLraOfev7zNN92070bftU8B+59wrvuXbgYudc7f7fY65eHfRNiZ/HjNr4X+czWwA0Mg5193MooGFzrmD5g1Cuwjo5ZxbnN5xAtjM5tw7oURyUftR6Y2dnDdN6zct2CGcsPKUT3ML7URyxIy4GQ7g5ktv5qXPX2LLP1v4/J3PGTZ2WKp9DrpmECsXrGyIckSezBGgPCGhq8LQCllXyiNin4zNulIeozyhPJFMeUJCUSjlCAi9PJFejjhRyXkiEG3LtT3ls1Tk9mOtK/FmVlp4XNke59wOvLs9Xx63zZccG3C0lZlt8vu6LLmS81oVb8YbaPSljAJw3lgUrwP+Y9ndCpQC1voS/HekviOWnnPNN/058Bm+ZOp7j2W+7b8xszXAN8Dg5GTqMx8Id84lT8+yAK+bd5q7Xb59TgX8B5g6F5hrZiuBX/DGocjsrlN30h7bz4Ebs/ic7wBXmNd9vwGwxLxu6guA0VklUxGRE3DCOaJJ+ybM/WouAMvmLaNng54pX6uXrE6ppxyhHCEipwXliWOUJ0RETmO52nNOzjy60yWhSj3ncld273bl9TtdEjjlCQlVodQrItR6RIDyhByjPCGhKJRyBIRenlDPOREREREREREREck1apzzY2Z1zZttyf/rp2DHJSIiwaccISJyZjCzdmb2u5mtNbOH0ln/gJmtNrMVZjbLzKr4ypUnRETOACebJzKT27O1hhTn3EqgXrDjEBGRvEc5QkTk9GdmYcCbQGu8GVEXm9kk59xqv2q/4E0YcdDM+uONWXeD8oSIyOkvO3kis/2q55yIiIiIiIinEbDWObfONxnEOKCLfwXn3Gzn3EHf4kKg4imOUUREgidX8oQa50RE5IxiZmPMbJuZ/epXVtLMvjWzP33fz/KVm5m97uuyvsLMLg5e5CIikhPMrJ+ZLfH76ue3ugKw0W95k68sI32B0JupSUREMhSMPKHHWkVE5EzzPvAGMNav7CFglnPuBd+4EQ8BQ4D2QHXf16XA277vIiISopxzo4BRGaxOb4a+dGf4M7OewCVA8xwKTURE8oBg5An1nBMRkTOKc+57YNdxxV2AD3yvPwCu9isf6zwLgRJmVu7URCoiIkGwCajkt1wR2Hx8JTO7EngU6OycO3KKYhMRkeDLlTyhxjkRETmtZNENPSNnO+fiAHzfy/jKT7TbuoiIhLbFQHUzq2ZmBYFuwCT/CmZWHxiJ9w/XtiDEKCIiwZMreUKPtYqIyGkli27oJyrgbusiIhL6nHOJZnY3MAMIA8Y451aZ2TBgiXNuEvAyUAyYYGYA/zjnOgctaBEROWVyK0+ocU5ERAS2mlk551yc77HV5DtcAXVbFxGR04dzbiow9biyJ/xeX3nKgxIRkTwjN/KEHmsVERHxuqLf7Ht9M/C1X3kv36ytjYE9yY+/ioiIiIiI5AT1nBMRAab1y3J26zyj/aj2wQ7hhC3vtzzYIaQws0+BFkBpM9sEPAm8AHxmZn2Bf4DrfNWnAh2AtcBBoM8pD1hE8oTYJ2ODHULAKgwNvaEx3ZMaMUBEQlco5QgIvTxxJuQINc6JiEieV7dc3Rzbl3OuewarWqVT1wF35dibi4hIrsjJPCEiIqefvJ4n9FiriIiIiIiIiIhIkKhxTkREREREREREJEjUOCciIiIiIiIiIhIkapwTEREREREREREJEjXOiYiIiIiIiIiIBIka50RERERERERERIJEjXMiIiIiIiIiIiJBosY5ERERERERERGRIFHjnIiIiIiIiIiISJCocU5ERERERERERCRI1DgnIiIiIiIiIiISJGqcExERERERERERCRI1zomIiIiIiIiIiASJGufklFv0/SJ6te1Fj9Y9+GTUJ2nWx8fHM/T+ofRo3YP+1/Vny6YtKes+HvkxPVr3oFfbXiz6YVHA+zxT4n3x4RfpellX+lzVJ931zjlef+Z1erTuQd9Offlj1R8p66Z/OZ2ebXrSs01Ppn85PaX8919/55ZOt9CjdQ9ef+bexTBzAAAgAElEQVR1nHPZinFb3DYG3DSAm9vfTO+OvZn4wcRU68e/O56WNVqyZ9eedLc/0Tj37t7LwD4D6dmmJwP7DGTfnn0nFG/8kXj6X9ufvp370rtjb957/T0Afl7wM/269uPWLrdyT/d7iP07Nt3tT/QciNsYR//r+tOzTU+G3j+UhPiEgOLMZ/kY/3/j+W/b/wJQIaICH139EZNumMRLrV4if7788P/t3XeYVOX9/vH3DSgqzYgCoqBiQ8QCdjSgGHuLLYqaaJTw05hYYks0UaMxlhhNsUQkRo0aFb9iiRowKBqxgQVQxF4ACwgqRSwsn98fZ3ZZYHfYZWfmzNm5X9e1lzPnTLlnXM49+5wzzwG6tOnCsP2HcdchdzH80OHs0m2XOh+v37r9uP8H9/PgEQ9y/FbH1yyv73FXarESV+x+BQ8e8SC3ff82urbt2qDcZnXJ0nbXeZe/XU8r78fTPmavLfdi8EGDGXzQYK46/6qa+xS625qa9dEHHq3JOfigwQzsOZC3XnsLgNH/Hs3xBxzPCQecwNknnF1vP66IvTbciyknT+HNn7/JOTufs8z67h26898f/pcJJ07g8WMfZ5126wCw6/q78tL/e6nmZ8F5Czho04Nq7ve7gb/j9Z+9zuSfTubn2/+8YHnNqlX6djdreYvZE8XoiGFXD+MHA37APn32KVjOpuZd+O1CLj3nUo4/4HiO3edYbr/h9pr7LO9vwKZYXk90a9+Nx370GC8OeZEJJ05gn40Wv2dbdNqCp49/mldOeoWJJ06kdcvWQPJ3ww3738DrP3ud105+jUM2O6TgubMoE4Nzkq6WdFqt6yMlDat1/Y+SfpG7fLqkryR1qLV+V0n/ruNxx0jaNnd5fUlvStqr9u0lHSdpkaQta93vFUnr5y63lXS9pLclvSTpBUk/yfNalski6WZJh9XK9LqkCZLGSto0t3z/3ONPkDRZ0v+TdJ6kl3M/VbUun1LrsSdI+lcDn2+cpK1r3e54SZMkTcy95oNooqqqKv580Z+5bNhl3PzQzYz+92jee+u9JW7z8PCHade+Hbc/ejuHH3c4N1x5AwDvvfUejz30GP946B9cPuxy/vzbP1NVVdWgx6yUvHsfsjeXD7u83vXPPfkc09+bzm2jbuOMi8/g6guvBpIBrFuvuZXr7r6O64dfz63X3FoziPWnC//EGRedwW2jbmP6e9N5/snn6338hmjZsiUn/fIkbnnkFq676zruv+P+mtc/46MZjH96PJ27dq7zviuS846hd9B3p77cNuo2+u7Ut9EfXlZaeSWuuuUq/v7A3xl23zCe/9/zTH55Mn+68E+cd+V5DLt/GLvvvzv/vP6fy9x3RX4HbrjyBg4/7nBuG3Ub7dq34+F7Hm5QzqN7H807n79Tc/3U7U/ltkm3ceBdBzLn6zkcvOnBAPyk708Y+fZIjrj3CM4ZfQ7n7nLuMo/VQi04d5dz+ekjP+Xg4Qez90Z702P1Hnkf9+CeBzPn6zkccNcB3DbpNk7b4bRlHreYLjj9Am780401190T7gn3ROny5tuup5kXoGv3rgy7fxjD7h/GLy76Rc3yQndbU7PuceAeNTnPveJcuqzThY0224iqhVVcc8k1XH3L1fz9wb/TY9MejLh9RJOzQrKtv3bfa9nn9n3odW0vBvUexGZrbrbEba7c40punXgrW/1tKy564iIu3f1SAMa8N4Y+N/Shzw19GHjLQL789ktGvT0KgOO2Po5u7bvR85qe9LquF3e+cmdB8jaVe8I9Ac1nu5u1vMXqiWJ1RL/d+nH98OubnK+Qecf8ZwzffvMtNz14EzfcewMP3vVgzcDd8v4GXFEN6Ylf9/81d0++m75D+3LkPUdy3X7XAdBSLbntkNs48aET6X19b3a9ZVe+XZQcdHBe//OYMX8Gm16zKb2u7cUT7z1R8OxZlInBOeBpoB+ApBbAmsDmtdb3A8bmLg8CxgEHN/TBJa0LjATOiIiRddxkGnBePXcfBnwGbBwRfYC9gTUa+tz1ODoitgJuAf4gaSVgKHBAbnkfYExEXBIRW0fE1sCC6ssR8Zfc69qM5P9xf0ltGvB81wF/yN133dxr3iUitgR2BCY28XUxZeIUuq7Xla7durLSyisxcL+BjB09donbjH1sLHsdvBcAA/YawIvPvEhEMHb0WAbuN5CVV16ZtbutTdf1ujJl4pQGPWal5N1qu61o36F9vevHjh7Lnt/fE0n02roX8+fMZ9aMWYx7ahzb7LwN7VdvT7sO7dhm5214/n/PM2vGLObPm8/mfTZHEnt+f0+eGv1UkzJ27NSRTTbfBIDV2q5G9x7d+fSTTwG49tJr+X9n/T9Q3fddkZxPj36avb6f/P/Z6/t7Mfa/jXuvJbFqm1UBWLhwIVULq0AgxPx58wGYP28+HTt1XOa+jf0diAheevYlBuw1IMl78F4Ner87tenEd7t/lxFTFv/Btv062/PoO48C8MAbDzBw/YE169qu3LbmvzPnz1zm8Xqv1ZupX0xl+tzpLFy0kP+8/R92XX/XvI+723q78cAbDwDw6DuPsv062y83dyFt229bxj89HnBPuCfcE6XOm2+7nmbe+hSj2wqZdfRDoxm4f7JtjQgiggULFhARfDnvyzr7ZkVsv872vDX7Ld79/F2+XfQtd756Jwf1XHLcpNdavRj9zmgAHn/v8WXWAxzW6zAeefMRFixcAMBJ257ERU9cRJC8rplfLtszaXBPuCea03Y3a3mL1RPF6AiAXlv3Kti2tlB5JfHVgq+oWljF1199zUorrcRqbVcDlv834IpqSE8EQfvWyXN3WKUDH879EIA9N9yTiZ9MZOInyT/52QtmsygWAXD81sdz6VOX1tx/1oJZBc+eRVkZnBtLbnCOpERfAeZK+o6k1sBmwEuSNgTaAr8mKdWG6AKMAn4dEQ/Uc5t/A5tX73Wqlnu+7XP3XQQQETMjolDD1k8CGwHtgFbArNxzfB0Rrzfg/kcB/yR5fQc24PbPAOvkLncC5gLzcs85LyLebVT6Onz6yad06tKp5vpanddaZsP86Sef0mnt5DYtW7Wkbbu2zPlsTr33bchjVkre5Vn6udfssmbeTJ9+8ilrdVlr8fIuhc368bSPeeu1t9hsq80YO3osa3Zak416btTg/A3JOXvW7Jpy7dipI5/N/qzROauqqhh80GAO7ncw2/Tbhl5b9eLMS87kV0N+xeH9D+fR+x/lqCFHNSpvXcvnfDaHtu3b0rJVy2VeRz5n73Q2Vz93dU3hrd56deZ+PZeqqALgk/mf0KlN8nzXj7+e/Tbej1FHjeLafa7lsqcvW+bxOrXpxMfzF389Ycb8GXRu0znv49a+T1VUMe+beazeevXlZi+U7XberuaPLtwT7oml7uueKG7e2mpv19POW53nJ9//CacecyoTx0+suX0xuq2pWauNeXgMu++3OwCtVmrF6ReezgkHnMBh3z2M999+n30P27fJWSGZpmDqnKk116fNmVbztdVqEz6ZwKG9DgWSI6Tbt27PGqsuOWZ0ZO8j+dcriw+q2vA7G3JE7yMY95NxPHzUw2y0Rv29XkruCfdEc9ruZi1vbYXsiWJ0RDE1Je+AvQawyqqrcOguh3Lkbkfyg+N/QPvVCz8gV1tDeuLCMRdyzBbHMPX0qTx81MP8/JFkKoNNOm5CRPCfo//DC0Ne4Kx+ZwHQoXVyQPLFu13MC0Ne4O7D7q75e6LSZWJwLiI+BBZK6k4ySPcM8BywE7AtMDEiviEp0H8B/wM2ldSQ/8u3AtdExPA8t1kEXAEs/f2vzYEJ1UVaBAcAkyJiNvAA8L6kf0k6OrfHb3mOAO4ieU8a8uFib+C+3OUJwCfAu5L+IemA+u4kaYik8ZLG3zb0trxPUNdeCknLvQ1q3PKlH3NFZS3v8jQ2UzGzLpi/gPNPOZ+Tzz2Zli1bctvfbuPHp+afJyGNnJB8FXfY/cMY/sRwpkycwrtvvMs9N9/DpUMvZfiTw9n7kL257tLrGpQ37/tN419H/+79mb1gNq99+lre+1Q/9j4b7cMDrz/AnnfsycmPnMwlu12CljpUcenr1a8l3+PWeZ86Xk+xdOnahVatWjH9g+ngnnBPNGB5pfZEMfJWq71db9M23wE2DdeUvGt0WoM7H7+TG++7kZ/+8qf87ozfMX/e/KK9v03JWm3yhMm0XrU1G2yyAZDMMXT/v+5n6H1Dued/99Bj0x7ccUNh5pbKt02vduaoMxmw3gBeHPIiA9YfwLQ501i4aGHN+i5tu7BFpy0Y+fbiA8Vat2rNVwu/Yrsbt+PGF2/kpgNvKkjepnJPuCea03Y3a3mrFbonitERxdSUvK9NfI0WLVpwz//u4Y7RdzD8puF8OPXDYkWtMxss2xODeg/i5gk30+3qbux7x7788+B/IkSrFq3YpfsuHH3v0exy0y4c3PNgBm4wkFYtWtGtQzfGTh3LNkO34Zlpz3DlHlcW9XVkRSYG53Kqj56rLtNnal1/OnebI4E7c+V2L3B4Ax73v8APJa22nNvdAewoaYP6blBrzoZ8/0rq+2u19vLbJb0M7AycCRARg4Hdgedzy/J+0pG0HTAzIt4HRgN9JX2nnpvfLmkacA7w19zzVZGU62HAG8DVki6sM3jE0IjYNiK2PWbIMflisVaXtZjx8Yya6zM/mbnMIcNrdVmLGR8lt6laWMW8ufNov3r7Ou+7Zqc1G/SYKypreZdn6ef+9ONP82Zaq8tazPx48ddRZn5cmKwLv13I+aecz/cO+B799+zPhx98yMfTPmbwQYM5cuCRzPx4JkMOGcLsmbPz5m9IzjU6rsGsGcmh0rNmzOI7a9T3z2D52rZvy9Y7bM1zTz7H21PeptdWvQDYbd/dePWlV5e5fWN/Bzp8pwPz5sxLvjpLw97vrTtvza7r7crDgx7m8t0vZ7t1tuOsnc6iXet2tFRyBF7nNp1rvr568KYHM/Kd5I+oiTOSiVm/s8qS78kn8z+hS5suNdc7tenEjC9n8NlXn9X7uLXv01ItabtyW774unCTljdEraMi3BPuiZr7uieKnxeW3a4XSlPyrrzyynT4TrKHftPem9K1e1emvTutaN3WlKzVHn/ocQbut3gaguqTQqzTfR0kses+u9bZNyti2pxpdGvfreb6uu3Xrfk6UrWP5n3EoXcfSt+hfTlvdPKNzDlfLz7S7web/4ARU0YsMWA3bc40/m/y/wEwYsoItuy8JeXCPeGeaC7b3azlheL0RDE6opiaknf0v0ez/Xe3p9VKrfhOx++wed/NeX1SQw5+XXEN6YkT+pzA3a/eDcCz055llVarsOZqazJtzjSeeP8JZi2YxYKFC3j4rYfpu3ZfZi2Yxfxv5jPitWQ6nuGTh9N37b5FfR1ZkaXBuep557YgOQz9WZI9Xf2AsUomWN0YeFTSeyTF2pC9O1eQ7DUbLqlVfTeKiIXAH0kKp9pkYKvqvU7VczYA+Y4vnQUsXWprALWPZz06N9fD9yOi5jjSiJgUEVcDewCHLud1DQJ65t6Lt3OZ6rvP0cAGJB8Yrq31fBERz0fEpSTv5/Kec7l6btGT6e9N56OpH/HtN9/y2EOP0W9gvyVu029gP0aOSAYPnhj5BH127IMk+g3sx2MPPcY333zDR1M/Yvp70+m5Zc8GPWal5F2efgP7Meq+UUQEk1+eTJt2bejYqSPb7bId458az9wv5jL3i7mMf2o82+2yHR07dWS1Nqsx+eXJRASj7hvFzrvv3KQMEcEV513Bej3W4wc//gFAMsH1MyO487E7ufOxO1mry1oMvXcoa6y15FdnViRnv4H9GHlf8v9n5H0j6bd7497rz2d/zrw58wD4+quveeHpF1hvw/WYN3ceU99N/nmOHzue7ht2X+a+jf0dkESfHfrwxMhkUtSRI0ay88D87/dfxv2FPe/Yk33/tS/njD6HcdPHce7j5zLuw3Hs0WMPAA7c5EAef/9xIPlDa4d1dgBgg9U3YOWWKzP7qyUHQV+d+SrdO3RnnXbr0KpFK/becG+eeD/JVN/jjnl/DAduknzbZY8ee/D89KZPrt5YteYTck+4J9wTJcxb13a9UJqS9/PZn1NVlezs+HDqh0x/bzprd1u7KN3W1KwAixYtYsx/xiwxOLdm5zV5/+33+Xz25wC8MPaFOvtmRYybPo6NO27M+quvz0otVuLIzY/kgdeX/EZmx1U71hwZ/avv/oqbXlpyLGdQ70FLfKUV4L4p9zFwg+Q1DFhvAG/MeoNy4Z5wTzSX7W7W8harJ4rREcXUlLyd1+7MS8+9lMxD+uUCXpvwGt17FKYP6tOQnvjgiw/YfYNkKoaea/ZklVarMPPLmYx8eyRbdt6SVVutSku1ZMB6A5g8czIAD77xYM181rtvsHvN8kpXb3mUobHAGcA7ub0wsyWtTnIo+E9y6y7MbfgBkPSupPUa8NinkxTJ3yUdl+d2NwNnk8zZQES8JWk88DtJv4mIKkmrUO909gC8CXSVtFlEvJbLtxXwcn13kNQW2DYixuQWbQ28n+f2LUj28m0ZEdNzy3YjmTtjWF33iYhvJf0aeFvJxK9fAF0i4sWGPGdDtWzVklPOP4WzB5/NoqpF7HPoPmyw8Qbc9Oeb2LT3puy8+87sd9h+/P6s33P0HkfTvkN7fnP1bwDYYOMN2G2f3fjxvj+mZcuWnHr+qbRsmRzBU9djFkLW8l78i4t5+fmX+eKzLzi8/+Ec9/Pjao7COnDQgew4YEeee+I5jtnjGFqv2ppzfp98Nmy/ent++NMfcuJhJwLwo5N/VLMn//QLT+eyX13GN199w/b9t2eH/js0KeMrL7zCo/c/So9NejD4oMEADP7FYHYcsGOdt3990us8cOcDnHXJWSuUc9CQQfz2tN/y8D0P02ntTlz45wsblXfWjFlc9svLWFS1iEWxiF333pWddtuJM393JheccgGSaNehHWf//mwgOQnE66+8zvGnHr9CvwNDzhrCxadfzN//9Hc23mxj9j18xeYW+tNzf+KK3a/g5G1PZsqsKTUni/jjs3/k/P7nc8wWxxARnD/mfADWWm0tLuh/AT/7z8+oiiouHXsp1+9zPS1atOC+1+/j7c/ezvu4I14fwSW7XcKDRzzInK/ncPbos1cod1Nsu/O23PDHGwBmuyfcE+6J0uWdNH5So7brpco7YdwE/vGXf9CyZUtatmzJ6b89vWjd1tSsABPHTWStLmvRtVvXmmVrdl6TY08+llOPPpVWrVrReZ3OnHPpOXU9faNVRRU/e/hnjDxmJC3VkptevonJMyfz211/y/gPx9f88XTp7pcSBE++/yQnP3xyzf3X67Ae3dp3W+Yse5c9dRm3H3I7p+94OvO+mcfgBwcXJG8huCfcE81lu5u1vMXqiWJ1xN+u+Buj/z2arxd8zeH9D2e/w/fjuJ8f16SsTc37/aO/z+W/upwf7/9jiOQMrRv23BCo+2/A/Q7fr8l5G9ITZ4w6gxsPuJHTdzydIDjuvuMA+Pyrz7nqmasY95NxBMHDbz7Mw28+DMA5/z2Hfx78T/60yp+YOX8mP74//9RGlUJ1fqe5DElqSXIWo79ExK9zy24GdoqITSW9C+wTEVNq3ecqknkOngMeITcBas7hwKXAmRExXtLKJBO1TgAeyi3fP1eu20bEz3KPeQrwZ2CDiHhPUnuSMxLtCcwGFpAcCn9NnteyM8les1WAb4FzI+LR3Lox1Zlq3b4dyVwPG+Yefz5w6lK3mRcRbXOXdwUui4gda61vSXKWqL651/3viLhn6eeTdAbQC7gI+AfQFfgKmAmcGBFv1/e6AD7kw2z8Qpll2D5D90k7QqNNGDKhSROhNGTbUlVVRa/v9GLe3HmXuCfcE2aVbJ3frrP8G5WZuCDcE+4JwD1hVgpZ64mmdgQ0btvSla6lmRS+lswMzlk2uEzNis+Dc/mlUabWcO4Js+LL2h9dUJrBuWruifLmnjArvqz1RCUMzmVpzjkzMzMzMzMzM7NmJUtzzmWKpC2Afy61+OuIaPqkJmZmlnnuCTMzy8c9YWZWOTw4VyQRMYlk0lMzM7NluCfMzCwf94SZWeXw11rNzMzMzMzMzMxS4sE5MzMzMzMzMzOzlHhwzszMzMzMzMzMLCUenDMzMzMzMzMzM0uJB+fMzMzMzMzMzMxS4sE5MzMzMzMzMzOzlHhwzszMKo6kvSW9LuktSb9MO4+ZmZWP5XWEpNaS7sqtf07S+qVPaWZmaSlGT3hwzszMKoqklsC1wD5AL2CQpF7ppjIzs3LQwI44AfgsIjYCrgYuL21KMzNLS7F6woNzZmZWabYH3oqIdyLiG+BO4KCUM5mZWXloSEccBNySu3wPsLsklTCjmZmlpyg90argMa2idaVrUT6YSBoSEUOL8djFkKW8WcoKzgswYciEQj7cEsr1/W3MtkXSEGBIrUVDl3pN6wBTa12fBuzQtITWUO6JRJbyZikrOC9AXBCFfLgllOv7W8CeaEhH1NwmIhZK+gLoCHza2Ny2LPdEtrKC8xabe6Iwyr0nfOScZcWQ5d+krGQpb5aygvMWW9byLiMihkbEtrV+lv5wUFcxF+8TipVK1n53s5Q3S1nBeYsta3mXsZyeaEhHuEeyKUu/u1nKCs5bbM5bYmn0hAfnzMys0kwDutW6vi7wYUpZzMysvDSkI2puI6kV0AGYXZJ0ZmaWtqL0hAfnzMys0owDNpa0gaSVgSOBB1LOZGZm5aEhHfEAcGzu8mHAYxHhI+fMzCpDUXrCc85ZVpTdd9aXI0t5s5QVnLfYspa30XLzPvwMGAm0BG6KiFdTjmVNl7Xf3SzlzVJWcN5iy1reRqmvIyRdBIyPiAeAvwP/lPQWyZEQR6aX2BohS7+7WcoKzltszltGitUT8k4eMzMzMzMzMzOzdPhrrWZmZmZmZmZmZinx4JyZmZmZmZmZmVlKPDhnZmZmZmZmZmaWEg/OmZmZmZmZmZmZpcRna7WyI2lzYMPcWU6QdDXQIbf6moh4MbVwS5HUHugcEW/mrh8OrJpbPTIiPkktXB2y9N42B5I6Av2BDyLihbTzLE3SAcDEiHg/d/184FDgfeDUiHg3zXxm9cnStsw9YfmUc0+4IyzLsrQtc09YPu6JyuEj56wcXQZ8Wuv6XsBDwOPA+akkqt+VwM61rl8KbEeyAf1tKonyy9J7i6QTJJ1V6/p0SXMkzZV0UprZ6iLp35J65y6vDbwCHE9yGu3TUg1Xt0uAmQCS9geOIcn7APC3FHOZLU+WtmXuiSLJWkdA5nrCHWFZlpltGe6JonFPFJ17ooA8OGflaO2IeLrW9TkR8X8R8U9gzbRC1WM74JZa1+dGxM8jYjDQO6VM+WTpvQU4Ebip1vUZEdEeWAsYlE6kvDaIiFdyl38MPBoRBwA7kBRVuYmI+DJ3+RDg7xHxQkQMI3mPzcpVlrZl7oniyVpHQLZ6wh1hWZalbZl7onjcE8XlniggD85ZOWpX+0pE7FjraqcSZ1meVhERta7/sNbl1UsdpgGy9N4CtIiIWbWuDweIiK9YfLh/Ofm21uXdgYcBImIusCiVRPlJUltJLUjyjq61bpWUMpk1RJa2Ze6J4slaR0C2esIdYVmWpW2Ze6J43BPF5Z4oIA/OWTn6UNIOSy+UtCPwYQp58lkkqUv1leq9HJLWofw2npCt9xYWz18BQET8HiBXAB1TSZTfVEk/l3Qw0Bf4D4CkVYGVUk1Wtz8BLwPjgdciYjyApD7AR2kGM1uOLG3L3BPFk7WOgGz1hDvCsixL2zL3RPG4J4rLPVFAWnKQ3ix9krYH7gJuBqonFN0GOBY4IiKeTynaMiQdA5wKnAG8lFvcl2TuiL/kDu8uG1l6bwEkXQfMjohfL7X8d8CaEXFiOsnqJqkTcBGwNnBtRIzKLd8N2CYirkwzX11yH/w6ARMiYlFu2doke3GnphrOrB5Z2pa5J4onax0B2esJd4RlVca2Ze6JInFPFJ97onA8OGdlSVJn4GRg89yiV0k2TmV1tiIASXsD57I46yvAZRHxSHqp6pex97YNMIxkLo4JucVbkeydGRwR89LK1liS1qs+k1G5k7QpcGZE/CTtLGb1ydi2zD1RBM2pIyA7PeGOsKzIyrYM3BPF4p5Ih3tixXhwzszKnqQeLC7/yRHxdpp58pG0E7AO8GREzJC0JfBL4LsR0S3ddEvKZbsS6ArcB/wVuI5kwtk/RsTVKcYzM2uQLHUEZKcn3BFm1ly4J4rDPVFYHpyzsiPpcaC+X8yIiN1LmScfSflOFx4RcXHJwjRAlt5bAEnd862PiA9KlaUhJP0B2J9k7oWNgH8DPwV+D9yQm3y2bEh6DrgeeAbYGzgbuAP4TbllNastS9sy90TxZK0jIFs94Y6wLMvYtsw9USTuieJyTxSWB+es7Ejapo7FO5L8Y58REduVOFK9JJ1Rx+I2wAlAx4hoW+JIeWXpvQWQNImk/FVrcZCcmrtTRLRMJVg9JE0G+kbEV5K+QzIp7pYR8WbK0eok6eWI2LrW9anA+hFRlWIss+XK0rbMPVE8WesIyFZPuCMsyzK2LXNPFIl7orjcE4XVKu0AZkuLiBeqL0saAPwGaA2cWG7zLkTEH6svS2pHMpnrj4E7gT/Wd7+0ZOm9BYiILWpfl7Q+cA7wPZK9R+VmQfVeooj4TNLr5ViktaySO5tS9QeWecCWkgQQES/We0+zFGVpW+aeKJ4MdgRkqyfcEZZZGduWuSeKxD1RdO6JAvKRc1aWJO1FsqH/CrgkIh5POVK9JK0B/AI4GrgF+HNEfJZuqvpl6b2tJmlj4Dxy8xcAt0TEt+mmWpakz4Enay3qX/t6RBxY8lB5SBpD/q8lDCxhHLNGydK2zD1RXFnpCMhWT7gjLOuytC1zTxSXe6I43BOF5cE5KzuSxpEcavwHku+vL6GcRuBzcwIcAgwlOUNRWZ/xJ8ttU70AAB5jSURBVEvvLYCk3iRFujlwBfCvcj5MOrf3sF4R8USpspg1Z1nalrkniidrHQHuCbNSydi2zD1RJO4JyxIPzlnZydIIvKRFwNfAQpbMLJKs7VMJVo8svbcAkqqAqcBDwDJFGhGnlDxUMyLpkHzrI+LeUmUxa4wsbcvcE8Xjjigud4RlWca2Ze6JInFPFJd7orA855yVnYjYNe0MDRURLdLO0BhZem9zTqD+8i87tSadXWYVyYeVLUscaXkOyLMuABeqlaUsbcvcE0WVqY6AzPWEO8IyK0vbMvdEUbkniss9UUA+cs7KTpZG4HPzQ9QrImaXKktDZOm9zSJJ6+VbHxHvlypLU0nqHBGfpJ3DrC5Z2pa5J6y25tIT7ggrd1nalrknrDb3ROXykXNWjrI0Av8Cy56eu1oAPUobZ7my9N4i6UHy7O0qpwlRof6ylLQzcBRwcmkTNY6kDsChJFk3A9ZJN5FZvbK0LXNPFEnWOgKy3RPuCMuYzGzLcE8UjXuitNwTTeMj56zsSFovQ3sEMpM1i7I8IaqkrUmK6QfAu8C9EfHXdFMtS9KqwIEkWfsC7YDvA09GxKI0s5nVJ0vb3ixlzZosdwRkoyfcEZZVWdr2Zilr1rgnis89UTg+cs7K0WhJw4ArI2Jh2mGWYwTJRigzJG0KDAF65ha9BgyNiDfSS1WvlSPi0bpWSLocKKtClbQJcCQwCJgF3EWyE2S3VIPVQ9LtJKdnHwVcAzwGvBURY9LMZdYA7okiylBPZKojIFs94Y6wjHNPFJF7onjcE5UrU5NPWsXoA3QGXpDUP+0wy1HX4edlS9JOwBhgHsnp2m8E5gNjJO2YYrT6XCtpv9oLJLWQdDOwVTqR8poC7A4cEBG75PZslfPp2nsDn5F8oJqSO7W8D6e2LHBPFEnGeiJrHQHZ6gl3hGWZe6JI3BNF556oUP5aq5UtSdsAo4FpwCLK8Aw1kmYAd9a3vtxOzy3pEeDypfdm5A75/mVE7JNKsHpIWh/4D3BuRNybO2x6ODAHODYivk0x3jIkHUyyp6sfSe47gWERsUGqwfKQ1JPkMPQjgBkke0C3iIiPUw1m1gDuicLLUk9krSMgez3hjrCsc08UnnuiuNwTlcuDc1aWJA0E/gyMBK4lKVOgvM5QI+l94Pz61kfELSWMs1yS3oiITepZ93pEbFrqTMsjaV2S34O/Aj8EnouIX6SbKj9JbUjmWhgEDARuAUZExKhUgy2HpG1JMh8OTIuIfilHMquXe6I4stYTWewIyGZPuCMsa9wTxeGeKA33ROXx4JyVHUl3kpzZ5acRMSntPPlIejEiMjNHhKQXImKbetaV3WuRVJ1nbeBW4FHgiur1EfFiGrnqI6nV0vOaSFqDpKCOiIiB6SSrm6SfRcQ1dSwX0L/cJ8m1yuWeKJ4s9UTWOgKy1RPuCMsy90TxuCeKyz1RuTw4Z2VH0k8i4sZ61nWOiE9Knak+kj6KiLXTztFQeQ6bF/CDiOhc4kh5SXo8z+oop3KC8vtAsjxZy2tWzT1RPFnqiax1BGRru5ulrGZLc08Uj3uiuLK07c1S1izw2Vqt7CxdpJI6AIeSfJd9M5K9YOUia9+lPyvPuvElS9FA+c5KVIYTzkLGJvQ1yyr3RFFlpicy2BHgnjArCfdEUbkniss9UaF85JyVpdxknQeSFGhfoB3Jd+6fjIhF+e5bSt5bkB5JH0RE97Rz1CZpGnBVfesjot51aZC0EPiyrlUkexPblziSWYO5JyyfcuwIyFZPuCMs69wTlo97ouncE4XlI+es7Ei6HegPjAKuAR4D3lr6jEBlYl1Jf6lvZRmeXekf1H9664iIE0qZp4nKca9SS6At5ZmtLpMiok/aIcwayz1RPM2oJ8p1O5ylnnBHWGa5J4rHPVF07okK5cE5K0e9gc+A14ApEVElqVwP8VwAvJB2iEb4dx3LugOnkRRBlpTj78RHEXFR2iHMKoB7oniaS0+U6++De8KsNNwTxeOeKC73RIXy4JyVnYjYSlJPkkPQ/5ubdLSdpC4RUW5zMswqt9Ob5xMR/1d9WVIP4FySvYqXAX9PK1d9JD1I3cUpoGOJ4zREFvZw1TY87QBmK8I9UTxZ6okMdgRkqyfcEZZZ7onicU8UnXuiQnnOOSt7krYFBpGcPnpaRPRLOVINSc9GRLlOJlonSZsB5wF9gD8Aty19uu5yIWlAvvXldnpuSd1J9nZ9m7u+KbAv8H5E3JtquDpI+gkwJiLezJ3y/CaSyZLfA44rx9PLm9XFPVFYWemJrHUEZKsn3BHWnLgnCss9UTzuicrlwTnLDEktgFMj4uq0s1STtA15Dokutw2SpOHAtsCVwN1AVe31ETE7jVyNJakbcGRE/CHtLLVJehI4IVdQGwHPA7cDvYDnI+JXqQZciqRXgD4R8a2ko4AzgD1JPmhdEBHfTTWgWSO5J5quOfREuXYEZKsn3BHWHLknms49UVzuicrlwTnLlHI7q46kx0nKtPrw4yX+QUXEwJKHykPSeyzOWP3fmuwR0aPkoRpI0pokezsHAesAIyLizHRTLUnSpIjYInf5YmCNiDhZ0srAC9XryoWklyNi69zlO4DnIuLPues+c5hlknuiabLaE1noCMhWT7gjrLlyTzSNe6K43BOVy3POWdaU23fwzwGmRsRHAJKOZfGhvBemF6tuEbF+2hkaQ1I74GCS+UI2AUYAPSJi3VSD1a/2h6mBJIf5ExHfSFqUTqS8Fklam2TC5N2BS2qtWzWdSGZN5p5ogiz1RAY7ArLVE+4Ia67cE03gnig690SFapF2ALNGKrdDPf8GfA0gqT9wKXAL8AUwNMVcDSZpQ0nn5Q5LLjczgBNINvQbRsQZwDfpRsproqQrJZ0ObASMApC0erqx6nU+MJ7kw98DEfEq1MzP8U6Kucyawj1RYGXcE1nrCMhWT7gjrLlyTxSYe6Kg3BMVyl9rtbIjaS71n1Vn1YgomyM+JU2IiK1yl68FZkbEhbnrNYf5lpvcHo4jSPYibUnyIeDeiJiUarCl5ErpSKANcAdwF/BoGR8uvypwKrA2cFNETMgt70fygeCfaeari6RWQLuI+KzWsjYk/TAvvWRm9XNPFF8WeiJrHQHZ6wl3hGWVe6L43BPF4Z6oXB6cM2uC3N6hrSNioaQpwJCIeLJ6XUT0TjfhknJn1BkErEsygevdwP0RsUGqwZZDyWnaB5GU68bABSTzRLyRarCMk7QxyaHyGwGTgDMjYnq6qcyaF/dE8bkjisMdYVYa7onic08Uh3uisDw4Z9YEks4jObX1p0B3oG9ERO7MOrdExM6pBlyKpG+AZ4AzImJ8btk75br3SNJpwFPAy5E7PbukLUjK9YiI2DDNfEurNaFvXSIidi9lnuWR9D/gVuBJ4EBgp4g4JN1UZs2Le6J4stYRkK2ecEeYlYZ7onjcE8XlnigsD86ZNZGkHUkOOx4VEfNzyzYB2kb5nfq89lmKOpPs6TouIrqlGqwekq4E+gE9gYnA08BY4Jkow9O0S9qmjsU7AmcDMyJiuxJHymvpr0r4rEpmxeGeKI6sdQRkqyfcEWal454oDvdEcbknCsuDc2YVStK6JId2DwJWIzm0+9x0U9VNyanDtyUp151yP59HRK9Ug+WRmwj1N0Br4PcR8UjKkZaR++rEIBaftex2knlDBFBuHwbNrLSy0hNZ7Ago/55wR5jZ8rgniss9UVk8OGdWQSTtGBHP1rF8U+DIiPhtCrGWS1IHkhLdOfff1YFJEfHjVIPVQdJeJCX6FXBJRDyecqR6SRpD/sPmB5YwjpmVgSz2RJY6ArLTE+4IM6uLe6L43BOVyYNzZhUka4caSxoKbA7MBZ4DngWerX02oHIiaRywFsnEqM8svd57j8ys3GWpJ7LWEeCeMLPsc08Ul3uicpXNKaTNzOrQneQw7jeB6cA04PNUE+U3H5gHHJb7qS2Astp7JGnpCVuDZDLilyNibgqRzMwaI2sdARnqCXeEmTUD7okick8Ulo+cM6sgkj4nOZtOnSLiwBLGaRBJItnj1S/30xuYTTKR6wVpZss6Sf+oY/EawJbACRHxWIkjmVnKstYT7ojicUeYWV3cE1bNPVFYHpwzqyCS3gQG17c+Ip4oYZxGyU04uzNJqe4PdIyI1dNNtSRJE0hO1/40MDYi3ks30YqRtB5wd0TskHYWMyutrPZEFjoCmkdPuCPMKpt7orjcE5XLg3NmFUTSSxHRJ+0cDSXpFJIC3Rn4ltypz3P/nRQRi1KMtwxJvVm8V64f0IakWJ8Gno6I51KM1yhZmk/EzAonSz2RtY6A5tMT7gizyuWeKC73ROXy4JxZBZH0GHBURHycu/4j4FDgfeDCiJidZr6lSbqKxXuNPko7T2NJWpPk9PKnARtERMuUIzVI7mxbN0fETmlnMbPSylJPZL0jIJs94Y4wq2zuidJyT1QOD86ZVRBJLwLfi4jZkvoDdwI/B7YGNouIpScdtUaQ1BLow+I9dBuSTD77DMm8FmV1mL+kB1n29OdrAGsDx0TEMmeIMrPmzT1RXFnqCXeEmdXFPVFc7onK5cE5swoi6eWI2Dp3+VpgZkRcuPQ6WzGS5gOvAdcCYyLi3ZQj5SVpwFKLApgFvBkR36QQycxS5p4oriz1hDvCzOrinigu90Tl8uCcWQWR9AqwdUQslDQFGBIRT1avi4je6SbMNkmDgJ2AbYAqYByL93JNTzNbU0h6xoelm1UG90RxNceecEeYVRb3RHG5JypXq7QDmFlJ/Qt4QtKnwALgfwCSNgK+SDNYcxAR/yJ5j5G0GrA9yeHol0paOSLWSzNfE6ySdgAzKxn3RBE1055wR5hVFvdEEbknKpcH58wqSERcImk0yTwAo2LxobMtSOaKsCaS1AbYgcXzRGwHTCU5K1RW+RBrswrhnii+ZtgT7gizCuKeKD73RGXy11rNzApE0ktAd2A8uTNDAc9GxLxUgzWRT4VuZlYYzbEn3BFmZoXjnqhcPnLOzKxwjgUmRfPb66G0A5iZNRPNsSfcEWZmheOeqFA+cs7MrIAk9QbOAjYnOYR7MvDHiJiYarAmkNQ7Il5JO4eZWXPQ3HrCHWFmVljuicrUIu0AZmbNhaSDgBHAE8DxwODc5f/LrSsrkk6QdFat69MlzZE0V9JJ1ctdpmZmhZGlnnBHmJmVnnuicvnIOTOzApE0ATgoIt5bavn6wP0RsVUKseolaRywd0TMyl1/KSL6SFqFZILf/ukmNDNrXrLUE+4IM7PSc09ULh85Z2ZWOCstXaQAuWUrlTzN8rWoLtOc4QAR8RWwajqRzMyatSz1hDvCzKz03BMVyoNzZmaF862k7ksvlLQesDCFPMvTofaViPg9gKQWQMdUEpmZNW9Z6gl3hJlZ6bknKpQH58zMCucC4L+SjpO0haTekn4MjALOTzlbXUZJ+l0dyy8iyWxmZoWVpZ5wR5iZlZ57okJ5zjkzswKStBVwBsnZlQS8ClwZERNSDVYHSW2AYcB2QHW+rYDxwOCImJdWNjOz5iorPeGOMDNLh3uiMnlwzsyswknqQVL+AJMj4u0085iZWflwR5iZWT7uicLw4JyZWQFJOhY4BeiZW/Qa8JeIuDW9VHWraz6L2iLig1JlMTOrFFnpCXeEmVk63BOVqVXaAczMmgtJPwJOA34BvEhyGHpf4A+SKLdCBR4CgiRntQDWAjoBLdMIZWbWXGWsJ9wRZmYl5p6oXD5yzsysQCQ9Cxy59OnPJa0P3BkRO6YQq8FyOc8Bvkeyd+6vqQYyM2tmstwT7ggzs+JzT1Qun63VzKxw2i9dpAC5Ze1LnqaBJG0s6WbgEeAFoJfL1MysKDLXE+4IM7OSck9UKH+t1cyscBas4LpUSOoNnEcygesVwAkRUZVuKjOzZi0zPeGOMDNLhXuiQvlrrWZmBSLpS+CtulYBPSKiTYkj5SWpCphKMl/EMkUaEaeUPJSZWTOWpZ5wR5iZlZ57onL5yDkzs8LZLO0AjXQCyaStZmZWGlnqCXeEmVnpuScqlI+cMzMrMUnPRMROaecwM7Py5J4wM7N83BPNj4+cMzMrvVXSDgAg6UHy7O2KiANLGMfMzBZLvSfcEWZmZc090cx4cM7MrPTK5ZDlK9MOYGZmdSqHnnBHmJmVL/dEM+PBOTOzyrVyRDxa1wpJlwNPlDiPmZmVD3eEmZnl454ooBZpBzAzq0BKO0DOtZL2q71AUgtJNwNbpRPJzMwoj55wR5iZlS/3RDPjI+fMzErvh2kHyNkT+I+k1hFxr6RVgeHAHOCAdKOZmVW0cugJd4SZWflyTzQzPnLOzKxAJJ0g6axa16dLmiNprqSTqpdHxCvpJFxSRLwHfA+4WNKJwH+BNyLiqIj4NtVwZmbNUJZ6wh1hZlZ67onKpYhymEfQzCz7JI0D9o6IWbnrL0VEH0mrAKMion+6CZckqW/u4trArcCjwBXV6yPixTRymZk1V1nqCXeEmVnpuScql7/WamZWOC2qizRnOEBEfJU7zLvc/LHW5YlA51rLAhhY8kRmZs1blnrCHWFmVnruiQrlI+fMzApE0lsRsVEdy1sAb0VEjxRirRBJO0bEs2nnMDNrTppLT7gjzMyKwz1RuTznnJlZ4YyS9Ls6ll8EjCp1mCa6O+0AZmbNUHPpCXeEmVlxuCcqlI+cMzMrEEltgGHAdsCE3OKtgPHA4IiYl1a2xpI0NSK6pZ3DzKw5aS494Y4wMysO90Tl8uCcmVmBSeoBbJ67Ojki3k4zz4qQ9EFEdE87h5lZc5T1nnBHmJkVl3ui8nhwzsysQCTlLaCI+KBUWRpC0oMkk7UuswoYGBFtShzJzKxZy1JPuCPMzErPPVG5PDhnZlYgkiaRFJRqLQ5gLaBTRLRMJVg9JA3Itz4inihVFjOzSpClnnBHmJmVnnuicrVKO4CZWXMREVvUvi5pfeAc4HvA71OIlFd9hSmpG3Ak4EI1MyugLPWEO8LMrPTcE5XLZ2s1MyswSRtLuhl4BHgB6BURf003VX6S1pR0kqQngTFA55QjmZk1W1nrCXeEmVlpuScqj4+cMzMrEEm9gfNIJm+9AjghIqrSTVU/Se2Ag4GjgE2AEUCPiFg31WBmZs1UlnrCHWFmVnruicrlOefMzApEUhUwFXgIWKZEI+KUkofKQ9IC4Hng18BTERGS3omIHilHMzNrlrLUE+4IM7PSc09ULh85Z2ZWOCdQ9xmLytW5JPNBXA/cIemulPOYmTV3WeoJd4SZWem5JyqUj5wzM6twknoAg0jKdWPgAmBERLyRajAzM0udO8LMzPJxTxSGB+fMzApE0oPk2dMVEQeWMM5ySToNeAp4OSIW5pZtQVKuR0TEhmnmMzNrbrLUE+4IM7PSc09ULg/OmZkViKQB+dbXd7rxtEi6EugH9AQmAk8DY4FnImJ2mtnMzJqjLPWEO8LMrPTcE5XLg3NmZgUiaY+IeLSedZdHxDmlztQQklYGtiUp151yP59HRK9Ug5mZNTNZ7Al3hJlZ6bgnKleLtAOYmTUj10rar/YCSS0k3QxslU6kBlkVaA90yP18CDyXaiIzs+Ypiz3hjjAzKx33RIXy2VrNzApnT+A/klpHxL2SVgWGA3OAA9KNtixJQ4HNgbkkBfo0cFVEfJZqMDOz5iszPeGOMDNLhXuiQnlwzsysQCLiPUnfA0ZK6gT8EHguIn6RcrT6dAdaA28C04FpwOepJjIza8Yy1hPuCDOzEnNPVC7POWdmViCS+uYurg3cCjwKXFG9PiJeTCNXPpJEsserX+6nNzCbZCLXC9LMZmbW3GStJ9wRZmal5Z6oXB6cMzMrEEmP51kdETGwZGEaSdK6wM4kpbo/0DEiVk83lZlZ85LVnnBHmJmVhnuicnlwzsysBCTtGBHPpp2jNkmnkBTozsC35E59nvvvpIhYlGI8M7OKUm494Y4wMysv7onmzYNzZmYlIOmDiOiedo7aJF1FMnHr2Ij4KO08ZmaVrNx6wh1hZlZe3BPNmwfnzMxKQNLUiOiWdg4zMytP7gkzM8vHPdG8tUg7gJlZhfCeEDMzy8c9YWZm+bgnmrFWaQcwM2suJD1I3aUpoGOJ45iZWZlxT5iZWT7uicrlr7WamRWIpAH51kfEE6XKYmZm5cc9YWZm+bgnKpcH58zMikxSN+DIiPhD2lnMzKz8uCfMzCwf90Tz5znnzMyKQNKakk6S9CQwBuicciQzMysj7gkzM8vHPVFZPOecmVmBSGoHHAwcBWwCjAB6RMS6qQYzM7Oy4J4wM7N83BOVy19rNTMrEEkLgOeBXwNPRURIeicieqQczczMyoB7wszM8nFPVC5/rdXMrHDOBVYBrgd+JWnDlPOYmVl5cU+YmVk+7okK5SPnzMwKTFIPYBBwJLAxcAEwIiLeSDWYmZmVBfeEmZnl456oPB6cMzMrEEmnAU8BL0fEwtyyLUiK9YiI8J4vM7MK5p4wM7N83BOVy4NzZmYFIulKoB/QE5gIPA2MBZ6JiNlpZjMzs/S5J8zMLB/3ROXy4JyZWYFJWhnYlqRYd8r9fB4RvVINZmZmZcE9YWZm+bgnKk+rtAOYmTVDqwLtgQ65nw+BSakmMjOzcuKeMDOzfNwTFcZHzpmZFYikocDmwFzgOeBZ4NmI+CzVYGZmVhbcE2Zmlo97onK1SDuAmVkz0h1oDXwMTAemAZ+nmsjMzMqJe8LMzPJxT1QoHzlnZlZAkkSyt6tf7qc3MJtkEtcL0sxmZmbpc0+YmVk+7onK5ME5M7MikLQusDNJoe4PdIyI1dNNZWZm5cI9YWZm+bgnKosH58zMCkTSKSTluTPwLbnTnuf+OykiFqUYz8zMUuaeMDOzfNwTlctnazUzK5z1gXuA0yPio5SzmJlZ+Vkf94SZmdVvfdwTFclHzpmZmZmZmZmZmaXEZ2s1MzMzMzMzMzNLiQfnzMzMzMzMzMzMUuLBObNmQFKVpJclvSJpuKTVmvBYu0r6d+7ygZJ+mee2q0v66Qo8x4WSzqxn3Y9yr+NVSZOrbyfpZkmHNfa5zMzMPWFmZvm5J8zS5cE5s+ZhQURsHRG9gW+AE2uvVKLR/94j4oGIuCzPTVYHGl2m9ZG0D3AasGdEbA70Bb4o1OObmVUw94SZmeXjnjBLkQfnzJqf/wEbSVpf0muSrgNeBLpJ2lPSM5JezO0RawsgaW9JUyQ9BRxS/UCSjpN0Te5yZ0kjJE3I/fQDLgM2zO1l+0PudmdJGidpoqTf1nqs8yS9Lum/wKb1ZP8VcGZEfAgQEV9FxI1L30jS+bnneEXSUEnKLT8lt3dsoqQ7c8sG5PK9LOklSe2a+P6amWWde8I9YWaWj3vCPWEl5sE5s2ZEUitgH2BSbtGmwK0R0QeYD/wa+F5E9AXGA7+QtApwI3AA8F2gSz0P/xfgiYjYimQP1KvAL4G3c3vZzpK0J7AxsD2wNbCNpP6StgGOBPqQlPV29TxHb+CFBrzUayJiu9yevVWB/XPLfwn0iYgtWby370zg5IjYOvf6FjTg8c3MmiX3hHvCzCwf94R7wtLhwTmz5mFVSS+TFOQHwN9zy9+PiGdzl3cEegFjc7c9FlgP6Am8GxFvRkQAt9XzHAOB6wEioioi6jo8fM/cz0ske9d6kpTrd4EREfFlRMwBHmjSq4XdJD0naVIu1+a55ROB2yUdAyzMLRsLXCXpFGD1iFi47MOZmTV77omEe8LMrG7uiYR7wlLRKu0AZlYQC3J7cmrkjsyeX3sR8GhEDFrqdlsDUaAcAi6NiBuWeo7TGvgcrwLbAI/V+wTJnrnrgG0jYqqkC4FVcqv3A/oDBwK/kbR5RFwm6SFgX+BZSd+LiCmNfF1mZlnnnki4J8zM6uaeSLgnLBU+cs6scjwL7CxpIwBJq0naBJgCbCBpw9ztBtVz/9HASbn7tpTUHpgL1J5zYSRwfK25J9aR1Al4EjhY0qq5ORoOqOc5LgWukNQld//WuT1UtVUX56e55zksd9sWQLeIeBw4m2Ry2baSNoyISRFxOcmewJ753iQzswrmnnBPmJnl455wT1iR+Mg5swoRETMlHQf8S1Lr3OJfR8QbkoYAD0n6FHiKZK6GpZ0KDJV0AlAFnBQRz0gaK+kV4JHcPBGbAc/k9rTNA46JiBcl3QW8DLxPMslsXRkfltQZ+K+SBwjgpqVu87mkG0nmwXgPGJdb1RK4TVIHkj1uV+due7Gk3XKZJwOPNO6dMzOrDO4J94SZWT7uCfeEFY+Sr4SbmZmZmZmZmZlZqflrrWZmZmZmZmZmZinx4JyZmZmZmZmZmVlKPDhnZmZmZmZmZmaWEg/OmZmZmZmZmZmZpcSDc2ZmZmZmZmZmZinx4JyZmZmZmZmZmVlKPDhnZmZmZmZmZmaWkv8PGDwpYxkvmQEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfd1ee11f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "plot_confusion_matrix(Y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 1s 419us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4131409560204217, 0.8937902952154734]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With a simple 2 layer architecture we got 90.09% accuracy and a loss of 0.30\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "### HyperParameter : Dropout (Uniform) and Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    # Loading the train and test data\n",
    "    X_train, X_test, Y_train, Y_test = pkl.load(open('har_data.pkl', 'rb'))\n",
    "    \n",
    "    return X_train, X_test, Y_train,Y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe, space_eval\n",
    "import keras.optimizers, keras.initializers\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing import sequence\n",
    " \n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "import numpy, json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, X_test, Y_train,Y_test):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(128, 9)))  # one Lstm Layer\n",
    "    model.add(Dropout({{uniform(0, 1)}}))       # Uniform Dropout between 0 an 1\n",
    "    model.add(Dense(6, activation='sigmoid'))   # Activation sigmoid\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',                          #Optimzer is adam\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "    checkpointer = ModelCheckpoint(filepath='keras_weights.hdf5',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True)\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([32, 64, 128])}},        # Different batch sizes\n",
    "              epochs=20,\n",
    "              validation_split=0.08,                       # splitting data into 80:20 ratio for train and validation\n",
    "              callbacks=[early_stopping, checkpointer])\n",
    "\n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Best Parameters and Corresponding Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle as pkl\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle as pkl\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.recurrent import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe, space_eval\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.optimizers, keras.initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import MinMaxScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy, json\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: This function is separated from create_model() so that hyperopt\n",
      "  6: won't reload data for each evaluation run.\n",
      "  7: \"\"\"\n",
      "  8: # Loading the train and test data\n",
      "  9: X_train, X_test, Y_train, Y_test = pkl.load(open('har_data.pkl', 'rb'))\n",
      " 10: \n",
      " 11: \n",
      " 12: \n",
      " 13: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     model = Sequential()\n",
      "  4:     model.add(LSTM(128, input_shape=(128, 9)))\n",
      "  5:     model.add(Dropout(space['Dropout']))\n",
      "  6:     model.add(Dense(6, activation='sigmoid'))\n",
      "  7: \n",
      "  8:     model.compile(loss='binary_crossentropy',\n",
      "  9:                   optimizer='adam',\n",
      " 10:                   metrics=['accuracy'])\n",
      " 11: \n",
      " 12:     early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
      " 13:     checkpointer = ModelCheckpoint(filepath='keras_weights.hdf5',\n",
      " 14:                                    verbose=1,\n",
      " 15:                                    save_best_only=True)\n",
      " 16: \n",
      " 17:     model.fit(X_train, Y_train,\n",
      " 18:               batch_size=space['batch_size'],\n",
      " 19:               epochs=20,\n",
      " 20:               validation_split=0.08,\n",
      " 21:               callbacks=[early_stopping, checkpointer])\n",
      " 22: \n",
      " 23:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
      " 24: \n",
      " 25:     print('Test accuracy:', acc)\n",
      " 26:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      " 27: \n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.4459 - acc: 0.8269 - val_loss: 0.3632 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36316, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.3621 - acc: 0.8619 - val_loss: 0.3478 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36316 to 0.34776, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.3491 - acc: 0.8621 - val_loss: 0.3539 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34776\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.3369 - acc: 0.8655 - val_loss: 0.3253 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34776 to 0.32529, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.3193 - acc: 0.8692 - val_loss: 0.3621 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.32529\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.3144 - acc: 0.8673 - val_loss: 0.4997 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32529\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.4561 - acc: 0.8130 - val_loss: 0.3619 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.32529\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.3307 - acc: 0.8631 - val_loss: 0.3103 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.32529 to 0.31027, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.2954 - acc: 0.8751 - val_loss: 0.2628 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.31027 to 0.26276, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.2602 - acc: 0.8912 - val_loss: 0.2294 - val_acc: 0.8984\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.26276 to 0.22943, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.2285 - acc: 0.8988 - val_loss: 0.2096 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.22943 to 0.20964, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.2144 - acc: 0.9001 - val_loss: 0.2444 - val_acc: 0.8769\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.20964\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.2046 - acc: 0.9035 - val_loss: 0.2036 - val_acc: 0.9024\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.20964 to 0.20356, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.1990 - acc: 0.9033 - val_loss: 0.1971 - val_acc: 0.9069\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.20356 to 0.19705, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.1912 - acc: 0.9084 - val_loss: 0.2131 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.19705\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.1863 - acc: 0.9082 - val_loss: 0.1804 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.19705 to 0.18041, saving model to keras_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.1840 - acc: 0.9113 - val_loss: 0.1591 - val_acc: 0.9171\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.18041 to 0.15914, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.1805 - acc: 0.9134 - val_loss: 0.1648 - val_acc: 0.9162\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.15914\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.2350 - acc: 0.8928 - val_loss: 0.1838 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.15914\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 0.1839 - acc: 0.9156 - val_loss: 0.1768 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.15914\n",
      "Test accuracy: 0.9002940728982344\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3753 - acc: 0.8558 - val_loss: 0.3659 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36586, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.3348 - acc: 0.8652 - val_loss: 0.3344 - val_acc: 0.8534\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36586 to 0.33437, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3151 - acc: 0.8701 - val_loss: 0.2551 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33437 to 0.25515, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.2624 - acc: 0.8737 - val_loss: 0.2198 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25515 to 0.21976, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.2688 - acc: 0.8741 - val_loss: 0.2320 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21976\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.2298 - acc: 0.8907 - val_loss: 0.2098 - val_acc: 0.8956\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.21976 to 0.20984, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.1986 - acc: 0.9064 - val_loss: 0.1841 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20984 to 0.18406, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.1627 - acc: 0.9362 - val_loss: 0.0985 - val_acc: 0.9669\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18406 to 0.09846, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.1055 - acc: 0.9636 - val_loss: 0.1040 - val_acc: 0.9632\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09846\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.0964 - acc: 0.9648 - val_loss: 0.1221 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09846\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.1208 - acc: 0.9544 - val_loss: 0.0745 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09846 to 0.07445, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.0832 - acc: 0.9709 - val_loss: 0.1200 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07445\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.0922 - acc: 0.9667 - val_loss: 0.0800 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07445\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0765 - acc: 0.9726 - val_loss: 0.0515 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07445 to 0.05154, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.0962 - acc: 0.9645 - val_loss: 0.0733 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.05154\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0672 - acc: 0.9755 - val_loss: 0.0596 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.05154\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0649 - acc: 0.9770 - val_loss: 0.0446 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05154 to 0.04457, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0628 - acc: 0.9792 - val_loss: 0.0449 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04457\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.0511 - acc: 0.9824 - val_loss: 0.0458 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04457\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0477 - acc: 0.9818 - val_loss: 0.0543 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04457\n",
      "Test accuracy: 0.9690645889059584\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 38s 6ms/step - loss: 0.4221 - acc: 0.8275 - val_loss: 0.3533 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35327, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 36s 5ms/step - loss: 0.3415 - acc: 0.8675 - val_loss: 0.2968 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35327 to 0.29680, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 36s 5ms/step - loss: 0.3055 - acc: 0.8710 - val_loss: 0.2526 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.29680 to 0.25258, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.2992 - acc: 0.8671 - val_loss: 0.3517 - val_acc: 0.8418\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25258\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.3206 - acc: 0.8524 - val_loss: 0.2431 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25258 to 0.24310, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.2744 - acc: 0.8638 - val_loss: 0.2300 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24310 to 0.22995, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.2294 - acc: 0.8937 - val_loss: 0.2041 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22995 to 0.20409, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.2579 - acc: 0.8778 - val_loss: 0.2564 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.20409\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.2303 - acc: 0.8987 - val_loss: 0.2064 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.20409\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 38s 6ms/step - loss: 0.2515 - acc: 0.8966 - val_loss: 0.2916 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20409\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 36s 5ms/step - loss: 0.2108 - acc: 0.9124 - val_loss: 0.2064 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20409\n",
      "Test accuracy: 0.9107001472781543\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 38s 6ms/step - loss: 0.4239 - acc: 0.8393 - val_loss: 0.3904 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39040, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.3229 - acc: 0.8699 - val_loss: 0.3108 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39040 to 0.31077, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.3172 - acc: 0.8688 - val_loss: 0.2748 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31077 to 0.27476, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.3330 - acc: 0.8624 - val_loss: 0.3920 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27476\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.3265 - acc: 0.8604 - val_loss: 0.3126 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27476\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.2981 - acc: 0.8677 - val_loss: 0.2669 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27476 to 0.26687, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.2938 - acc: 0.8672 - val_loss: 0.2616 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26687 to 0.26158, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.3324 - acc: 0.8592 - val_loss: 0.2969 - val_acc: 0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss did not improve from 0.26158\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.2559 - acc: 0.8881 - val_loss: 0.2931 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.26158\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.2981 - acc: 0.8682 - val_loss: 0.2559 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.26158 to 0.25593, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.2379 - acc: 0.8886 - val_loss: 0.2085 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.25593 to 0.20854, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.2159 - acc: 0.8993 - val_loss: 0.2128 - val_acc: 0.9004\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.20854\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.2301 - acc: 0.8965 - val_loss: 0.1898 - val_acc: 0.9086\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.20854 to 0.18983, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.1921 - acc: 0.9100 - val_loss: 0.1549 - val_acc: 0.9312\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.18983 to 0.15487, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.1607 - acc: 0.9259 - val_loss: 0.1388 - val_acc: 0.9502\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.15487 to 0.13883, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.1488 - acc: 0.9384 - val_loss: 0.1048 - val_acc: 0.9686\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13883 to 0.10483, saving model to keras_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.1226 - acc: 0.9563 - val_loss: 0.0817 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.10483 to 0.08167, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.1013 - acc: 0.9665 - val_loss: 0.0697 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08167 to 0.06971, saving model to keras_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.0758 - acc: 0.9735 - val_loss: 0.0636 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06971 to 0.06362, saving model to keras_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 37s 6ms/step - loss: 0.0677 - acc: 0.9764 - val_loss: 0.0529 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.06362 to 0.05293, saving model to keras_weights.hdf5\n",
      "Test accuracy: 0.9661237440313126\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.3810 - acc: 0.8479 - val_loss: 0.3945 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39452, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3045 - acc: 0.8757 - val_loss: 0.2509 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39452 to 0.25086, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2856 - acc: 0.8747 - val_loss: 0.3135 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25086\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2918 - acc: 0.8738 - val_loss: 0.3056 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25086\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2645 - acc: 0.8813 - val_loss: 0.2204 - val_acc: 0.9075\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25086 to 0.22042, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2902 - acc: 0.8811 - val_loss: 0.4999 - val_acc: 0.8707\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.22042\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3361 - acc: 0.8568 - val_loss: 0.2586 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22042\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2647 - acc: 0.8822 - val_loss: 0.2409 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22042\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2730 - acc: 0.8814 - val_loss: 0.2502 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22042\n",
      "Test accuracy: 0.8794253853607631\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 33s 5ms/step - loss: 0.4366 - acc: 0.8382 - val_loss: 0.3617 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36167, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 32s 5ms/step - loss: 0.3413 - acc: 0.8647 - val_loss: 0.3508 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36167 to 0.35080, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 32s 5ms/step - loss: 0.3465 - acc: 0.8646 - val_loss: 0.3464 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35080 to 0.34641, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 33s 5ms/step - loss: 0.3243 - acc: 0.8714 - val_loss: 0.3044 - val_acc: 0.8778\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34641 to 0.30440, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 33s 5ms/step - loss: 0.3174 - acc: 0.8697 - val_loss: 0.3314 - val_acc: 0.8430\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30440\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 33s 5ms/step - loss: 0.3226 - acc: 0.8670 - val_loss: 0.3210 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30440\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 33s 5ms/step - loss: 0.3350 - acc: 0.8529 - val_loss: 0.4497 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.30440\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 33s 5ms/step - loss: 0.3751 - acc: 0.8447 - val_loss: 0.3614 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.30440\n",
      "Test accuracy: 0.8423820672740684\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 0.3902 - acc: 0.8506 - val_loss: 0.3456 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34558, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3885 - acc: 0.8528 - val_loss: 0.3504 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34558\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3604 - acc: 0.8608 - val_loss: 0.3435 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34558 to 0.34354, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3230 - acc: 0.8726 - val_loss: 0.2824 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34354 to 0.28245, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2812 - acc: 0.8804 - val_loss: 0.2334 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28245 to 0.23341, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2243 - acc: 0.8965 - val_loss: 0.2228 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23341 to 0.22276, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.2113 - acc: 0.9025 - val_loss: 0.1732 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22276 to 0.17318, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.1684 - acc: 0.9237 - val_loss: 0.1773 - val_acc: 0.9179\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.17318\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.1500 - acc: 0.9290 - val_loss: 0.1549 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.17318 to 0.15495, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.1416 - acc: 0.9356 - val_loss: 0.1221 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15495 to 0.12209, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.1325 - acc: 0.9542 - val_loss: 0.1103 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12209 to 0.11034, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.1287 - acc: 0.9551 - val_loss: 0.1163 - val_acc: 0.9553\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11034\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.1444 - acc: 0.9426 - val_loss: 0.0968 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11034 to 0.09677, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0956 - acc: 0.9666 - val_loss: 0.0713 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09677 to 0.07127, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0787 - acc: 0.9727 - val_loss: 0.0699 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07127 to 0.06986, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0904 - acc: 0.9689 - val_loss: 0.0429 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.06986 to 0.04290, saving model to keras_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0612 - acc: 0.9792 - val_loss: 0.0443 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04290\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0654 - acc: 0.9772 - val_loss: 0.0530 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04290\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0580 - acc: 0.9799 - val_loss: 0.0598 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04290\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.0509 - acc: 0.9820 - val_loss: 0.0511 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04290\n",
      "Test accuracy: 0.9704219014271664\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 34s 5ms/step - loss: 0.4649 - acc: 0.8040 - val_loss: 0.3670 - val_acc: 0.8551\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36702, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 34s 5ms/step - loss: 0.3570 - acc: 0.8576 - val_loss: 0.3405 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36702 to 0.34055, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 34s 5ms/step - loss: 0.3375 - acc: 0.8654 - val_loss: 0.2864 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34055 to 0.28638, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 0.3234 - acc: 0.8721 - val_loss: 0.2895 - val_acc: 0.8727\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.28638\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 0.3052 - acc: 0.8771 - val_loss: 0.2742 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28638 to 0.27418, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 0.3081 - acc: 0.8747 - val_loss: 0.2388 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27418 to 0.23881, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 0.2768 - acc: 0.8800 - val_loss: 0.2090 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23881 to 0.20901, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 0.3466 - acc: 0.8579 - val_loss: 0.2717 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.20901\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 0.3074 - acc: 0.8587 - val_loss: 0.2324 - val_acc: 0.8928\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.20901\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 0.2605 - acc: 0.8789 - val_loss: 0.2184 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20901\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 0.2892 - acc: 0.8800 - val_loss: 0.2687 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20901\n",
      "Test accuracy: 0.8750141196668371\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.3720 - acc: 0.8540 - val_loss: 0.3334 - val_acc: 0.8871\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33342, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.3527 - acc: 0.8564 - val_loss: 0.3381 - val_acc: 0.8390\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33342\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.3072 - acc: 0.8579 - val_loss: 0.2582 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33342 to 0.25822, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.2580 - acc: 0.8800 - val_loss: 0.2624 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25822\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.2269 - acc: 0.8917 - val_loss: 0.1786 - val_acc: 0.9171\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25822 to 0.17863, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.2074 - acc: 0.9034 - val_loss: 0.2036 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.17863\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.1825 - acc: 0.9155 - val_loss: 0.1617 - val_acc: 0.9332\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.17863 to 0.16173, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.1964 - acc: 0.9163 - val_loss: 0.3593 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16173\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.1736 - acc: 0.9308 - val_loss: 0.2035 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16173\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.1270 - acc: 0.9545 - val_loss: 0.1329 - val_acc: 0.9499\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.16173 to 0.13288, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0924 - acc: 0.9686 - val_loss: 0.0763 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.13288 to 0.07634, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0920 - acc: 0.9674 - val_loss: 0.1401 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07634\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0898 - acc: 0.9685 - val_loss: 0.0507 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.07634 to 0.05075, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0728 - acc: 0.9749 - val_loss: 0.0498 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05075 to 0.04980, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0595 - acc: 0.9794 - val_loss: 0.0550 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04980\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0562 - acc: 0.9799 - val_loss: 0.0417 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.04980 to 0.04174, saving model to keras_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0716 - acc: 0.9745 - val_loss: 0.0505 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04174\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0839 - acc: 0.9738 - val_loss: 0.0524 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04174\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0605 - acc: 0.9791 - val_loss: 0.0564 - val_acc: 0.9663\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04174\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.0552 - acc: 0.9805 - val_loss: 0.0419 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04174\n",
      "Test accuracy: 0.9674245072576123\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.4057 - acc: 0.8454 - val_loss: 0.3559 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35593, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.3443 - acc: 0.8658 - val_loss: 0.3318 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35593 to 0.33184, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.3357 - acc: 0.8669 - val_loss: 0.3123 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33184 to 0.31229, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.3543 - acc: 0.8563 - val_loss: 0.4458 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31229\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.3449 - acc: 0.8653 - val_loss: 0.3724 - val_acc: 0.8551\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31229\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.3335 - acc: 0.8668 - val_loss: 0.3788 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31229\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.3362 - acc: 0.8658 - val_loss: 0.3687 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31229\n",
      "Test accuracy: 0.8532405655020813\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.4095 - acc: 0.8445 - val_loss: 0.3532 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35320, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3458 - acc: 0.8647 - val_loss: 0.3222 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35320 to 0.32224, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3825 - acc: 0.8554 - val_loss: 0.3489 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.32224\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.3440 - acc: 0.8627 - val_loss: 0.3662 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.32224\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.3596 - acc: 0.8611 - val_loss: 0.3367 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.32224\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.3495 - acc: 0.8630 - val_loss: 0.3418 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32224\n",
      "Test accuracy: 0.8598009043925894\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3958 - acc: 0.8488 - val_loss: 0.3686 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36860, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.3484 - acc: 0.8623 - val_loss: 0.3648 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36860 to 0.36475, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.3416 - acc: 0.8557 - val_loss: 0.2961 - val_acc: 0.8721\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36475 to 0.29611, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.3565 - acc: 0.8507 - val_loss: 0.3637 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29611\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.3236 - acc: 0.8618 - val_loss: 0.2613 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.29611 to 0.26130, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.2657 - acc: 0.8798 - val_loss: 0.2524 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26130 to 0.25236, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.2249 - acc: 0.8969 - val_loss: 0.2070 - val_acc: 0.9024\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25236 to 0.20702, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.2116 - acc: 0.9001 - val_loss: 0.2070 - val_acc: 0.9004\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.20702 to 0.20696, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2199 - acc: 0.8998 - val_loss: 0.2217 - val_acc: 0.8947\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.20696\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.2242 - acc: 0.8928 - val_loss: 0.2001 - val_acc: 0.9089\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.20696 to 0.20012, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.2583 - acc: 0.8851 - val_loss: 0.2719 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20012\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3298 - acc: 0.8683 - val_loss: 0.2124 - val_acc: 0.9061\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.20012\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.2579 - acc: 0.8933 - val_loss: 0.2294 - val_acc: 0.9114\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.20012\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.2005 - acc: 0.9159 - val_loss: 0.2327 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.20012\n",
      "Test accuracy: 0.904648797306563\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.4116 - acc: 0.8387 - val_loss: 0.3763 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37633, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.3670 - acc: 0.8551 - val_loss: 0.3185 - val_acc: 0.8763\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37633 to 0.31849, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3471 - acc: 0.8632 - val_loss: 0.3463 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31849\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3235 - acc: 0.8707 - val_loss: 0.2591 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31849 to 0.25914, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2801 - acc: 0.8791 - val_loss: 0.2316 - val_acc: 0.8984\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25914 to 0.23164, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2442 - acc: 0.8892 - val_loss: 0.2260 - val_acc: 0.8902\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23164 to 0.22599, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2158 - acc: 0.9011 - val_loss: 0.2066 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22599 to 0.20657, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2243 - acc: 0.8981 - val_loss: 0.2400 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.20657\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2093 - acc: 0.9043 - val_loss: 0.2014 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.20657 to 0.20138, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.2411 - acc: 0.8958 - val_loss: 0.2081 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20138\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2241 - acc: 0.9084 - val_loss: 0.1661 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.20138 to 0.16610, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 56s 8ms/step - loss: 0.1802 - acc: 0.9272 - val_loss: 0.1543 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.16610 to 0.15431, saving model to keras_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2156 - acc: 0.9039 - val_loss: 0.1435 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15431 to 0.14353, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.1976 - acc: 0.9259 - val_loss: 0.1346 - val_acc: 0.9513\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14353 to 0.13463, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1467 - acc: 0.9504 - val_loss: 0.1211 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.13463 to 0.12109, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.1552 - acc: 0.9468 - val_loss: 0.1809 - val_acc: 0.9072\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.12109\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.1542 - acc: 0.9381 - val_loss: 0.1009 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.12109 to 0.10085, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1256 - acc: 0.9562 - val_loss: 0.0710 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10085 to 0.07096, saving model to keras_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.0927 - acc: 0.9706 - val_loss: 0.0905 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.07096\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.1235 - acc: 0.9538 - val_loss: 0.0890 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.07096\n",
      "Test accuracy: 0.9479696942378919\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.3772 - acc: 0.8508 - val_loss: 0.3473 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34735, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3369 - acc: 0.8632 - val_loss: 0.3620 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34735\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.3205 - acc: 0.8662 - val_loss: 0.2532 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34735 to 0.25319, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2995 - acc: 0.8655 - val_loss: 0.2605 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25319\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.2498 - acc: 0.8833 - val_loss: 0.2398 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25319 to 0.23978, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.2226 - acc: 0.8974 - val_loss: 0.2508 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.23978\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2401 - acc: 0.8946 - val_loss: 0.2181 - val_acc: 0.8978\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23978 to 0.21813, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2742 - acc: 0.8877 - val_loss: 0.4846 - val_acc: 0.8376\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21813\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.3897 - acc: 0.8473 - val_loss: 0.3819 - val_acc: 0.8684\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21813\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2998 - acc: 0.8811 - val_loss: 0.3631 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21813\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3330 - acc: 0.8635 - val_loss: 0.3405 - val_acc: 0.8594\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21813\n",
      "Test accuracy: 0.8609320095607212\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.3882 - acc: 0.8494 - val_loss: 0.3028 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30284, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.3524 - acc: 0.8614 - val_loss: 0.3464 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.30284\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 56s 8ms/step - loss: 0.3472 - acc: 0.8638 - val_loss: 0.3375 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.30284\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.3339 - acc: 0.8683 - val_loss: 0.3421 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.30284\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.3141 - acc: 0.8748 - val_loss: 0.2983 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30284 to 0.29830, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.2847 - acc: 0.8796 - val_loss: 0.2553 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29830 to 0.25532, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.2436 - acc: 0.8882 - val_loss: 0.2108 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25532 to 0.21078, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.2259 - acc: 0.8944 - val_loss: 0.2148 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21078\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.2214 - acc: 0.8950 - val_loss: 0.2490 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21078\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.2083 - acc: 0.9017 - val_loss: 0.1998 - val_acc: 0.9004\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21078 to 0.19975, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2878 - acc: 0.8699 - val_loss: 0.5117 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.19975\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2675 - acc: 0.8822 - val_loss: 0.2068 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19975\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.2311 - acc: 0.8963 - val_loss: 0.2143 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19975\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2054 - acc: 0.9082 - val_loss: 0.1876 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.19975 to 0.18758, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.1930 - acc: 0.9184 - val_loss: 0.1768 - val_acc: 0.9273\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.18758 to 0.17685, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.1803 - acc: 0.9250 - val_loss: 0.2099 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.17685\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.1836 - acc: 0.9282 - val_loss: 0.1013 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.17685 to 0.10128, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.1300 - acc: 0.9533 - val_loss: 0.0969 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10128 to 0.09692, saving model to keras_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1335 - acc: 0.9557 - val_loss: 0.1251 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09692\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.3636 - acc: 0.8644 - val_loss: 0.2912 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09692\n",
      "Test accuracy: 0.8704897469692728\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 58s 9ms/step - loss: 0.5708 - acc: 0.7122 - val_loss: 0.3957 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39571, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.4841 - acc: 0.7994 - val_loss: 0.3759 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39571 to 0.37589, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.4447 - acc: 0.8289 - val_loss: 0.3332 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37589 to 0.33318, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.4175 - acc: 0.8433 - val_loss: 0.3343 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33318\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 58s 9ms/step - loss: 0.4467 - acc: 0.8430 - val_loss: 0.3576 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33318\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.4114 - acc: 0.8542 - val_loss: 0.3548 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.33318\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.3963 - acc: 0.8572 - val_loss: 0.3271 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.33318 to 0.32710, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.3854 - acc: 0.8604 - val_loss: 0.3110 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.32710 to 0.31103, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 56s 8ms/step - loss: 0.3756 - acc: 0.8627 - val_loss: 0.3069 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.31103 to 0.30695, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.3713 - acc: 0.8648 - val_loss: 0.3016 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.30695 to 0.30163, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.3673 - acc: 0.8641 - val_loss: 0.2883 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.30163 to 0.28828, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.3574 - acc: 0.8667 - val_loss: 0.3211 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.28828\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.3721 - acc: 0.8606 - val_loss: 0.3037 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.28828\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.3487 - acc: 0.8683 - val_loss: 0.2814 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.28828 to 0.28144, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.3392 - acc: 0.8689 - val_loss: 0.2778 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.28144 to 0.27781, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 58s 9ms/step - loss: 0.3394 - acc: 0.8664 - val_loss: 0.2623 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.27781 to 0.26228, saving model to keras_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 56s 8ms/step - loss: 0.3232 - acc: 0.8685 - val_loss: 0.2534 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.26228 to 0.25341, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 58s 9ms/step - loss: 0.3264 - acc: 0.8666 - val_loss: 0.2569 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.25341\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 56s 8ms/step - loss: 0.3344 - acc: 0.8691 - val_loss: 0.2560 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.25341\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.3120 - acc: 0.8696 - val_loss: 0.2467 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.25341 to 0.24674, saving model to keras_weights.hdf5\n",
      "Test accuracy: 0.8718470510766593\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.3932 - acc: 0.8451 - val_loss: 0.3434 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34342, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 58s 9ms/step - loss: 0.3541 - acc: 0.8597 - val_loss: 0.3426 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34342 to 0.34257, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.3071 - acc: 0.8752 - val_loss: 0.2800 - val_acc: 0.8778\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34257 to 0.28001, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.2870 - acc: 0.8755 - val_loss: 0.2393 - val_acc: 0.8959\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.28001 to 0.23926, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.2848 - acc: 0.8752 - val_loss: 0.2754 - val_acc: 0.8690\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.23926\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.2530 - acc: 0.8828 - val_loss: 0.2056 - val_acc: 0.9080\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23926 to 0.20557, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.2455 - acc: 0.8894 - val_loss: 0.2884 - val_acc: 0.8690\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.20557\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.2706 - acc: 0.8819 - val_loss: 0.2506 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.20557\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.2470 - acc: 0.8883 - val_loss: 0.2234 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.20557\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.2137 - acc: 0.9083 - val_loss: 0.2174 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20557\n",
      "Test accuracy: 0.8940730725137266\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.4129 - acc: 0.8321 - val_loss: 0.3420 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34198, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3405 - acc: 0.8683 - val_loss: 0.2929 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34198 to 0.29287, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3438 - acc: 0.8620 - val_loss: 0.3003 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.29287\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3736 - acc: 0.8468 - val_loss: 0.3574 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29287\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3687 - acc: 0.8461 - val_loss: 0.3248 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29287\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3133 - acc: 0.8692 - val_loss: 0.2710 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29287 to 0.27099, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2854 - acc: 0.8740 - val_loss: 0.2525 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.27099 to 0.25249, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2879 - acc: 0.8762 - val_loss: 0.2596 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25249\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2984 - acc: 0.8727 - val_loss: 0.2360 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25249 to 0.23601, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3244 - acc: 0.8704 - val_loss: 0.3457 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.23601\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3331 - acc: 0.8690 - val_loss: 0.2941 - val_acc: 0.8769\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.23601\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3361 - acc: 0.8680 - val_loss: 0.3687 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.23601\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3146 - acc: 0.8640 - val_loss: 0.2475 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.23601\n",
      "Test accuracy: 0.8778418523601083\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.3800 - acc: 0.8508 - val_loss: 0.4243 - val_acc: 0.8401\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42434, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3622 - acc: 0.8536 - val_loss: 0.3078 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42434 to 0.30784, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3714 - acc: 0.8524 - val_loss: 0.3437 - val_acc: 0.8404\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.30784\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.3118 - acc: 0.8507 - val_loss: 0.2842 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30784 to 0.28421, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2444 - acc: 0.8806 - val_loss: 0.2239 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28421 to 0.22394, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2207 - acc: 0.8982 - val_loss: 0.2576 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.22394\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2448 - acc: 0.8903 - val_loss: 0.2328 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22394\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2919 - acc: 0.8803 - val_loss: 0.2157 - val_acc: 0.9072\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.22394 to 0.21566, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2308 - acc: 0.8935 - val_loss: 0.1687 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.21566 to 0.16873, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.2411 - acc: 0.9038 - val_loss: 0.1560 - val_acc: 0.9448\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.16873 to 0.15604, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.1452 - acc: 0.9482 - val_loss: 0.1042 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.15604 to 0.10417, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.1078 - acc: 0.9635 - val_loss: 0.0546 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10417 to 0.05463, saving model to keras_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.1014 - acc: 0.9645 - val_loss: 0.0688 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.05463\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.0861 - acc: 0.9714 - val_loss: 0.0489 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05463 to 0.04888, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 51s 7ms/step - loss: 0.0888 - acc: 0.9689 - val_loss: 0.0682 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04888\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 51s 7ms/step - loss: 0.0701 - acc: 0.9755 - val_loss: 0.0548 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04888\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 51s 7ms/step - loss: 0.0624 - acc: 0.9767 - val_loss: 0.0458 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.04888 to 0.04584, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 51s 7ms/step - loss: 0.0883 - acc: 0.9666 - val_loss: 0.0467 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04584\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 51s 7ms/step - loss: 0.0599 - acc: 0.9774 - val_loss: 0.0420 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.04584 to 0.04205, saving model to keras_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.0574 - acc: 0.9787 - val_loss: 0.0536 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04205\n",
      "Test accuracy: 0.9662934100381148\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 64s 9ms/step - loss: 0.4100 - acc: 0.8443 - val_loss: 0.3698 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36981, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 58s 9ms/step - loss: 0.4482 - acc: 0.8318 - val_loss: 0.4371 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36981\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.4344 - acc: 0.8353 - val_loss: 0.3969 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36981\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 58s 9ms/step - loss: 0.3948 - acc: 0.8469 - val_loss: 0.3638 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36981 to 0.36380, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 63s 9ms/step - loss: 0.3360 - acc: 0.8590 - val_loss: 0.2562 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36380 to 0.25617, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.2806 - acc: 0.8685 - val_loss: 0.2678 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25617\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.2752 - acc: 0.8633 - val_loss: 0.3313 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25617\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.2884 - acc: 0.8564 - val_loss: 0.2604 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25617\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.2584 - acc: 0.8669 - val_loss: 0.2494 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25617 to 0.24935, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.2426 - acc: 0.8844 - val_loss: 0.2248 - val_acc: 0.8970\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.24935 to 0.22485, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 65s 10ms/step - loss: 0.2416 - acc: 0.8842 - val_loss: 0.2470 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22485\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 62s 9ms/step - loss: 0.2756 - acc: 0.8621 - val_loss: 0.2360 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22485\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.2477 - acc: 0.8703 - val_loss: 0.2276 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22485\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.3006 - acc: 0.8660 - val_loss: 0.2797 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22485\n",
      "Test accuracy: 0.8695283183454456\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.4058 - acc: 0.8432 - val_loss: 0.3528 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35284, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3545 - acc: 0.8624 - val_loss: 0.3282 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35284 to 0.32817, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3738 - acc: 0.8542 - val_loss: 0.3485 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.32817\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3128 - acc: 0.8618 - val_loss: 0.2727 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32817 to 0.27271, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2585 - acc: 0.8714 - val_loss: 0.2325 - val_acc: 0.8928\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.27271 to 0.23246, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3786 - acc: 0.8295 - val_loss: 0.3155 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.23246\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3207 - acc: 0.8383 - val_loss: 0.2872 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.23246\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2831 - acc: 0.8611 - val_loss: 0.2521 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.23246\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2541 - acc: 0.8768 - val_loss: 0.2291 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23246 to 0.22912, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2442 - acc: 0.8810 - val_loss: 0.2090 - val_acc: 0.9052\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22912 to 0.20901, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2217 - acc: 0.8942 - val_loss: 0.2004 - val_acc: 0.9004\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.20901 to 0.20040, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2242 - acc: 0.8950 - val_loss: 0.2211 - val_acc: 0.8933\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.20040\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2176 - acc: 0.8950 - val_loss: 0.1888 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.20040 to 0.18879, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2141 - acc: 0.8981 - val_loss: 0.1879 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.18879 to 0.18787, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2014 - acc: 0.9036 - val_loss: 0.1636 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.18787 to 0.16364, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1782 - acc: 0.9152 - val_loss: 0.1415 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.16364 to 0.14145, saving model to keras_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.1601 - acc: 0.9294 - val_loss: 0.1619 - val_acc: 0.9213\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14145\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.1690 - acc: 0.9294 - val_loss: 0.1695 - val_acc: 0.9273\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14145\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 51s 7ms/step - loss: 0.1500 - acc: 0.9387 - val_loss: 0.0952 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.14145 to 0.09515, saving model to keras_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1384 - acc: 0.9461 - val_loss: 0.0920 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09515 to 0.09195, saving model to keras_weights.hdf5\n",
      "Test accuracy: 0.9449723013627719\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.3777 - acc: 0.8514 - val_loss: 0.3386 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33860, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3116 - acc: 0.8683 - val_loss: 0.3084 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33860 to 0.30845, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3137 - acc: 0.8690 - val_loss: 0.3279 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.30845\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2793 - acc: 0.8726 - val_loss: 0.2501 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30845 to 0.25008, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2364 - acc: 0.8895 - val_loss: 0.1999 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25008 to 0.19993, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2004 - acc: 0.9108 - val_loss: 0.1908 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.19993 to 0.19077, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2025 - acc: 0.9117 - val_loss: 0.1819 - val_acc: 0.9247\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19077 to 0.18187, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1725 - acc: 0.9335 - val_loss: 0.1458 - val_acc: 0.9409\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18187 to 0.14583, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1512 - acc: 0.9431 - val_loss: 0.1465 - val_acc: 0.9346\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14583\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1163 - acc: 0.9593 - val_loss: 0.1222 - val_acc: 0.9428\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14583 to 0.12224, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1151 - acc: 0.9615 - val_loss: 0.0791 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12224 to 0.07909, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1031 - acc: 0.9638 - val_loss: 0.0499 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.07909 to 0.04988, saving model to keras_weights.hdf5\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0769 - acc: 0.9716 - val_loss: 0.0672 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04988\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0670 - acc: 0.9789 - val_loss: 0.0426 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04988 to 0.04256, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0738 - acc: 0.9724 - val_loss: 0.1147 - val_acc: 0.9502\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04256\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 56s 8ms/step - loss: 0.0722 - acc: 0.9753 - val_loss: 0.0659 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04256\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0573 - acc: 0.9795 - val_loss: 0.0387 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.04256 to 0.03869, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0527 - acc: 0.9808 - val_loss: 0.0486 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03869\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0706 - acc: 0.9731 - val_loss: 0.0405 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.03869\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0570 - acc: 0.9780 - val_loss: 0.0455 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03869\n",
      "Test accuracy: 0.9579798734345839\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.3794 - acc: 0.8523 - val_loss: 0.3449 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34489, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3479 - acc: 0.8601 - val_loss: 0.3970 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34489\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3371 - acc: 0.8635 - val_loss: 0.3172 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34489 to 0.31724, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3375 - acc: 0.8654 - val_loss: 0.3190 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31724\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3220 - acc: 0.8631 - val_loss: 0.2691 - val_acc: 0.8755\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31724 to 0.26913, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2558 - acc: 0.8846 - val_loss: 0.2452 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26913 to 0.24519, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2230 - acc: 0.8962 - val_loss: 0.2557 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24519\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2263 - acc: 0.8967 - val_loss: 0.2227 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24519 to 0.22267, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2092 - acc: 0.9034 - val_loss: 0.2024 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.22267 to 0.20238, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1887 - acc: 0.9103 - val_loss: 0.2305 - val_acc: 0.8913\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20238\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1861 - acc: 0.9157 - val_loss: 0.1666 - val_acc: 0.9352\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.20238 to 0.16658, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1665 - acc: 0.9292 - val_loss: 0.1996 - val_acc: 0.9080\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.16658\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1510 - acc: 0.9408 - val_loss: 0.1497 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.16658 to 0.14965, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1569 - acc: 0.9372 - val_loss: 0.1272 - val_acc: 0.9493\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14965 to 0.12718, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1155 - acc: 0.9563 - val_loss: 0.0726 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.12718 to 0.07256, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.1003 - acc: 0.9639 - val_loss: 0.1026 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.07256\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0873 - acc: 0.9669 - val_loss: 0.0935 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.07256\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0880 - acc: 0.9671 - val_loss: 0.0613 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.07256 to 0.06135, saving model to keras_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0813 - acc: 0.9684 - val_loss: 0.0835 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.06135\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0689 - acc: 0.9760 - val_loss: 0.0866 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06135\n",
      "Test accuracy: 0.9557176854677591\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.3828 - acc: 0.8532 - val_loss: 0.3006 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30063, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3044 - acc: 0.8734 - val_loss: 0.2999 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.30063 to 0.29991, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3701 - acc: 0.8503 - val_loss: 0.3093 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.29991\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3165 - acc: 0.8737 - val_loss: 0.3115 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29991\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3387 - acc: 0.8618 - val_loss: 0.3490 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29991\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.3188 - acc: 0.8615 - val_loss: 0.2592 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29991 to 0.25915, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2607 - acc: 0.8716 - val_loss: 0.2192 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25915 to 0.21922, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2372 - acc: 0.8861 - val_loss: 0.2101 - val_acc: 0.9049\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.21922 to 0.21008, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2421 - acc: 0.8872 - val_loss: 0.2684 - val_acc: 0.8891\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21008\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 5009s 741ms/step - loss: 0.2300 - acc: 0.8959 - val_loss: 0.2265 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21008\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.1956 - acc: 0.9148 - val_loss: 0.1954 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.21008 to 0.19542, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.2951 - acc: 0.8679 - val_loss: 0.2589 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19542\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.2806 - acc: 0.8669 - val_loss: 0.2464 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19542\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.2242 - acc: 0.8971 - val_loss: 0.1606 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.19542 to 0.16058, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.1899 - acc: 0.9162 - val_loss: 0.1589 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.16058 to 0.15894, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1468 - acc: 0.9412 - val_loss: 0.1588 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.15894 to 0.15876, saving model to keras_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.2244 - acc: 0.9054 - val_loss: 0.1652 - val_acc: 0.9276\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.15876\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.1796 - acc: 0.9301 - val_loss: 0.1177 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.15876 to 0.11768, saving model to keras_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1218 - acc: 0.9580 - val_loss: 0.0596 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11768 to 0.05961, saving model to keras_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.1235 - acc: 0.9562 - val_loss: 0.1272 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.05961\n",
      "Test accuracy: 0.9412396854572794\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.3863 - acc: 0.8540 - val_loss: 0.3491 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34907, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.3419 - acc: 0.8633 - val_loss: 0.2836 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34907 to 0.28364, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.3010 - acc: 0.8741 - val_loss: 0.2508 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28364 to 0.25079, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.2509 - acc: 0.8879 - val_loss: 0.2146 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25079 to 0.21461, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.2462 - acc: 0.8840 - val_loss: 0.2597 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21461\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.2324 - acc: 0.8927 - val_loss: 0.2091 - val_acc: 0.9044\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.21461 to 0.20912, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.1765 - acc: 0.9251 - val_loss: 0.1894 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20912 to 0.18943, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.1596 - acc: 0.9381 - val_loss: 0.1101 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18943 to 0.11006, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.2480 - acc: 0.8934 - val_loss: 0.1537 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11006\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.1289 - acc: 0.9519 - val_loss: 0.1114 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11006\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 0.0947 - acc: 0.9680 - val_loss: 0.0708 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11006 to 0.07079, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.0785 - acc: 0.9721 - val_loss: 0.0903 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07079\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.0739 - acc: 0.9731 - val_loss: 0.0420 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.07079 to 0.04202, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.0686 - acc: 0.9767 - val_loss: 0.0421 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.04202\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.0538 - acc: 0.9800 - val_loss: 0.0403 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.04202 to 0.04026, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0553 - acc: 0.9796 - val_loss: 0.0494 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04026\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0517 - acc: 0.9805 - val_loss: 0.0447 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04026\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0573 - acc: 0.9788 - val_loss: 0.0401 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.04026 to 0.04009, saving model to keras_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0486 - acc: 0.9821 - val_loss: 0.0510 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04009\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0514 - acc: 0.9804 - val_loss: 0.0777 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04009\n",
      "Test accuracy: 0.9515891934314905\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.3831 - acc: 0.8534 - val_loss: 0.3254 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32542, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.3458 - acc: 0.8582 - val_loss: 0.2781 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32542 to 0.27807, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.3340 - acc: 0.8658 - val_loss: 0.3091 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.27807\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.3363 - acc: 0.8478 - val_loss: 0.5849 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27807\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.3551 - acc: 0.8601 - val_loss: 0.3663 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27807\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2957 - acc: 0.8688 - val_loss: 0.2288 - val_acc: 0.9024\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27807 to 0.22880, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2337 - acc: 0.8909 - val_loss: 0.2068 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22880 to 0.20679, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2158 - acc: 0.8962 - val_loss: 0.2106 - val_acc: 0.9063\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.20679\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.2074 - acc: 0.9003 - val_loss: 0.2018 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.20679 to 0.20180, saving model to keras_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.1798 - acc: 0.9170 - val_loss: 0.1852 - val_acc: 0.9168\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.20180 to 0.18524, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.1554 - acc: 0.9329 - val_loss: 0.1448 - val_acc: 0.9290\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.18524 to 0.14478, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.1311 - acc: 0.9490 - val_loss: 0.1145 - val_acc: 0.9448\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.14478 to 0.11453, saving model to keras_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0997 - acc: 0.9650 - val_loss: 0.0670 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11453 to 0.06698, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0920 - acc: 0.9675 - val_loss: 0.0543 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.06698 to 0.05430, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0721 - acc: 0.9756 - val_loss: 0.0445 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05430 to 0.04455, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0677 - acc: 0.9761 - val_loss: 0.0463 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04455\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0633 - acc: 0.9765 - val_loss: 0.0417 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.04455 to 0.04167, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0692 - acc: 0.9768 - val_loss: 0.0422 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04167\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0540 - acc: 0.9809 - val_loss: 0.0379 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.04167 to 0.03792, saving model to keras_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 0.0527 - acc: 0.9812 - val_loss: 0.0391 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03792\n",
      "Test accuracy: 0.9703653476017388\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 40s 6ms/step - loss: 0.4388 - acc: 0.8374 - val_loss: 0.3792 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37915, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.3806 - acc: 0.8478 - val_loss: 0.3482 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37915 to 0.34819, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 38s 6ms/step - loss: 0.3615 - acc: 0.8476 - val_loss: 0.3044 - val_acc: 0.8763\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34819 to 0.30438, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 38s 6ms/step - loss: 0.4019 - acc: 0.8522 - val_loss: 0.3721 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.30438\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 39s 6ms/step - loss: 0.3780 - acc: 0.8552 - val_loss: 0.4014 - val_acc: 0.8297\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30438\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 39s 6ms/step - loss: 0.3592 - acc: 0.8613 - val_loss: 0.3363 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30438\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 39s 6ms/step - loss: 0.3421 - acc: 0.8620 - val_loss: 0.3385 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.30438\n",
      "Test accuracy: 0.8612713258590866\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 0.3970 - acc: 0.8475 - val_loss: 0.3586 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35865, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.3629 - acc: 0.8598 - val_loss: 0.3515 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35865 to 0.35149, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.3511 - acc: 0.8634 - val_loss: 0.3497 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35149 to 0.34969, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.3248 - acc: 0.8711 - val_loss: 0.2713 - val_acc: 0.8834\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34969 to 0.27134, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.3152 - acc: 0.8741 - val_loss: 0.3434 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27134\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.2661 - acc: 0.8807 - val_loss: 0.2409 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27134 to 0.24094, saving model to keras_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.2525 - acc: 0.8840 - val_loss: 0.2303 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24094 to 0.23034, saving model to keras_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.2316 - acc: 0.8912 - val_loss: 0.2170 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23034 to 0.21701, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.2197 - acc: 0.8933 - val_loss: 0.2248 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21701\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1994 - acc: 0.9037 - val_loss: 0.1938 - val_acc: 0.9044\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21701 to 0.19376, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1773 - acc: 0.9153 - val_loss: 0.1603 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.19376 to 0.16033, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1904 - acc: 0.9119 - val_loss: 0.2202 - val_acc: 0.8894\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.16033\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1667 - acc: 0.9229 - val_loss: 0.1301 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.16033 to 0.13007, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1355 - acc: 0.9395 - val_loss: 0.1174 - val_acc: 0.9581\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13007 to 0.11738, saving model to keras_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1199 - acc: 0.9517 - val_loss: 0.0843 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11738 to 0.08431, saving model to keras_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1235 - acc: 0.9529 - val_loss: 0.1443 - val_acc: 0.9301\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08431\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.1158 - acc: 0.9502 - val_loss: 0.0834 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.08431 to 0.08340, saving model to keras_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.0864 - acc: 0.9693 - val_loss: 0.0778 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08340 to 0.07781, saving model to keras_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.0778 - acc: 0.9736 - val_loss: 0.0514 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.07781 to 0.05141, saving model to keras_weights.hdf5\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 43s 6ms/step - loss: 0.0657 - acc: 0.9769 - val_loss: 0.0536 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.05141\n",
      "Test accuracy: 0.9593937384867854\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.4247 - acc: 0.8378 - val_loss: 0.3400 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34005, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 0.3604 - acc: 0.8584 - val_loss: 0.3561 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34005\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 0.3416 - acc: 0.8628 - val_loss: 0.3332 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34005 to 0.33321, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 0.3157 - acc: 0.8755 - val_loss: 0.2873 - val_acc: 0.8755\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.33321 to 0.28734, saving model to keras_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 0.3346 - acc: 0.8728 - val_loss: 0.4861 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.28734\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 0.3515 - acc: 0.8559 - val_loss: 0.3323 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.28734\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 0.3220 - acc: 0.8627 - val_loss: 0.3333 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.28734\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 0.3252 - acc: 0.8636 - val_loss: 0.3369 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.28734\n",
      "Test accuracy: 0.862741747325584\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 0.4139 - acc: 0.8349 - val_loss: 0.3497 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34973, saving model to keras_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.3422 - acc: 0.8654 - val_loss: 0.3202 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34973 to 0.32021, saving model to keras_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 0.3232 - acc: 0.8633 - val_loss: 0.2526 - val_acc: 0.8885\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32021 to 0.25264, saving model to keras_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.3604 - acc: 0.8607 - val_loss: 0.3361 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25264\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.3005 - acc: 0.8696 - val_loss: 0.2304 - val_acc: 0.8947\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25264 to 0.23037, saving model to keras_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.3202 - acc: 0.8564 - val_loss: 0.2398 - val_acc: 0.8959\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.23037\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.2631 - acc: 0.8795 - val_loss: 0.2361 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.23037\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.2296 - acc: 0.8944 - val_loss: 0.2121 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23037 to 0.21207, saving model to keras_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.2141 - acc: 0.8998 - val_loss: 0.2859 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21207\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.2020 - acc: 0.9054 - val_loss: 0.1623 - val_acc: 0.9117\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21207 to 0.16233, saving model to keras_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.1869 - acc: 0.9121 - val_loss: 0.1417 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.16233 to 0.14170, saving model to keras_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.1563 - acc: 0.9294 - val_loss: 0.1152 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.14170 to 0.11520, saving model to keras_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.1377 - acc: 0.9436 - val_loss: 0.0909 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11520 to 0.09089, saving model to keras_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.1601 - acc: 0.9405 - val_loss: 0.1400 - val_acc: 0.9434\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09089\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.2148 - acc: 0.9038 - val_loss: 0.1404 - val_acc: 0.9270\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09089\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.1583 - acc: 0.9248 - val_loss: 0.1271 - val_acc: 0.9270\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09089\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 44s 7ms/step - loss: 0.1421 - acc: 0.9318 - val_loss: 0.1230 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09089\n",
      "Test accuracy: 0.9297025202289046\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                          data = data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=30,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='HAR_LSTM')  # saving to Har_LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dropout': 0.39730440328653105, 'batch_size': 0}\n"
     ]
    }
   ],
   "source": [
    "print(best_run)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best batch size is 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing best model on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of best performing model:\n",
      "[0.07511318304856766, 0.9704219014271664]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dropout': 0.39730440328653105, 'batch_size': 0}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train,Y_test = data()\n",
    "print(\"Evaluation of best performing model:\")\n",
    "    \n",
    "print(best_model.evaluate(X_test, Y_test,verbose=0))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "json.dump(best_run, open(\"best_run1.txt\", 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy both improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfd22107828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "plot_confusion_matrix(Y_test, best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning on more parameters\n",
    "\n",
    "### Hyperparameter: \n",
    "\n",
    "#### 1. Number of lstm units,\n",
    "\n",
    "#### 2. Number of lstm layers,\n",
    "\n",
    "#### 3. Optimzer : Adam or  RMSProp or  SGD with different learning rates\n",
    "\n",
    "#### 4. Batch Size\n",
    "\n",
    "Dropout is fixed here 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(X_train, X_test, Y_train,Y_test):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM({{choice([32, 64, 128])}}, input_shape=(128,9), return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    " \n",
    "    if conditional({{choice(['one', 'two'])}}) == 'two':\n",
    "        model.add(Dense({{choice([32,64, 128])}}))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Activation('relu'))\n",
    " \n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    " \n",
    "    adam = keras.optimizers.Adam(lr={{choice([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1])}}, clipnorm=1.)\n",
    "    rmsprop = keras.optimizers.RMSprop(lr={{choice([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1])}}, clipnorm=1.)\n",
    "    sgd = keras.optimizers.SGD(lr={{choice([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1])}}, clipnorm=1.)\n",
    " \n",
    "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = rmsprop\n",
    "    else:\n",
    "        optim = sgd\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer=optim)\n",
    " \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "    checkpointer = ModelCheckpoint(filepath='keras_lstm_weights.hdf5',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True)\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([32, 64, 128])}},\n",
    "              epochs=20,\n",
    "              validation_split=0.08,\n",
    "              callbacks=[early_stopping, checkpointer])\n",
    "\n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Best Parameters and corresponding Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle as pkl\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle as pkl\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe, space_eval\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.optimizers, keras.initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import MinMaxScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy, json\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'LSTM': hp.choice('LSTM', [32, 64, 128]),\n",
      "        'conditional': hp.choice('conditional', ['one', 'two']),\n",
      "        'Dense': hp.choice('Dense', [32,64, 128]),\n",
      "        'lr': hp.choice('lr', [10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1]),\n",
      "        'lr_1': hp.choice('lr_1', [10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1]),\n",
      "        'lr_2': hp.choice('lr_2', [10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1]),\n",
      "        'choiceval': hp.choice('choiceval', ['adam', 'sgd', 'rmsprop']),\n",
      "        'LSTM_1': hp.choice('LSTM_1', [32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: This function is separated from create_model() so that hyperopt\n",
      "  6: won't reload data for each evaluation run.\n",
      "  7: \"\"\"\n",
      "  8: # Loading the train and test data\n",
      "  9: X_train, X_test, Y_train, Y_test = pkl.load(open('har_data.pkl', 'rb'))\n",
      " 10: \n",
      " 11: \n",
      " 12: \n",
      " 13: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     model = Sequential()\n",
      "   4:     model.add(LSTM(space['LSTM'], input_shape=(128,9), return_sequences=False))\n",
      "   5:     model.add(Dropout(0.5))\n",
      "   6:     model.add(Activation('relu'))\n",
      "   7:  \n",
      "   8:     if conditional(space['conditional']) == 'two':\n",
      "   9:         model.add(Dense(space['Dense']))\n",
      "  10:         model.add(Dropout(0.5))\n",
      "  11:         model.add(Activation('relu'))\n",
      "  12:  \n",
      "  13:     model.add(Dense(6))\n",
      "  14:     model.add(Activation('softmax'))\n",
      "  15:  \n",
      "  16:     adam = keras.optimizers.Adam(lr=space['lr'], clipnorm=1.)\n",
      "  17:     rmsprop = keras.optimizers.RMSprop(lr=space['lr_1'], clipnorm=1.)\n",
      "  18:     sgd = keras.optimizers.SGD(lr=space['lr_2'], clipnorm=1.)\n",
      "  19:  \n",
      "  20:     choiceval = space['choiceval']\n",
      "  21:     if choiceval == 'adam':\n",
      "  22:         optim = adam\n",
      "  23:     elif choiceval == 'rmsprop':\n",
      "  24:         optim = rmsprop\n",
      "  25:     else:\n",
      "  26:         optim = sgd\n",
      "  27:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
      "  28:                   optimizer=optim)\n",
      "  29:  \n",
      "  30:     early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
      "  31:     checkpointer = ModelCheckpoint(filepath='keras_lstm_weights.hdf5',\n",
      "  32:                                    verbose=1,\n",
      "  33:                                    save_best_only=True)\n",
      "  34: \n",
      "  35:     model.fit(X_train, Y_train,\n",
      "  36:               batch_size=space['LSTM_1'],\n",
      "  37:               epochs=20,\n",
      "  38:               validation_split=0.08,\n",
      "  39:               callbacks=[early_stopping, checkpointer])\n",
      "  40: \n",
      "  41:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
      "  42: \n",
      "  43:     print('Test accuracy:', acc)\n",
      "  44:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  45: \n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 1.7973 - acc: 0.1436 - val_loss: 1.7931 - val_acc: 0.1171\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.79313, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7970 - acc: 0.1388 - val_loss: 1.7931 - val_acc: 0.1256\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.79313 to 1.79309, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7968 - acc: 0.1436 - val_loss: 1.7931 - val_acc: 0.1307\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.79309 to 1.79305, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7973 - acc: 0.1436 - val_loss: 1.7930 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.79305 to 1.79301, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7988 - acc: 0.1363 - val_loss: 1.7930 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.79301 to 1.79297, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7976 - acc: 0.1323 - val_loss: 1.7929 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.79297 to 1.79293, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7976 - acc: 0.1383 - val_loss: 1.7929 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.79293 to 1.79288, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7975 - acc: 0.1363 - val_loss: 1.7928 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.79288 to 1.79284, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7960 - acc: 0.1442 - val_loss: 1.7928 - val_acc: 0.1426\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.79284 to 1.79280, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7955 - acc: 0.1517 - val_loss: 1.7928 - val_acc: 0.1443\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.79280 to 1.79276, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7953 - acc: 0.1488 - val_loss: 1.7927 - val_acc: 0.1477\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.79276 to 1.79272, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7988 - acc: 0.1338 - val_loss: 1.7927 - val_acc: 0.1477\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.79272 to 1.79268, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7966 - acc: 0.1385 - val_loss: 1.7926 - val_acc: 0.1511\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.79268 to 1.79264, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7964 - acc: 0.1465 - val_loss: 1.7926 - val_acc: 0.1562\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.79264 to 1.79259, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7964 - acc: 0.1357 - val_loss: 1.7926 - val_acc: 0.1596\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.79259 to 1.79255, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7962 - acc: 0.1439 - val_loss: 1.7925 - val_acc: 0.1647\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.79255 to 1.79251, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7962 - acc: 0.1449 - val_loss: 1.7925 - val_acc: 0.1664\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.79251 to 1.79247, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7971 - acc: 0.1425 - val_loss: 1.7924 - val_acc: 0.1715\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.79247 to 1.79243, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7963 - acc: 0.1372 - val_loss: 1.7924 - val_acc: 0.1732\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.79243 to 1.79239, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.7970 - acc: 0.1403 - val_loss: 1.7923 - val_acc: 0.1749\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.79239 to 1.79235, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.16185951815405497\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 24s 3ms/step - loss: 1.7751 - acc: 0.1971 - val_loss: 1.7617 - val_acc: 0.4160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.76174, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.7381 - acc: 0.3389 - val_loss: 1.7331 - val_acc: 0.3430\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.76174 to 1.73310, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.7093 - acc: 0.3932 - val_loss: 1.7072 - val_acc: 0.3294\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.73310 to 1.70715, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.6782 - acc: 0.4032 - val_loss: 1.6790 - val_acc: 0.3277\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.70715 to 1.67898, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.6458 - acc: 0.4115 - val_loss: 1.6469 - val_acc: 0.3430\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.67898 to 1.64687, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.6091 - acc: 0.4145 - val_loss: 1.6104 - val_acc: 0.3260\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.64687 to 1.61042, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.5657 - acc: 0.4239 - val_loss: 1.5682 - val_acc: 0.3294\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.61042 to 1.56822, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 28s 4ms/step - loss: 1.5236 - acc: 0.4318 - val_loss: 1.5234 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.56822 to 1.52341, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 29s 4ms/step - loss: 1.4787 - acc: 0.4270 - val_loss: 1.4798 - val_acc: 0.3735\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.52341 to 1.47983, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 26s 4ms/step - loss: 1.4357 - acc: 0.4352 - val_loss: 1.4381 - val_acc: 0.3837\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.47983 to 1.43813, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.3955 - acc: 0.4421 - val_loss: 1.4002 - val_acc: 0.4177\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.43813 to 1.40018, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.3608 - acc: 0.4505 - val_loss: 1.3646 - val_acc: 0.4177\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.40018 to 1.36461, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.3341 - acc: 0.4575 - val_loss: 1.3313 - val_acc: 0.4261\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.36461 to 1.33132, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.3049 - acc: 0.4652 - val_loss: 1.2936 - val_acc: 0.4261\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.33132 to 1.29360, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.2773 - acc: 0.4856 - val_loss: 1.2529 - val_acc: 0.4312\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.29360 to 1.25289, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.2548 - acc: 0.4888 - val_loss: 1.2309 - val_acc: 0.4414\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.25289 to 1.23092, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.2326 - acc: 0.4992 - val_loss: 1.2099 - val_acc: 0.4363\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.23092 to 1.20990, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.2170 - acc: 0.5045 - val_loss: 1.1982 - val_acc: 0.4312\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.20990 to 1.19816, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.2006 - acc: 0.5087 - val_loss: 1.1618 - val_acc: 0.4737\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.19816 to 1.16177, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.1864 - acc: 0.5153 - val_loss: 1.1629 - val_acc: 0.4550\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.16177\n",
      "Test accuracy: 0.5018663047267737\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 7s 1ms/step - loss: 1.7439 - acc: 0.2968 - val_loss: 1.6600 - val_acc: 0.3260\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66000, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 6s 901us/step - loss: 1.5596 - acc: 0.3742 - val_loss: 1.4155 - val_acc: 0.4126\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66000 to 1.41552, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 6s 900us/step - loss: 1.3997 - acc: 0.4001 - val_loss: 1.2697 - val_acc: 0.4363\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.41552 to 1.26974, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 6s 901us/step - loss: 1.3216 - acc: 0.4275 - val_loss: 1.1942 - val_acc: 0.4601\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.26974 to 1.19423, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 6s 892us/step - loss: 1.2625 - acc: 0.4519 - val_loss: 1.1415 - val_acc: 0.4924\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.19423 to 1.14150, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 6s 904us/step - loss: 1.2119 - acc: 0.4701 - val_loss: 1.0945 - val_acc: 0.4482\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.14150 to 1.09447, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 6s 909us/step - loss: 1.1804 - acc: 0.4848 - val_loss: 1.0933 - val_acc: 0.4550\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.09447 to 1.09330, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 6s 892us/step - loss: 1.1537 - acc: 0.4897 - val_loss: 1.0529 - val_acc: 0.5144\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.09330 to 1.05287, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 6s 900us/step - loss: 1.1362 - acc: 0.4956 - val_loss: 1.0868 - val_acc: 0.4805\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.05287\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 6s 898us/step - loss: 1.1377 - acc: 0.4984 - val_loss: 1.1328 - val_acc: 0.4431\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.05287\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 6s 893us/step - loss: 1.1189 - acc: 0.5016 - val_loss: 1.0005 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.05287 to 1.00046, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 6s 904us/step - loss: 1.0945 - acc: 0.5245 - val_loss: 1.1279 - val_acc: 0.4261\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.00046\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 6s 893us/step - loss: 1.0823 - acc: 0.5311 - val_loss: 1.0033 - val_acc: 0.5467\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.00046\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 6s 900us/step - loss: 1.0622 - acc: 0.5416 - val_loss: 1.0788 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.00046\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 6s 903us/step - loss: 1.0505 - acc: 0.5336 - val_loss: 0.9620 - val_acc: 0.5654\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.00046 to 0.96197, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 6s 894us/step - loss: 1.0097 - acc: 0.5552 - val_loss: 0.9597 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.96197 to 0.95967, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 6s 900us/step - loss: 0.9654 - acc: 0.5774 - val_loss: 0.8607 - val_acc: 0.5535\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.95967 to 0.86073, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 6s 904us/step - loss: 0.9452 - acc: 0.5897 - val_loss: 0.7677 - val_acc: 0.6503\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.86073 to 0.76765, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 6s 888us/step - loss: 0.8777 - acc: 0.6107 - val_loss: 0.7861 - val_acc: 0.6078\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.76765\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 7s 1ms/step - loss: 0.8401 - acc: 0.6238 - val_loss: 0.7161 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.76765 to 0.71607, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.6202918221920597\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 6.4420 - acc: 0.1807 - val_loss: 1.8340 - val_acc: 0.1596\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.83396, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 38s 6ms/step - loss: 1.7944 - acc: 0.1850 - val_loss: 1.8471 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.83396\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7929 - acc: 0.1831 - val_loss: 1.8156 - val_acc: 0.1579\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.83396 to 1.81555, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7922 - acc: 0.1857 - val_loss: 1.8081 - val_acc: 0.1596\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.81555 to 1.80814, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7921 - acc: 0.1801 - val_loss: 1.8199 - val_acc: 0.1562\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.80814\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7938 - acc: 0.1835 - val_loss: 1.8339 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.80814\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7959 - acc: 0.1841 - val_loss: 1.8227 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.80814\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7924 - acc: 0.1869 - val_loss: 1.8251 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.80814\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7936 - acc: 0.1814 - val_loss: 1.8510 - val_acc: 0.1579\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.80814\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7926 - acc: 0.1903 - val_loss: 1.8247 - val_acc: 0.1579\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.80814\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7926 - acc: 0.1845 - val_loss: 1.8165 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.80814\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7931 - acc: 0.1783 - val_loss: 1.8195 - val_acc: 0.1579\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.80814\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7924 - acc: 0.1865 - val_loss: 1.8260 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.80814\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7932 - acc: 0.1801 - val_loss: 1.8167 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.80814\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7920 - acc: 0.1838 - val_loss: 1.8025 - val_acc: 0.1562\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.80814 to 1.80254, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7925 - acc: 0.1798 - val_loss: 1.8441 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.80254\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7936 - acc: 0.1859 - val_loss: 1.8003 - val_acc: 0.1579\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.80254 to 1.80030, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7933 - acc: 0.1825 - val_loss: 1.8412 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.80030\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7937 - acc: 0.1842 - val_loss: 1.8109 - val_acc: 0.1681\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.80030\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 35s 5ms/step - loss: 1.7921 - acc: 0.1816 - val_loss: 1.8168 - val_acc: 0.1579\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.80030\n",
      "Test accuracy: 0.18052256532066507\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 31s 5ms/step - loss: 1.5056 - acc: 0.3586 - val_loss: 1.2669 - val_acc: 0.4924\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.26687, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.9711 - acc: 0.5867 - val_loss: 0.7502 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.26687 to 0.75020, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.5957 - acc: 0.7690 - val_loss: 0.4257 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.75020 to 0.42572, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.5185 - acc: 0.8165 - val_loss: 0.8794 - val_acc: 0.6367\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.42572\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.3292 - acc: 0.8927 - val_loss: 0.1276 - val_acc: 0.9338\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42572 to 0.12762, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.2258 - acc: 0.9258 - val_loss: 0.1403 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12762\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1827 - acc: 0.9341 - val_loss: 0.1430 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12762\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1936 - acc: 0.9372 - val_loss: 0.1299 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12762\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.2345 - acc: 0.9318 - val_loss: 0.2383 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12762\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1615 - acc: 0.9432 - val_loss: 0.0942 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12762 to 0.09420, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1650 - acc: 0.9446 - val_loss: 0.0988 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09420\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1603 - acc: 0.9386 - val_loss: 0.1054 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09420\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1311 - acc: 0.9500 - val_loss: 0.1167 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09420\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.2051 - acc: 0.9440 - val_loss: 0.1635 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09420\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.2491 - acc: 0.9407 - val_loss: 0.0827 - val_acc: 0.9491\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09420 to 0.08272, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.2827 - acc: 0.9341 - val_loss: 0.1390 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08272\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.2201 - acc: 0.9478 - val_loss: 0.1010 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08272\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1879 - acc: 0.9474 - val_loss: 0.1088 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08272\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1551 - acc: 0.9511 - val_loss: 0.2325 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08272\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 30s 4ms/step - loss: 0.1885 - acc: 0.9460 - val_loss: 0.1558 - val_acc: 0.9626\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08272\n",
      "Test accuracy: 0.9090600610790635\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 38s 6ms/step - loss: 1.7968 - acc: 0.1748 - val_loss: 1.8033 - val_acc: 0.0849\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.80331, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7948 - acc: 0.1804 - val_loss: 1.8020 - val_acc: 0.1290\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.80331 to 1.80201, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7938 - acc: 0.2052 - val_loss: 1.8007 - val_acc: 0.1545\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.80201 to 1.80070, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7923 - acc: 0.2126 - val_loss: 1.7994 - val_acc: 0.1834\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.80070 to 1.79939, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7916 - acc: 0.2191 - val_loss: 1.7981 - val_acc: 0.1868\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.79939 to 1.79810, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7890 - acc: 0.2310 - val_loss: 1.7968 - val_acc: 0.2071\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.79810 to 1.79681, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7884 - acc: 0.2437 - val_loss: 1.7955 - val_acc: 0.2207\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.79681 to 1.79550, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7878 - acc: 0.2503 - val_loss: 1.7942 - val_acc: 0.2428\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.79550 to 1.79422, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7857 - acc: 0.2633 - val_loss: 1.7929 - val_acc: 0.3820\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.79422 to 1.79290, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7838 - acc: 0.2697 - val_loss: 1.7916 - val_acc: 0.4075\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.79290 to 1.79162, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7829 - acc: 0.2824 - val_loss: 1.7903 - val_acc: 0.4177\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.79162 to 1.79033, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7807 - acc: 0.2848 - val_loss: 1.7891 - val_acc: 0.4160\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.79033 to 1.78906, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7803 - acc: 0.3021 - val_loss: 1.7878 - val_acc: 0.4143\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.78906 to 1.78779, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7787 - acc: 0.3052 - val_loss: 1.7865 - val_acc: 0.4177\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.78779 to 1.78651, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7774 - acc: 0.3102 - val_loss: 1.7852 - val_acc: 0.4160\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.78651 to 1.78525, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7765 - acc: 0.3232 - val_loss: 1.7840 - val_acc: 0.4261\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.78525 to 1.78401, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7742 - acc: 0.3408 - val_loss: 1.7828 - val_acc: 0.4363\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.78401 to 1.78278, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7731 - acc: 0.3351 - val_loss: 1.7816 - val_acc: 0.4363\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.78278 to 1.78155, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7704 - acc: 0.3433 - val_loss: 1.7803 - val_acc: 0.4380\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.78155 to 1.78034, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.7688 - acc: 0.3574 - val_loss: 1.7791 - val_acc: 0.4380\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.78034 to 1.77913, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.4472344757380387\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 28s 4ms/step - loss: 1.7951 - acc: 0.1708 - val_loss: 1.8006 - val_acc: 0.1545\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.80056, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7964 - acc: 0.1677 - val_loss: 1.8001 - val_acc: 0.1545\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.80056 to 1.80011, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7968 - acc: 0.1705 - val_loss: 1.7997 - val_acc: 0.1545\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.80011 to 1.79966, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7948 - acc: 0.1684 - val_loss: 1.7992 - val_acc: 0.1562\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.79966 to 1.79924, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7958 - acc: 0.1678 - val_loss: 1.7988 - val_acc: 0.1562\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.79924 to 1.79881, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7946 - acc: 0.1709 - val_loss: 1.7984 - val_acc: 0.1562\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.79881 to 1.79841, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7935 - acc: 0.1675 - val_loss: 1.7980 - val_acc: 0.1562\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.79841 to 1.79799, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7947 - acc: 0.1695 - val_loss: 1.7976 - val_acc: 0.1562\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.79799 to 1.79758, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7934 - acc: 0.1764 - val_loss: 1.7972 - val_acc: 0.1545\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.79758 to 1.79718, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7914 - acc: 0.1814 - val_loss: 1.7968 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.79718 to 1.79678, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7927 - acc: 0.1739 - val_loss: 1.7964 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.79678 to 1.79638, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7929 - acc: 0.1740 - val_loss: 1.7960 - val_acc: 0.1511\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.79638 to 1.79600, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7907 - acc: 0.1834 - val_loss: 1.7956 - val_acc: 0.1511\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.79600 to 1.79561, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7910 - acc: 0.1736 - val_loss: 1.7952 - val_acc: 0.1511\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.79561 to 1.79523, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7904 - acc: 0.1813 - val_loss: 1.7949 - val_acc: 0.1494\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.79523 to 1.79485, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7899 - acc: 0.1777 - val_loss: 1.7945 - val_acc: 0.1494\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.79485 to 1.79450, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7902 - acc: 0.1838 - val_loss: 1.7941 - val_acc: 0.1494\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.79450 to 1.79414, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7886 - acc: 0.1887 - val_loss: 1.7938 - val_acc: 0.1494\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.79414 to 1.79379, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7891 - acc: 0.1851 - val_loss: 1.7934 - val_acc: 0.1494\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.79379 to 1.79343, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 27s 4ms/step - loss: 1.7891 - acc: 0.1822 - val_loss: 1.7931 - val_acc: 0.1494\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.79343 to 1.79308, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.16762809636918902\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 8s 1ms/step - loss: 1.7969 - acc: 0.3111 - val_loss: 1.3235 - val_acc: 0.3531\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.32353, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 6s 916us/step - loss: 1.3620 - acc: 0.4360 - val_loss: 1.2474 - val_acc: 0.4414\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.32353 to 1.24736, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 6s 929us/step - loss: 1.1722 - acc: 0.5298 - val_loss: 0.8469 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.24736 to 0.84693, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 6s 925us/step - loss: 0.9389 - acc: 0.5632 - val_loss: 0.8094 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.84693 to 0.80938, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 6s 919us/step - loss: 0.8567 - acc: 0.5977 - val_loss: 0.6985 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.80938 to 0.69847, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 6s 929us/step - loss: 0.9655 - acc: 0.5854 - val_loss: 0.7984 - val_acc: 0.5772\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69847\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 6s 925us/step - loss: 0.9174 - acc: 0.6077 - val_loss: 0.7372 - val_acc: 0.5739\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69847\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 6s 913us/step - loss: 0.8008 - acc: 0.6173 - val_loss: 0.7405 - val_acc: 0.5637\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69847\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 6s 932us/step - loss: 0.8541 - acc: 0.6130 - val_loss: 0.7540 - val_acc: 0.5840\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69847\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 6s 925us/step - loss: 0.8536 - acc: 0.6203 - val_loss: 1.7795 - val_acc: 0.3752\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69847\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 6s 920us/step - loss: 1.0835 - acc: 0.5897 - val_loss: 1.5369 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69847\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 6s 932us/step - loss: 1.9901 - acc: 0.4816 - val_loss: 1.4341 - val_acc: 0.3718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69847\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 6s 918us/step - loss: 2.3774 - acc: 0.3858 - val_loss: 1.8951 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69847\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 6s 915us/step - loss: 1.9317 - acc: 0.3612 - val_loss: 1.2416 - val_acc: 0.3956\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69847\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 6s 930us/step - loss: 1.8703 - acc: 0.3807 - val_loss: 1.7187 - val_acc: 0.4788\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.69847\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 6s 918us/step - loss: 1.9362 - acc: 0.3679 - val_loss: 2.0204 - val_acc: 0.3260\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.69847\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 6s 923us/step - loss: 1.4679 - acc: 0.4130 - val_loss: 1.2202 - val_acc: 0.4278\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.69847\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 6s 933us/step - loss: 1.4027 - acc: 0.4523 - val_loss: 1.2835 - val_acc: 0.4720\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.69847\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 6s 915us/step - loss: 1.4677 - acc: 0.4612 - val_loss: 1.3135 - val_acc: 0.5093\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.69847\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 6s 915us/step - loss: 3.1274 - acc: 0.4622 - val_loss: 3.7516 - val_acc: 0.4584\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.69847\n",
      "Test accuracy: 0.4781133355955209\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 11s 2ms/step - loss: 1.7874 - acc: 0.1767 - val_loss: 1.7695 - val_acc: 0.3022\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.76951, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7507 - acc: 0.2601 - val_loss: 1.7239 - val_acc: 0.3480\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.76951 to 1.72393, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.6929 - acc: 0.3170 - val_loss: 1.6392 - val_acc: 0.3922\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.72393 to 1.63918, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.6071 - acc: 0.3515 - val_loss: 1.5522 - val_acc: 0.3786\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.63918 to 1.55218, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.5342 - acc: 0.3849 - val_loss: 1.4934 - val_acc: 0.4448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss improved from 1.55218 to 1.49337, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.4839 - acc: 0.3985 - val_loss: 1.4425 - val_acc: 0.4890\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.49337 to 1.44249, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.4415 - acc: 0.4096 - val_loss: 1.3935 - val_acc: 0.4839\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.44249 to 1.39349, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.3934 - acc: 0.4306 - val_loss: 1.3385 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.39349 to 1.33845, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.3530 - acc: 0.4446 - val_loss: 1.2846 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.33845 to 1.28462, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.3143 - acc: 0.4641 - val_loss: 1.2427 - val_acc: 0.5195\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.28462 to 1.24266, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.2796 - acc: 0.4723 - val_loss: 1.2224 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.24266 to 1.22241, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.2522 - acc: 0.4845 - val_loss: 1.2032 - val_acc: 0.5025\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.22241 to 1.20319, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.2275 - acc: 0.4843 - val_loss: 1.1566 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.20319 to 1.15658, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.2136 - acc: 0.5030 - val_loss: 1.1243 - val_acc: 0.5416\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.15658 to 1.12427, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.1950 - acc: 0.5021 - val_loss: 1.1026 - val_acc: 0.5467\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.12427 to 1.10261, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.1906 - acc: 0.5058 - val_loss: 1.1324 - val_acc: 0.5348\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.10261\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.1710 - acc: 0.5061 - val_loss: 1.1002 - val_acc: 0.5586\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.10261 to 1.10023, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.1543 - acc: 0.5076 - val_loss: 1.0638 - val_acc: 0.5789\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.10023 to 1.06376, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.1464 - acc: 0.5180 - val_loss: 1.0539 - val_acc: 0.5857\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.06376 to 1.05388, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.1322 - acc: 0.5180 - val_loss: 1.0535 - val_acc: 0.5925\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.05388 to 1.05348, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.5405497115710892\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 38s 6ms/step - loss: 1.4091 - acc: 0.3957 - val_loss: 1.1863 - val_acc: 0.4754\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.18626, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.1608 - acc: 0.4919 - val_loss: 0.9909 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.18626 to 0.99091, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 1.1075 - acc: 0.5271 - val_loss: 1.0352 - val_acc: 0.5178\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.99091\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.9995 - acc: 0.5814 - val_loss: 1.1098 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.99091\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.8993 - acc: 0.6179 - val_loss: 0.7414 - val_acc: 0.6044\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.99091 to 0.74140, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.8165 - acc: 0.6302 - val_loss: 0.7289 - val_acc: 0.6401\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.74140 to 0.72894, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.7592 - acc: 0.6478 - val_loss: 0.7341 - val_acc: 0.5908\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.72894\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.7501 - acc: 0.6459 - val_loss: 0.7362 - val_acc: 0.5959\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.72894\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.7191 - acc: 0.6620 - val_loss: 0.7041 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.72894 to 0.70414, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.6929 - acc: 0.6828 - val_loss: 0.8788 - val_acc: 0.5603\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.70414\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.6788 - acc: 0.6995 - val_loss: 0.7318 - val_acc: 0.5756\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.70414\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.6414 - acc: 0.7304 - val_loss: 0.4987 - val_acc: 0.8183\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.70414 to 0.49868, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.6132 - acc: 0.7563 - val_loss: 0.4230 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.49868 to 0.42299, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.5597 - acc: 0.7906 - val_loss: 0.5063 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.42299\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.5104 - acc: 0.8193 - val_loss: 0.3449 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.42299 to 0.34492, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.4496 - acc: 0.8461 - val_loss: 0.2961 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.34492 to 0.29610, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.4537 - acc: 0.8656 - val_loss: 0.4605 - val_acc: 0.7963\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.29610\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.3817 - acc: 0.8709 - val_loss: 0.5364 - val_acc: 0.7725\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.29610\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.3651 - acc: 0.8817 - val_loss: 0.2318 - val_acc: 0.8998\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.29610 to 0.23181, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 37s 5ms/step - loss: 0.3504 - acc: 0.8898 - val_loss: 0.1988 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.23181 to 0.19877, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.830335934848999\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 22s 3ms/step - loss: 1.2198 - acc: 0.4968 - val_loss: 1.0147 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01475, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.8788 - acc: 0.6453 - val_loss: 0.7832 - val_acc: 0.6537\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01475 to 0.78320, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.6940 - acc: 0.7273 - val_loss: 0.5828 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.78320 to 0.58282, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.5761 - acc: 0.7857 - val_loss: 0.4848 - val_acc: 0.8217\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58282 to 0.48475, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.4580 - acc: 0.8524 - val_loss: 0.3544 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48475 to 0.35443, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.3743 - acc: 0.8873 - val_loss: 0.2113 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35443 to 0.21125, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2842 - acc: 0.9136 - val_loss: 0.4754 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21125\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2525 - acc: 0.9267 - val_loss: 0.2013 - val_acc: 0.9117\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.21125 to 0.20130, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2206 - acc: 0.9315 - val_loss: 0.4345 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.20130\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2319 - acc: 0.9355 - val_loss: 0.1271 - val_acc: 0.9491\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.20130 to 0.12709, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2019 - acc: 0.9413 - val_loss: 0.1359 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.12709\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1876 - acc: 0.9423 - val_loss: 0.2123 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.12709\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2111 - acc: 0.9414 - val_loss: 0.0911 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12709 to 0.09111, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1706 - acc: 0.9462 - val_loss: 0.3599 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09111\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1586 - acc: 0.9496 - val_loss: 0.2422 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09111\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1853 - acc: 0.9443 - val_loss: 0.1980 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09111\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1673 - acc: 0.9468 - val_loss: 0.1659 - val_acc: 0.9626\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09111\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1900 - acc: 0.9468 - val_loss: 0.1609 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09111\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1679 - acc: 0.9488 - val_loss: 0.2192 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09111\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1571 - acc: 0.9499 - val_loss: 0.1474 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09111\n",
      "Test accuracy: 0.8958262639972854\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.7866 - acc: 0.1946 - val_loss: 1.7735 - val_acc: 0.2649\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.77350, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7873 - acc: 0.1974 - val_loss: 1.7735 - val_acc: 0.2683\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.77350 to 1.77347, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.7856 - acc: 0.1977 - val_loss: 1.7734 - val_acc: 0.2699\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.77347 to 1.77344, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7873 - acc: 0.1981 - val_loss: 1.7734 - val_acc: 0.2699\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.77344 to 1.77340, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7878 - acc: 0.1937 - val_loss: 1.7734 - val_acc: 0.2699\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.77340 to 1.77337, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7872 - acc: 0.1962 - val_loss: 1.7733 - val_acc: 0.2733\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.77337 to 1.77334, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7865 - acc: 0.1938 - val_loss: 1.7733 - val_acc: 0.2750\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.77334 to 1.77331, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7871 - acc: 0.1959 - val_loss: 1.7733 - val_acc: 0.2750\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.77331 to 1.77328, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7866 - acc: 0.1952 - val_loss: 1.7732 - val_acc: 0.2750\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.77328 to 1.77325, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7858 - acc: 0.2005 - val_loss: 1.7732 - val_acc: 0.2750\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.77325 to 1.77321, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7853 - acc: 0.1975 - val_loss: 1.7732 - val_acc: 0.2750\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.77321 to 1.77318, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7866 - acc: 0.1937 - val_loss: 1.7732 - val_acc: 0.2767\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.77318 to 1.77315, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7854 - acc: 0.1964 - val_loss: 1.7731 - val_acc: 0.2818\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.77315 to 1.77312, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7879 - acc: 0.1959 - val_loss: 1.7731 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.77312 to 1.77309, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7843 - acc: 0.1993 - val_loss: 1.7731 - val_acc: 0.2869\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.77309 to 1.77306, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7875 - acc: 0.1925 - val_loss: 1.7730 - val_acc: 0.2869\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.77306 to 1.77303, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7865 - acc: 0.1984 - val_loss: 1.7730 - val_acc: 0.2920\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.77303 to 1.77299, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7851 - acc: 0.2004 - val_loss: 1.7730 - val_acc: 0.2920\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.77299 to 1.77296, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7855 - acc: 0.2008 - val_loss: 1.7729 - val_acc: 0.2954\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.77296 to 1.77293, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.7870 - acc: 0.2033 - val_loss: 1.7729 - val_acc: 0.2954\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.77293 to 1.77290, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.22667119104173736\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 22s 3ms/step - loss: 1.7984 - acc: 0.1665 - val_loss: 1.7937 - val_acc: 0.2462\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.79366, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7874 - acc: 0.2138 - val_loss: 1.7822 - val_acc: 0.3226\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.79366 to 1.78217, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7752 - acc: 0.2700 - val_loss: 1.7711 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.78217 to 1.77108, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7632 - acc: 0.3229 - val_loss: 1.7600 - val_acc: 0.3650\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.77108 to 1.75999, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7536 - acc: 0.3473 - val_loss: 1.7495 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.75999 to 1.74947, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7375 - acc: 0.3827 - val_loss: 1.7382 - val_acc: 0.3735\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.74947 to 1.73818, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7279 - acc: 0.3921 - val_loss: 1.7261 - val_acc: 0.3735\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.73818 to 1.72615, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7136 - acc: 0.4105 - val_loss: 1.7130 - val_acc: 0.3752\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.72615 to 1.71298, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.6973 - acc: 0.4171 - val_loss: 1.6982 - val_acc: 0.3786\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.71298 to 1.69815, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.6838 - acc: 0.4145 - val_loss: 1.6817 - val_acc: 0.3939\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.69815 to 1.68169, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.6618 - acc: 0.4318 - val_loss: 1.6623 - val_acc: 0.3803\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.68169 to 1.66230, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.6415 - acc: 0.4270 - val_loss: 1.6406 - val_acc: 0.4007\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.66230 to 1.64063, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.6201 - acc: 0.4279 - val_loss: 1.6175 - val_acc: 0.4024\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.64063 to 1.61751, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.5965 - acc: 0.4321 - val_loss: 1.5924 - val_acc: 0.4177\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.61751 to 1.59241, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.5672 - acc: 0.4387 - val_loss: 1.5660 - val_acc: 0.4211\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.59241 to 1.56599, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.5438 - acc: 0.4505 - val_loss: 1.5401 - val_acc: 0.4261\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.56599 to 1.54009, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.5252 - acc: 0.4464 - val_loss: 1.5144 - val_acc: 0.4397\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.54009 to 1.51439, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.5009 - acc: 0.4587 - val_loss: 1.4882 - val_acc: 0.4414\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.51439 to 1.48819, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.4762 - acc: 0.4698 - val_loss: 1.4622 - val_acc: 0.4618\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.48819 to 1.46217, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.4555 - acc: 0.4792 - val_loss: 1.4370 - val_acc: 0.4618\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.46217 to 1.43703, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.4363759755683746\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 8s 1ms/step - loss: 1.8059 - acc: 0.1451 - val_loss: 1.7938 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.79378, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 6s 940us/step - loss: 1.8016 - acc: 0.1616 - val_loss: 1.7919 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.79378 to 1.79189, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 6s 953us/step - loss: 1.7985 - acc: 0.1601 - val_loss: 1.7901 - val_acc: 0.1409\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.79189 to 1.79010, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 6s 944us/step - loss: 1.7982 - acc: 0.1516 - val_loss: 1.7882 - val_acc: 0.1426\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.79010 to 1.78823, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 6s 943us/step - loss: 1.7978 - acc: 0.1607 - val_loss: 1.7864 - val_acc: 0.1715\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.78823 to 1.78638, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 6s 951us/step - loss: 1.7947 - acc: 0.1634 - val_loss: 1.7845 - val_acc: 0.2037\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.78638 to 1.78451, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 6s 942us/step - loss: 1.7920 - acc: 0.1658 - val_loss: 1.7827 - val_acc: 0.2054\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.78451 to 1.78270, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 6s 943us/step - loss: 1.7924 - acc: 0.1618 - val_loss: 1.7809 - val_acc: 0.2207\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.78270 to 1.78091, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 6s 951us/step - loss: 1.7900 - acc: 0.1690 - val_loss: 1.7791 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.78091 to 1.77908, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 6s 946us/step - loss: 1.7879 - acc: 0.1652 - val_loss: 1.7773 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.77908 to 1.77729, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 6s 941us/step - loss: 1.7872 - acc: 0.1706 - val_loss: 1.7754 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.77729 to 1.77543, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 6s 945us/step - loss: 1.7856 - acc: 0.1733 - val_loss: 1.7736 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.77543 to 1.77359, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 6s 956us/step - loss: 1.7854 - acc: 0.1754 - val_loss: 1.7717 - val_acc: 0.2377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_loss improved from 1.77359 to 1.77173, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 6s 944us/step - loss: 1.7854 - acc: 0.1712 - val_loss: 1.7699 - val_acc: 0.2360\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.77173 to 1.76991, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 6s 943us/step - loss: 1.7799 - acc: 0.1904 - val_loss: 1.7680 - val_acc: 0.2360\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.76991 to 1.76803, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 6s 948us/step - loss: 1.7799 - acc: 0.1902 - val_loss: 1.7662 - val_acc: 0.2360\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.76803 to 1.76617, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 6s 944us/step - loss: 1.7789 - acc: 0.1873 - val_loss: 1.7644 - val_acc: 0.2377\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.76617 to 1.76435, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 6s 943us/step - loss: 1.7755 - acc: 0.1896 - val_loss: 1.7625 - val_acc: 0.2360\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.76435 to 1.76254, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 6s 956us/step - loss: 1.7740 - acc: 0.2001 - val_loss: 1.7607 - val_acc: 0.2360\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.76254 to 1.76067, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 6s 943us/step - loss: 1.7724 - acc: 0.1961 - val_loss: 1.7588 - val_acc: 0.2343\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.76067 to 1.75885, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.2534781133355955\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 1.7950 - acc: 0.1619 - val_loss: 1.7917 - val_acc: 0.1443\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.79169, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 1.7955 - acc: 0.1604 - val_loss: 1.7916 - val_acc: 0.1409\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.79169 to 1.79165, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 1.7963 - acc: 0.1578 - val_loss: 1.7916 - val_acc: 0.1409\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.79165 to 1.79161, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 1.7963 - acc: 0.1613 - val_loss: 1.7916 - val_acc: 0.1409\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.79161 to 1.79156, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 40s 6ms/step - loss: 1.7953 - acc: 0.1638 - val_loss: 1.7915 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.79156 to 1.79152, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 40s 6ms/step - loss: 1.7946 - acc: 0.1576 - val_loss: 1.7915 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.79152 to 1.79147, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 39s 6ms/step - loss: 1.7966 - acc: 0.1548 - val_loss: 1.7914 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.79147 to 1.79143, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 40s 6ms/step - loss: 1.7947 - acc: 0.1579 - val_loss: 1.7914 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.79143 to 1.79139, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 1.7950 - acc: 0.1592 - val_loss: 1.7913 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.79139 to 1.79134, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 1.7956 - acc: 0.1616 - val_loss: 1.7913 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.79134 to 1.79130, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 1.7949 - acc: 0.1607 - val_loss: 1.7913 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.79130 to 1.79126, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 41s 6ms/step - loss: 1.7945 - acc: 0.1637 - val_loss: 1.7912 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.79126 to 1.79121, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 1.7952 - acc: 0.1601 - val_loss: 1.7912 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.79121 to 1.79117, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 1.7958 - acc: 0.1613 - val_loss: 1.7911 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.79117 to 1.79113, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 47s 7ms/step - loss: 1.7971 - acc: 0.1572 - val_loss: 1.7911 - val_acc: 0.1392\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.79113 to 1.79108, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 1.7944 - acc: 0.1675 - val_loss: 1.7910 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.79108 to 1.79104, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 42s 6ms/step - loss: 1.7956 - acc: 0.1622 - val_loss: 1.7910 - val_acc: 0.1358\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.79104 to 1.79099, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 45s 7ms/step - loss: 1.7945 - acc: 0.1683 - val_loss: 1.7910 - val_acc: 0.1358\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.79099 to 1.79095, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 1.7950 - acc: 0.1631 - val_loss: 1.7909 - val_acc: 0.1358\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.79095 to 1.79091, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 43s 6ms/step - loss: 1.7962 - acc: 0.1631 - val_loss: 1.7909 - val_acc: 0.1358\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.79091 to 1.79086, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.12215812692894626\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 12s 2ms/step - loss: 1.7931 - acc: 0.1737 - val_loss: 1.7900 - val_acc: 0.2054\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78998, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7925 - acc: 0.1746 - val_loss: 1.7897 - val_acc: 0.2020\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.78998 to 1.78968, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7924 - acc: 0.1708 - val_loss: 1.7894 - val_acc: 0.2020\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.78968 to 1.78937, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7898 - acc: 0.1799 - val_loss: 1.7891 - val_acc: 0.2020\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.78937 to 1.78907, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7914 - acc: 0.1727 - val_loss: 1.7888 - val_acc: 0.2003\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.78907 to 1.78876, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7912 - acc: 0.1797 - val_loss: 1.7885 - val_acc: 0.2020\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.78876 to 1.78846, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7915 - acc: 0.1770 - val_loss: 1.7882 - val_acc: 0.2020\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.78846 to 1.78816, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7898 - acc: 0.1807 - val_loss: 1.7879 - val_acc: 0.1969\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.78816 to 1.78786, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7913 - acc: 0.1761 - val_loss: 1.7876 - val_acc: 0.1969\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.78786 to 1.78757, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7910 - acc: 0.1671 - val_loss: 1.7873 - val_acc: 0.1935\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.78757 to 1.78728, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7903 - acc: 0.1770 - val_loss: 1.7870 - val_acc: 0.2003\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.78728 to 1.78699, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7895 - acc: 0.1838 - val_loss: 1.7867 - val_acc: 0.2054\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.78699 to 1.78669, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7900 - acc: 0.1814 - val_loss: 1.7864 - val_acc: 0.2207\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.78669 to 1.78640, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7888 - acc: 0.1702 - val_loss: 1.7861 - val_acc: 0.2360\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.78640 to 1.78611, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7890 - acc: 0.1828 - val_loss: 1.7858 - val_acc: 0.2462\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.78611 to 1.78581, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7878 - acc: 0.1817 - val_loss: 1.7855 - val_acc: 0.2530\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.78581 to 1.78552, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7878 - acc: 0.1814 - val_loss: 1.7852 - val_acc: 0.2581\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.78552 to 1.78523, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7874 - acc: 0.1829 - val_loss: 1.7849 - val_acc: 0.2666\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.78523 to 1.78494, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7860 - acc: 0.1879 - val_loss: 1.7847 - val_acc: 0.2683\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.78494 to 1.78466, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7875 - acc: 0.1869 - val_loss: 1.7844 - val_acc: 0.2666\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.78466 to 1.78437, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.27689175432643365\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 18s 3ms/step - loss: 1.8076 - acc: 0.1522 - val_loss: 1.8107 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.81067, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.8094 - acc: 0.1519 - val_loss: 1.8106 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.81067 to 1.81061, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8075 - acc: 0.1496 - val_loss: 1.8106 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.81061 to 1.81055, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.8079 - acc: 0.1510 - val_loss: 1.8105 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.81055 to 1.81049, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 15s 2ms/step - loss: 1.8072 - acc: 0.1542 - val_loss: 1.8104 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.81049 to 1.81044, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8107 - acc: 0.1532 - val_loss: 1.8104 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.81044 to 1.81037, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8073 - acc: 0.1572 - val_loss: 1.8103 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.81037 to 1.81032, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8075 - acc: 0.1542 - val_loss: 1.8103 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.81032 to 1.81026, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8086 - acc: 0.1510 - val_loss: 1.8102 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.81026 to 1.81020, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8067 - acc: 0.1508 - val_loss: 1.8101 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.81020 to 1.81014, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8086 - acc: 0.1548 - val_loss: 1.8101 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.81014 to 1.81008, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8088 - acc: 0.1572 - val_loss: 1.8100 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.81008 to 1.81002, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8083 - acc: 0.1604 - val_loss: 1.8100 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.81002 to 1.80996, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8063 - acc: 0.1585 - val_loss: 1.8099 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.80996 to 1.80990, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8089 - acc: 0.1448 - val_loss: 1.8098 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.80990 to 1.80985, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8081 - acc: 0.1607 - val_loss: 1.8098 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.80985 to 1.80979, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8073 - acc: 0.1520 - val_loss: 1.8097 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.80979 to 1.80973, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8075 - acc: 0.1517 - val_loss: 1.8097 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.80973 to 1.80967, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8057 - acc: 0.1600 - val_loss: 1.8096 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.80967 to 1.80962, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 16s 2ms/step - loss: 1.8068 - acc: 0.1557 - val_loss: 1.8096 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.80962 to 1.80956, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.16457414319647098\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 12s 2ms/step - loss: 1.7919 - acc: 0.1879 - val_loss: 1.7954 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.79544, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7927 - acc: 0.1814 - val_loss: 1.7950 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.79544 to 1.79498, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7904 - acc: 0.1912 - val_loss: 1.7945 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.79498 to 1.79453, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7918 - acc: 0.1822 - val_loss: 1.7941 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.79453 to 1.79408, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7886 - acc: 0.1860 - val_loss: 1.7936 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.79408 to 1.79363, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7891 - acc: 0.1819 - val_loss: 1.7932 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.79363 to 1.79319, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7889 - acc: 0.1893 - val_loss: 1.7927 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.79319 to 1.79273, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7884 - acc: 0.1894 - val_loss: 1.7923 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.79273 to 1.79228, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7869 - acc: 0.1906 - val_loss: 1.7918 - val_acc: 0.1324\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.79228 to 1.79184, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7889 - acc: 0.1865 - val_loss: 1.7914 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.79184 to 1.79138, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7878 - acc: 0.1885 - val_loss: 1.7909 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.79138 to 1.79093, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7855 - acc: 0.1906 - val_loss: 1.7905 - val_acc: 0.1341\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.79093 to 1.79048, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7857 - acc: 0.1938 - val_loss: 1.7900 - val_acc: 0.1324\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.79048 to 1.79003, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7868 - acc: 0.1938 - val_loss: 1.7896 - val_acc: 0.1324\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.79003 to 1.78959, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7852 - acc: 0.1984 - val_loss: 1.7891 - val_acc: 0.1324\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.78959 to 1.78914, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7843 - acc: 0.1952 - val_loss: 1.7887 - val_acc: 0.1324\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.78914 to 1.78869, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7855 - acc: 0.1949 - val_loss: 1.7882 - val_acc: 0.1324\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.78869 to 1.78825, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7844 - acc: 0.1891 - val_loss: 1.7878 - val_acc: 0.1307\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.78825 to 1.78779, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7832 - acc: 0.1955 - val_loss: 1.7873 - val_acc: 0.1290\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.78779 to 1.78734, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 9s 1ms/step - loss: 1.7843 - acc: 0.2005 - val_loss: 1.7869 - val_acc: 0.1290\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.78734 to 1.78689, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.15982354937224297\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 1.7712 - acc: 0.2477 - val_loss: 1.7471 - val_acc: 0.4244\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74706, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 1.7161 - acc: 0.3880 - val_loss: 1.6971 - val_acc: 0.4584\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.74706 to 1.69709, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 1.6578 - acc: 0.4290 - val_loss: 1.6361 - val_acc: 0.4465\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.69709 to 1.63607, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 1.5823 - acc: 0.4430 - val_loss: 1.5516 - val_acc: 0.4397\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.63607 to 1.55164, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 1.4884 - acc: 0.4474 - val_loss: 1.4566 - val_acc: 0.4194\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.55164 to 1.45660, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 1.4035 - acc: 0.4588 - val_loss: 1.3859 - val_acc: 0.4228\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.45660 to 1.38590, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 1.3402 - acc: 0.4727 - val_loss: 1.3224 - val_acc: 0.4346\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.38590 to 1.32240, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 1.2814 - acc: 0.4919 - val_loss: 1.2422 - val_acc: 0.4346\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.32240 to 1.24221, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 51s 8ms/step - loss: 1.2339 - acc: 0.5002 - val_loss: 1.1815 - val_acc: 0.4652\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.24221 to 1.18150, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 1.2057 - acc: 0.5042 - val_loss: 1.1473 - val_acc: 0.4618\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.18150 to 1.14727, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 1.1822 - acc: 0.5104 - val_loss: 1.1293 - val_acc: 0.4618\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.14727 to 1.12934, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 1.1633 - acc: 0.5116 - val_loss: 1.1516 - val_acc: 0.4448\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.12934\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 1.1469 - acc: 0.5119 - val_loss: 1.1078 - val_acc: 0.4635\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.12934 to 1.10783, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 1.1347 - acc: 0.5156 - val_loss: 1.0968 - val_acc: 0.4669\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.10783 to 1.09680, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 1.1148 - acc: 0.5267 - val_loss: 1.0949 - val_acc: 0.4584\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.09680 to 1.09494, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 1.1027 - acc: 0.5294 - val_loss: 1.1142 - val_acc: 0.4465\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.09494\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 1.0883 - acc: 0.5356 - val_loss: 1.1196 - val_acc: 0.4244\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.09494\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 46s 7ms/step - loss: 1.0754 - acc: 0.5410 - val_loss: 1.0831 - val_acc: 0.4652\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.09494 to 1.08315, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 52s 8ms/step - loss: 1.0648 - acc: 0.5459 - val_loss: 1.0597 - val_acc: 0.4788\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.08315 to 1.05975, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 56s 8ms/step - loss: 1.0562 - acc: 0.5512 - val_loss: 1.0415 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.05975 to 1.04145, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.5191720393620631\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 17s 2ms/step - loss: 1.7883 - acc: 0.1884 - val_loss: 1.7879 - val_acc: 0.2343\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78786, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7904 - acc: 0.1771 - val_loss: 1.7876 - val_acc: 0.2343\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.78786 to 1.78756, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7888 - acc: 0.1865 - val_loss: 1.7873 - val_acc: 0.2343\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.78756 to 1.78725, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7887 - acc: 0.1873 - val_loss: 1.7869 - val_acc: 0.2326\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.78725 to 1.78693, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7885 - acc: 0.1850 - val_loss: 1.7866 - val_acc: 0.2326\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.78693 to 1.78661, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7867 - acc: 0.1943 - val_loss: 1.7863 - val_acc: 0.2292\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.78661 to 1.78630, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7865 - acc: 0.1952 - val_loss: 1.7860 - val_acc: 0.2258\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.78630 to 1.78598, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7853 - acc: 0.2002 - val_loss: 1.7857 - val_acc: 0.2241\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.78598 to 1.78568, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7850 - acc: 0.1962 - val_loss: 1.7854 - val_acc: 0.2224\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.78568 to 1.78536, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7853 - acc: 0.1995 - val_loss: 1.7850 - val_acc: 0.2224\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.78536 to 1.78505, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7858 - acc: 0.1964 - val_loss: 1.7847 - val_acc: 0.2241\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.78505 to 1.78473, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7850 - acc: 0.2054 - val_loss: 1.7844 - val_acc: 0.2258\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.78473 to 1.78442, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7833 - acc: 0.2017 - val_loss: 1.7841 - val_acc: 0.2224\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.78442 to 1.78410, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7847 - acc: 0.1992 - val_loss: 1.7838 - val_acc: 0.2207\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.78410 to 1.78376, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7841 - acc: 0.2064 - val_loss: 1.7834 - val_acc: 0.2241\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.78376 to 1.78344, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7834 - acc: 0.2017 - val_loss: 1.7831 - val_acc: 0.2258\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.78344 to 1.78311, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7836 - acc: 0.2058 - val_loss: 1.7828 - val_acc: 0.2377\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.78311 to 1.78277, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7805 - acc: 0.2101 - val_loss: 1.7824 - val_acc: 0.2513\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.78277 to 1.78243, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7845 - acc: 0.2060 - val_loss: 1.7821 - val_acc: 0.2581\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.78243 to 1.78209, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.7814 - acc: 0.2117 - val_loss: 1.7817 - val_acc: 0.2615\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.78209 to 1.78174, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.3193077706141839\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.2035 - acc: 0.5094 - val_loss: 0.8403 - val_acc: 0.6027\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84026, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.8751 - acc: 0.6366 - val_loss: 0.7443 - val_acc: 0.6655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84026 to 0.74426, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.7214 - acc: 0.7158 - val_loss: 0.5631 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.74426 to 0.56305, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.6014 - acc: 0.7825 - val_loss: 0.5492 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56305 to 0.54919, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.5330 - acc: 0.8181 - val_loss: 0.4314 - val_acc: 0.8098\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54919 to 0.43141, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.4708 - acc: 0.8455 - val_loss: 0.5342 - val_acc: 0.8098\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43141\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.4255 - acc: 0.8691 - val_loss: 0.3916 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.43141 to 0.39161, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.3596 - acc: 0.8887 - val_loss: 0.2225 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.39161 to 0.22252, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.3044 - acc: 0.9082 - val_loss: 0.3850 - val_acc: 0.9117\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22252\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2754 - acc: 0.9203 - val_loss: 0.1890 - val_acc: 0.9066\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22252 to 0.18896, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2454 - acc: 0.9262 - val_loss: 0.3480 - val_acc: 0.8947\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.18896\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2232 - acc: 0.9302 - val_loss: 0.1740 - val_acc: 0.9219\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.18896 to 0.17400, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2068 - acc: 0.9392 - val_loss: 0.1559 - val_acc: 0.9491\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.17400 to 0.15587, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1951 - acc: 0.9404 - val_loss: 0.1347 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.15587 to 0.13472, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1790 - acc: 0.9419 - val_loss: 0.1019 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.13472 to 0.10189, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1724 - acc: 0.9438 - val_loss: 0.1042 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10189\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1805 - acc: 0.9429 - val_loss: 0.1364 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10189\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1580 - acc: 0.9465 - val_loss: 0.2307 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10189\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1608 - acc: 0.9450 - val_loss: 0.1176 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10189\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1560 - acc: 0.9472 - val_loss: 0.2235 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10189\n",
      "Test accuracy: 0.8832711231761113\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.7799 - acc: 0.2080 - val_loss: 1.7863 - val_acc: 0.2767\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78628, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 1.7682 - acc: 0.2432 - val_loss: 1.7720 - val_acc: 0.3090\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.78628 to 1.77195, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7527 - acc: 0.2906 - val_loss: 1.7569 - val_acc: 0.3107\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.77195 to 1.75695, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7377 - acc: 0.3161 - val_loss: 1.7404 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.75695 to 1.74039, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.7183 - acc: 0.3405 - val_loss: 1.7198 - val_acc: 0.3209\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.74039 to 1.71982, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.6961 - acc: 0.3549 - val_loss: 1.6913 - val_acc: 0.3243\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.71982 to 1.69125, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.6612 - acc: 0.3671 - val_loss: 1.6459 - val_acc: 0.3362\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.69125 to 1.64590, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.6000 - acc: 0.3778 - val_loss: 1.5695 - val_acc: 0.3260\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.64590 to 1.56955, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.5246 - acc: 0.3807 - val_loss: 1.5114 - val_acc: 0.3277\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.56955 to 1.51137, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.4814 - acc: 0.3917 - val_loss: 1.4760 - val_acc: 0.3480\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.51137 to 1.47600, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.4425 - acc: 0.4029 - val_loss: 1.4365 - val_acc: 0.3786\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.47600 to 1.43651, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.4008 - acc: 0.4185 - val_loss: 1.3750 - val_acc: 0.4465\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.43651 to 1.37503, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.3533 - acc: 0.4458 - val_loss: 1.3209 - val_acc: 0.4618\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.37503 to 1.32085, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.3151 - acc: 0.4659 - val_loss: 1.2840 - val_acc: 0.4822\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.32085 to 1.28404, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.2934 - acc: 0.4825 - val_loss: 1.2569 - val_acc: 0.4720\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.28404 to 1.25694, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.2776 - acc: 0.4806 - val_loss: 1.2386 - val_acc: 0.4703\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.25694 to 1.23865, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.2600 - acc: 0.4865 - val_loss: 1.2233 - val_acc: 0.4567\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.23865 to 1.22328, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.2514 - acc: 0.4924 - val_loss: 1.2132 - val_acc: 0.5263\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.22328 to 1.21319, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.2436 - acc: 0.4834 - val_loss: 1.1991 - val_acc: 0.5450\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.21319 to 1.19912, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 1.2348 - acc: 0.4927 - val_loss: 1.1919 - val_acc: 0.5450\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.19912 to 1.19188, saving model to keras_lstm_weights.hdf5\n",
      "Test accuracy: 0.5154394299287411\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 23s 3ms/step - loss: 1.0479 - acc: 0.5453 - val_loss: 0.7648 - val_acc: 0.6231\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76480, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.6592 - acc: 0.7066 - val_loss: 0.4683 - val_acc: 0.7895\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.76480 to 0.46831, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.4133 - acc: 0.8554 - val_loss: 0.1407 - val_acc: 0.9491\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.46831 to 0.14066, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.2839 - acc: 0.9141 - val_loss: 0.0958 - val_acc: 0.9542\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14066 to 0.09577, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2245 - acc: 0.9309 - val_loss: 0.1008 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.09577\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.2700 - acc: 0.9273 - val_loss: 0.2711 - val_acc: 0.8778\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09577\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.2199 - acc: 0.9314 - val_loss: 0.0983 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09577\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1902 - acc: 0.9426 - val_loss: 0.1314 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09577\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1828 - acc: 0.9419 - val_loss: 0.0865 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09577 to 0.08651, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.1940 - acc: 0.9440 - val_loss: 0.1762 - val_acc: 0.8998\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08651\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1837 - acc: 0.9450 - val_loss: 0.1186 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.08651\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1821 - acc: 0.9477 - val_loss: 0.1258 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08651\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1526 - acc: 0.9500 - val_loss: 0.1202 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.08651\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 20s 3ms/step - loss: 0.2015 - acc: 0.9456 - val_loss: 0.1285 - val_acc: 0.9049\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08651\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1813 - acc: 0.9477 - val_loss: 0.2700 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08651\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1554 - acc: 0.9471 - val_loss: 0.2211 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08651\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1946 - acc: 0.9434 - val_loss: 0.1674 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08651\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1891 - acc: 0.9491 - val_loss: 0.0891 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08651\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.1867 - acc: 0.9471 - val_loss: 0.2187 - val_acc: 0.9491\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08651\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 21s 3ms/step - loss: 0.2353 - acc: 0.9469 - val_loss: 0.1422 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08651\n",
      "Test accuracy: 0.8941296233457754\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 17s 3ms/step - loss: 1.3454 - acc: 0.4701 - val_loss: 0.9904 - val_acc: 0.5688\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99041, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.0139 - acc: 0.5849 - val_loss: 0.7762 - val_acc: 0.6027\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.99041 to 0.77620, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.8580 - acc: 0.6263 - val_loss: 0.7250 - val_acc: 0.6978\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.77620 to 0.72497, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.7891 - acc: 0.6868 - val_loss: 0.6317 - val_acc: 0.7284\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.72497 to 0.63168, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.6266 - acc: 0.7702 - val_loss: 0.6882 - val_acc: 0.7742\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63168\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.5894 - acc: 0.7937 - val_loss: 0.3678 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.63168 to 0.36781, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.5329 - acc: 0.8307 - val_loss: 0.2546 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.36781 to 0.25457, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.4427 - acc: 0.8657 - val_loss: 0.2377 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25457 to 0.23767, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.3610 - acc: 0.8900 - val_loss: 0.1784 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23767 to 0.17837, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.3309 - acc: 0.8955 - val_loss: 0.2616 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.17837\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.3075 - acc: 0.9031 - val_loss: 0.1792 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.17837\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2560 - acc: 0.9231 - val_loss: 0.1222 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.17837 to 0.12217, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2671 - acc: 0.9227 - val_loss: 0.1095 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12217 to 0.10949, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2276 - acc: 0.9309 - val_loss: 0.1026 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.10949 to 0.10264, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2227 - acc: 0.9301 - val_loss: 0.1023 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.10264 to 0.10232, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1919 - acc: 0.9386 - val_loss: 0.1400 - val_acc: 0.9525\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10232\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2283 - acc: 0.9364 - val_loss: 0.1665 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10232\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1985 - acc: 0.9409 - val_loss: 0.1406 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10232\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1976 - acc: 0.9450 - val_loss: 0.1122 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10232\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2168 - acc: 0.9398 - val_loss: 0.1326 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10232\n",
      "Test accuracy: 0.9032914828639295\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 17s 3ms/step - loss: 1.3462 - acc: 0.4491 - val_loss: 1.1762 - val_acc: 0.4907\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.17618, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.0548 - acc: 0.5614 - val_loss: 0.8692 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.17618 to 0.86923, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.8902 - acc: 0.6311 - val_loss: 1.0335 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.86923\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.7469 - acc: 0.7077 - val_loss: 0.7673 - val_acc: 0.7453\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86923 to 0.76727, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.6626 - acc: 0.7544 - val_loss: 0.5729 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.76727 to 0.57288, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.5828 - acc: 0.7918 - val_loss: 0.5054 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.57288 to 0.50538, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.4996 - acc: 0.8289 - val_loss: 0.5902 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50538\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.4340 - acc: 0.8641 - val_loss: 0.5673 - val_acc: 0.7742\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.50538\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.4310 - acc: 0.8748 - val_loss: 0.2052 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50538 to 0.20523, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.3736 - acc: 0.8937 - val_loss: 0.3208 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20523\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2842 - acc: 0.9138 - val_loss: 0.2122 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20523\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2940 - acc: 0.9160 - val_loss: 0.1986 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.20523 to 0.19857, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2590 - acc: 0.9179 - val_loss: 0.1599 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.19857 to 0.15994, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2953 - acc: 0.9184 - val_loss: 0.1338 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.15994 to 0.13382, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2285 - acc: 0.9256 - val_loss: 0.1402 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13382\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1923 - acc: 0.9335 - val_loss: 0.3127 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13382\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1919 - acc: 0.9394 - val_loss: 0.1586 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13382\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1943 - acc: 0.9412 - val_loss: 0.0934 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.13382 to 0.09340, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1970 - acc: 0.9395 - val_loss: 0.1336 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09340\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1633 - acc: 0.9434 - val_loss: 0.2035 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09340\n",
      "Test accuracy: 0.8920936545639634\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 17s 3ms/step - loss: 1.7465 - acc: 0.2778 - val_loss: 1.7199 - val_acc: 0.4380\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.71989, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.6141 - acc: 0.3949 - val_loss: 1.4975 - val_acc: 0.4737\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.71989 to 1.49753, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.3686 - acc: 0.4591 - val_loss: 1.2914 - val_acc: 0.5127\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49753 to 1.29143, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.2670 - acc: 0.4863 - val_loss: 1.2243 - val_acc: 0.4805\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.29143 to 1.22433, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.2243 - acc: 0.5013 - val_loss: 1.1731 - val_acc: 0.5025\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.22433 to 1.17305, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.1886 - acc: 0.5017 - val_loss: 1.1143 - val_acc: 0.5484\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.17305 to 1.11429, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.1476 - acc: 0.5191 - val_loss: 1.0884 - val_acc: 0.5705\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.11429 to 1.08841, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.1044 - acc: 0.5441 - val_loss: 1.0337 - val_acc: 0.6044\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.08841 to 1.03372, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.0864 - acc: 0.5617 - val_loss: 0.9965 - val_acc: 0.6078\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.03372 to 0.99645, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.0400 - acc: 0.5913 - val_loss: 0.9655 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99645 to 0.96546, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 1.0130 - acc: 0.6005 - val_loss: 0.9454 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.96546 to 0.94536, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.9821 - acc: 0.6132 - val_loss: 0.9110 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.94536 to 0.91098, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.9523 - acc: 0.6207 - val_loss: 0.9016 - val_acc: 0.6537\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.91098 to 0.90159, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.9413 - acc: 0.6266 - val_loss: 0.8861 - val_acc: 0.6537\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.90159 to 0.88613, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.9201 - acc: 0.6352 - val_loss: 0.8518 - val_acc: 0.7029\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.88613 to 0.85177, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.9040 - acc: 0.6416 - val_loss: 0.8303 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.85177 to 0.83035, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.8750 - acc: 0.6550 - val_loss: 0.8243 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.83035 to 0.82428, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.8669 - acc: 0.6549 - val_loss: 0.8287 - val_acc: 0.6961\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.82428\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.8504 - acc: 0.6639 - val_loss: 0.8159 - val_acc: 0.6808\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.82428 to 0.81593, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.8350 - acc: 0.6670 - val_loss: 0.8410 - val_acc: 0.6587\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.81593\n",
      "Test accuracy: 0.6874787920120816\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 1.2929 - acc: 0.4661 - val_loss: 1.1401 - val_acc: 0.5789\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.14013, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.9521 - acc: 0.6088 - val_loss: 0.7843 - val_acc: 0.6231\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.14013 to 0.78427, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.7731 - acc: 0.6879 - val_loss: 0.6331 - val_acc: 0.7963\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.78427 to 0.63311, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 49s 7ms/step - loss: 0.5934 - acc: 0.7721 - val_loss: 0.4482 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63311 to 0.44821, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.4869 - acc: 0.8294 - val_loss: 0.3313 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.44821 to 0.33134, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.3822 - acc: 0.8688 - val_loss: 0.2554 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.33134 to 0.25541, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.3088 - acc: 0.9005 - val_loss: 0.2571 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25541\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.2524 - acc: 0.9197 - val_loss: 0.1608 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25541 to 0.16084, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 48s 7ms/step - loss: 0.2326 - acc: 0.9246 - val_loss: 0.1558 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.16084 to 0.15579, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.2064 - acc: 0.9324 - val_loss: 0.1467 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15579 to 0.14666, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.1833 - acc: 0.9383 - val_loss: 0.2116 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14666\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 52s 8ms/step - loss: 0.1675 - acc: 0.9389 - val_loss: 0.1592 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14666\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.1679 - acc: 0.9437 - val_loss: 0.1230 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.14666 to 0.12298, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.1582 - acc: 0.9428 - val_loss: 0.1784 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12298\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.2410 - acc: 0.9389 - val_loss: 0.1876 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.12298\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.1610 - acc: 0.9441 - val_loss: 0.1536 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.12298\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 50s 7ms/step - loss: 0.1456 - acc: 0.9491 - val_loss: 0.1077 - val_acc: 0.9491\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.12298 to 0.10772, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.1418 - acc: 0.9490 - val_loss: 0.0912 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10772 to 0.09116, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 54s 8ms/step - loss: 0.1400 - acc: 0.9497 - val_loss: 0.1711 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09116\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 53s 8ms/step - loss: 0.1338 - acc: 0.9500 - val_loss: 0.1390 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09116\n",
      "Test accuracy: 0.8747879199185612\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 18s 3ms/step - loss: 1.2486 - acc: 0.4510 - val_loss: 1.1610 - val_acc: 0.4992\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.16102, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.7738 - acc: 0.6642 - val_loss: 0.5615 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.16102 to 0.56154, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.5223 - acc: 0.8101 - val_loss: 0.3148 - val_acc: 0.8727\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56154 to 0.31483, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.3389 - acc: 0.8931 - val_loss: 0.5056 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31483\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2976 - acc: 0.9108 - val_loss: 0.7304 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31483\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2395 - acc: 0.9210 - val_loss: 0.1703 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.31483 to 0.17035, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2036 - acc: 0.9286 - val_loss: 0.1930 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.17035\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1810 - acc: 0.9352 - val_loss: 0.1042 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.17035 to 0.10423, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1769 - acc: 0.9416 - val_loss: 0.1274 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10423\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1634 - acc: 0.9462 - val_loss: 0.1962 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10423\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1743 - acc: 0.9435 - val_loss: 0.0840 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.10423 to 0.08405, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1542 - acc: 0.9481 - val_loss: 0.1573 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08405\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1509 - acc: 0.9466 - val_loss: 0.1552 - val_acc: 0.9525\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.08405\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1397 - acc: 0.9512 - val_loss: 0.0948 - val_acc: 0.9542\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08405\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2871 - acc: 0.9308 - val_loss: 0.0949 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08405\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1296 - acc: 0.9509 - val_loss: 0.1039 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08405\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1605 - acc: 0.9496 - val_loss: 0.0883 - val_acc: 0.9626\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08405\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1967 - acc: 0.9420 - val_loss: 0.4690 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08405\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.1902 - acc: 0.9460 - val_loss: 0.1446 - val_acc: 0.9626\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08405\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 14s 2ms/step - loss: 0.2537 - acc: 0.9437 - val_loss: 0.1001 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08405\n",
      "Test accuracy: 0.9019341703427214\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 62s 9ms/step - loss: 1.4363 - acc: 0.3976 - val_loss: 1.2725 - val_acc: 0.3888\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.27245, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.9443 - acc: 0.5796 - val_loss: 0.9497 - val_acc: 0.5756\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.27245 to 0.94969, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.6260 - acc: 0.7606 - val_loss: 0.4349 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.94969 to 0.43485, saving model to keras_lstm_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.5558 - acc: 0.8067 - val_loss: 0.2190 - val_acc: 0.9338\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43485 to 0.21896, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.2734 - acc: 0.9102 - val_loss: 0.1504 - val_acc: 0.9219\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21896 to 0.15042, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.2355 - acc: 0.9261 - val_loss: 0.1906 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15042\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.2329 - acc: 0.9268 - val_loss: 0.1710 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.15042\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.2146 - acc: 0.9318 - val_loss: 0.1219 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.15042 to 0.12194, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.2186 - acc: 0.9375 - val_loss: 0.1596 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12194\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.1837 - acc: 0.9318 - val_loss: 0.1307 - val_acc: 0.9542\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12194\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 63s 9ms/step - loss: 0.1507 - acc: 0.9404 - val_loss: 0.1163 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12194 to 0.11630, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.1771 - acc: 0.9428 - val_loss: 0.1170 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11630\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.1704 - acc: 0.9451 - val_loss: 0.0989 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11630 to 0.09893, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 63s 9ms/step - loss: 0.1475 - acc: 0.9474 - val_loss: 0.1526 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09893\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 59s 9ms/step - loss: 0.1474 - acc: 0.9465 - val_loss: 0.1205 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09893\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.1731 - acc: 0.9432 - val_loss: 0.1371 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09893\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 55s 8ms/step - loss: 0.2970 - acc: 0.9287 - val_loss: 0.2022 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09893\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 58s 9ms/step - loss: 0.1985 - acc: 0.9376 - val_loss: 0.1853 - val_acc: 0.9338\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09893\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.1546 - acc: 0.9465 - val_loss: 0.1945 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09893\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 60s 9ms/step - loss: 0.1477 - acc: 0.9440 - val_loss: 0.1388 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09893\n",
      "Test accuracy: 0.9026128266033254\n",
      "Train on 6763 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "6763/6763 [==============================] - 68s 10ms/step - loss: 1.3708 - acc: 0.4264 - val_loss: 1.1037 - val_acc: 0.5925\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.10365, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 2/20\n",
      "6763/6763 [==============================] - 65s 10ms/step - loss: 1.0092 - acc: 0.5807 - val_loss: 0.8404 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.10365 to 0.84037, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 3/20\n",
      "6763/6763 [==============================] - 65s 10ms/step - loss: 0.7899 - acc: 0.6831 - val_loss: 0.6297 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.84037 to 0.62968, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 4/20\n",
      "6763/6763 [==============================] - 64s 10ms/step - loss: 0.6197 - acc: 0.7662 - val_loss: 0.4409 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62968 to 0.44092, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 5/20\n",
      "6763/6763 [==============================] - 65s 10ms/step - loss: 0.5373 - acc: 0.8150 - val_loss: 0.4519 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.44092\n",
      "Epoch 6/20\n",
      "6763/6763 [==============================] - 61s 9ms/step - loss: 0.4700 - acc: 0.8498 - val_loss: 0.3797 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44092 to 0.37966, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 7/20\n",
      "6763/6763 [==============================] - 64s 9ms/step - loss: 0.4025 - acc: 0.8808 - val_loss: 0.2923 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.37966 to 0.29233, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 8/20\n",
      "6763/6763 [==============================] - 57s 8ms/step - loss: 0.3224 - acc: 0.9076 - val_loss: 0.1681 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.29233 to 0.16810, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 9/20\n",
      "6763/6763 [==============================] - 64s 9ms/step - loss: 0.2716 - acc: 0.9083 - val_loss: 0.3196 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16810\n",
      "Epoch 10/20\n",
      "6763/6763 [==============================] - 69s 10ms/step - loss: 0.2460 - acc: 0.9168 - val_loss: 0.2373 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.16810\n",
      "Epoch 11/20\n",
      "6763/6763 [==============================] - 67s 10ms/step - loss: 0.2102 - acc: 0.9330 - val_loss: 0.1336 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.16810 to 0.13364, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 12/20\n",
      "6763/6763 [==============================] - 67s 10ms/step - loss: 0.1957 - acc: 0.9366 - val_loss: 0.1432 - val_acc: 0.9542\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13364\n",
      "Epoch 13/20\n",
      "6763/6763 [==============================] - 62s 9ms/step - loss: 0.1760 - acc: 0.9423 - val_loss: 0.1453 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13364\n",
      "Epoch 14/20\n",
      "6763/6763 [==============================] - 62s 9ms/step - loss: 0.1867 - acc: 0.9422 - val_loss: 0.1166 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13364 to 0.11656, saving model to keras_lstm_weights.hdf5\n",
      "Epoch 15/20\n",
      "6763/6763 [==============================] - 65s 10ms/step - loss: 0.1614 - acc: 0.9414 - val_loss: 0.1590 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11656\n",
      "Epoch 16/20\n",
      "6763/6763 [==============================] - 62s 9ms/step - loss: 0.1802 - acc: 0.9398 - val_loss: 0.1240 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11656\n",
      "Epoch 17/20\n",
      "6763/6763 [==============================] - 64s 9ms/step - loss: 0.1678 - acc: 0.9435 - val_loss: 0.1369 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11656\n",
      "Epoch 18/20\n",
      "6763/6763 [==============================] - 68s 10ms/step - loss: 0.1496 - acc: 0.9453 - val_loss: 0.1525 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11656\n",
      "Epoch 19/20\n",
      "6763/6763 [==============================] - 64s 9ms/step - loss: 0.1477 - acc: 0.9437 - val_loss: 0.1422 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11656\n",
      "Epoch 20/20\n",
      "6763/6763 [==============================] - 64s 9ms/step - loss: 0.1520 - acc: 0.9457 - val_loss: 0.1210 - val_acc: 0.9491\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11656\n",
      "Test accuracy: 0.9104173736002714\n"
     ]
    }
   ],
   "source": [
    "best_run2, best_model2 = optim.minimize(model=model2, #A function defining a keras model with hyperas templates, which returns a valid hyperopt results dictionary, e.g. return {‘loss’: -acc, ‘status’: STATUS_OK}\n",
    "                                          data = data,  #A parameter-less function that defines and return all data needed in the above model definition\n",
    "                                          algo=tpe.suggest,# Tree-structured Parzen Estimator (TPE) algorithm is a bayesian algorithm \n",
    "                                          max_evals=30,  #we use at most 30 evaluation runs for optimization.\n",
    "                                          trials=Trials(),  #A hyperopt Trials object, used to store intermediate results for all optimization runs\n",
    "                                          notebook_name='HAR_LSTM') # If running from an ipython notebook, provide filename (not path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dense': 2, 'LSTM': 2, 'LSTM_1': 1, 'choiceval': 0, 'conditional': 0, 'lr': 3, 'lr_1': 5, 'lr_2': 3}\n"
     ]
    }
   ],
   "source": [
    "print(best_run2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense:2          means   2-hidden layer\n",
    "\n",
    "LSTM :2          means   128 units of LSTM\n",
    "\n",
    "LSTM_1:1         means   no. of LSTM units in second layer which is 64 here\n",
    "\n",
    "Choiceval:0      means   Adam\n",
    "\n",
    "Conditional :0   means   Adding 1 layer if it will be 1 means we need to to add 2 layer\n",
    "\n",
    "lr:3             means   Adam with learning rate 10**-3\n",
    "\n",
    "lr_1:5           means   RMSProp with learning rate 10**-1\n",
    "\n",
    "lr_2:3           means   SGD with learning rate 10**-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Best Model on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of best performing model:\n",
      "[0.3377331184284287, 0.9104173736002714]\n",
      "---------------------------------------------------------------------\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 2, 'LSTM': 2, 'LSTM_1': 1, 'choiceval': 0, 'conditional': 0, 'lr': 3, 'lr_1': 5, 'lr_2': 3}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train,Y_test = data()\n",
    "print(\"Evaluation of best performing model:\")\n",
    "    \n",
    "print(best_model2.evaluate(X_test, Y_test,verbose=0))\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run2)\n",
    "json.dump(best_run2, open(\"best_run.txt\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6b80e22b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "plot_confusion_matrix(Y_test, best_model2.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "After Hyperparameter tuning loss improved as well as Accuracy is also improved.\n",
    "\n",
    "If we tune the Dropout rate then our result will more improve.\n",
    "\n",
    "Model is confused between Sitting and Standing  ,but other results are good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
